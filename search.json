[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ec1027-guide",
    "section": "",
    "text": "Prefacio\nHola, esto es una pequeña prueba."
  },
  {
    "objectID": "index.html#un-cálculo",
    "href": "index.html#un-cálculo",
    "title": "ec1027-guide",
    "section": "Un cálculo",
    "text": "Un cálculo\n\n\n[1] 2"
  },
  {
    "objectID": "index.html#una-regresión",
    "href": "index.html#una-regresión",
    "title": "ec1027-guide",
    "section": "Una regresión",
    "text": "Una regresión\nEcuación (7.5) en la página 250 de Stock y Watson (2020): \n  \\widehat{\\mathit{TestScore}}_i =\n  \\underset{\\displaystyle \\mathllap{(}8.7\\mathrlap{)}}{686.0}\n  - \\underset{\\displaystyle \\mathllap{(}0.43\\mathrlap{)}}{1.10}\\, \\mathit{STR}_i\n  - \\underset{\\displaystyle \\mathllap{(}0.031\\mathrlap{)}}{0.650}\\, \\mathit{PctEL}_i\n  \\\\[1ex]\n  N = 156;\\quad R^2= 0.89;\\quad \\hat{\\sigma}= 1.78."
  },
  {
    "objectID": "index.html#un-diagrama",
    "href": "index.html#un-diagrama",
    "title": "ec1027-guide",
    "section": "Un diagrama",
    "text": "Un diagrama\n\n\n\n\n\nFigura 1: Funky tikz"
  },
  {
    "objectID": "mcrl.html#supuestos",
    "href": "mcrl.html#supuestos",
    "title": "1  El modelo clásico de regresión lineal",
    "section": "1.1 Supuestos",
    "text": "1.1 Supuestos\n\nRLM.1: Linealidad en los parámetros.\nRLM.2: Muestreo aleatorio.\nRLM.3: Ausencia de multicolinealidad perfecta.\nRLM.4: Media condicional nula.\nRLM.5: Homoscedasticidad.\nRLM.6: Normalidad.\n\n\nRLM.1: Linealidad en los parámetros\nEl modelo poblacional puede expresarse como: \n  y = \\beta_{0} + \\beta_{1} x_{1} + \\dots + \\beta_{k} x_{k} + u\n donde:\n\ny es la variable dependiente,\n\\beta_0, \\beta_1, \\dots, \\beta_k son los parámetros desconocidos,\nx_1, x_2, \\dots, x_k son las variables explicativas y\nu es un término de error no observable.\n\n\n\nRLM.2: Muestreo aleatorio\nDisponemos de una muestra aleatoria de n observaciones: \n  \\{(x_{1i}, x_{2i}, \\dots, x_{ki}, y_{i}); i = 1, 2, \\dots, n \\}\n\n\n\nRLM.3: No hay colinealidad perfecta\nEn la muestra se cumplen todas las condiciones:\n\nEl número de observaciones, n, es mayor que el de parámetros, k + 1.\nNinguna de las variables explicativas es constante.\nNo existen relaciones lineales exactas entre las explicativas.\n\n\n\nRLM.4: Media condicional nula\nEl valor esperado del término de error para cualquier combinación de valores que tomen las variables explicativas es 0: \n  \\mathop{\\mathrm{E}}(u | x_{1}, x_{2}, \\dots, x_{k}) = 0\n\n\n\nRLM.5: Homoscedasticidad\nLa varianza del término de error no depende de los valores que tomen las explicativas: \n  \\mathop{\\mathrm{var}}(u | x_{1}, x_{2}, \\dots, x_{k}) = \\sigma^{2}\n\n\n\nRLM.6: Normalidad\nEl término de error es independiente de las variables explicativas y su distribución es normal con media 0 y varianza \\sigma^2: \n  u \\sim \\mathop{\\mathrm{Normal}}(0, \\sigma^{2})"
  },
  {
    "objectID": "mcrl.html#estimación",
    "href": "mcrl.html#estimación",
    "title": "1  El modelo clásico de regresión lineal",
    "section": "1.2 Estimación",
    "text": "1.2 Estimación\n\nFunción de regresión muestral\nFunción de regresión muestral: \n  \\hat{y}= \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{1} + \\dots + \\hat{\\beta}_{k} x_{k}\n donde:\n\n\\hat{y} son las predicciones,\n\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_k son las estimaciones de los parámetros.\n\n\n\nResiduos\nResiduos: \n   \\hat{u}= y - \\hat{y}\n\nLa función de regresión muestral también puede expresarse como: \n  y = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{1} + \\dots + \\hat{\\beta}_{k} x_{k} + \\hat{u}\n\n\n\nEstimación por MCO\nEl estimador de mínimos cuadrados ordinarios, MCO, minimiza la suma del cuadrado de los residuos: \n  \\min_{\\{\\hat{\\beta}_{0}, \\dots, \\hat{\\beta}_{k}\\}} \\sum_{i = 1}^n \\hat{u}_i^{2}\n\n\n\nEcuaciones normales\nEl estimador MCO se obtiene resolviendo las ecuaciones normales: \n\\begin{gather*}\n  \\sum_{i = 1}^n \\hat{u}_{i} = 0 \\\\\n  \\sum_{i = 1}^n \\hat{u}_{i} x_{ji} = 0 \\quad (j = 1, 2, \\dots, k)\n\\end{gather*}\n\n\n\nSumas de cuadrados\nSuma de cuadrados total: \n   \\mathop{\\mathrm{SCT}}= \\sum_{1=1}^n (y_i - \\bar{y})^{2}\n\nSuma de cuadrados explicada: \n   \\mathop{\\mathrm{SCE}}= \\sum_{1=1}^n (\\hat{y}_i - \\bar{y})^{2}\n\nSuma de cuadrados de los residuos: \n  \\mathop{\\mathrm{SCR}}= \\sum_{1=1}^n \\hat{u}_i^{2}\n\nLa estimación por MCO garantiza que: \n  \\mathop{\\mathrm{SCT}}= \\mathop{\\mathrm{SCE}}+ \\mathop{\\mathrm{SCR}}\n\n\n\nBondad del ajuste\n\nError típico de la regresión: \n  \\hat{\\sigma}= \\sqrt{\\frac{\\mathop{\\mathrm{SCR}}}{n - k - 1}}\n\nCoeficiente de determinación: \n  R^2= 1 - \\frac{\\mathop{\\mathrm{SCR}}}{\\mathop{\\mathrm{SCT}}}\n\nR^2 ajustado: \n  \\bar{R}^2= 1 - \\frac{\\mathop{\\mathrm{SCR}}/(n - k - 1)}{\\mathop{\\mathrm{SCT}}/(n - 1)}"
  },
  {
    "objectID": "mcrl.html#propiedades-del-estimador-de-mco",
    "href": "mcrl.html#propiedades-del-estimador-de-mco",
    "title": "1  El modelo clásico de regresión lineal",
    "section": "1.3 Propiedades del estimador de MCO",
    "text": "1.3 Propiedades del estimador de MCO\n\nPropiedades de muestras pequeñas\n\nSe refieren al método de estimación, MCO, no a las estimaciones obtenidas con una muestra particular.\nDependen del cumplimiento de los supuestos del modelo de regresión lineal clásico.\nNo dependen del tamaño muestral: son válidas para cualquier n.\n\n\n\nInsesgadez\nInsesgadez: El valor esperado de las estimaciones coincide con los parámetros poblacionales: \n  \\mathop{\\mathrm{E}}(\\hat{\\beta}_{j})  = \\beta_{j} \\quad (j = 0, 1, \\dots, k).\n\nEl estimador de MCO es insesgado si se cumplen los supuestos RLM.1 a RLM.4.\n\n\nOmisión de variables relevantes\n\nEjemplo: modelo que cumple los supuestos RLM.1 a RLM.4: \n  y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + u\n\nRealizamos la regresión de y sobre x_1 omitiendo la variable x_2.\n¿Qué ocurre si especificamos un modelo donde falta una de las explicativas?\n\n\n\nSesgo\n\nCuando se omite x_2: \n  \\mathop{\\mathrm{E}}(\\hat{\\beta}_{1}) = \\beta_{1} + \\beta_2 \\delta_{12}\n donde \\delta_{12} es la pendiente de la regresión de x_1 sobre x_2.\nSesgo por omisión de x_2: \n  \\operatorname{Sesgo}(\\hat{\\beta}_{1}) = \\mathop{\\mathrm{E}}(\\hat{\\beta}_{1}) - \\beta_{1} = \\beta_2 \\delta_{12}\n\n\n\n\nSesgo de variable omitida\nEl estimador de MCO presenta sesgo cuando se cumplen las dos condiciones:\n\n\\beta_2 \\neq 0: las variables omitidas son relevantes.\n\\delta_{12} \\neq 0: las variables omitidas están correlacionadas con alguna de las variables incluidas en la regresión.\n\n\n\nVarianza del estimador MCO\nSi se cumplen los supuestos de Gauss-Markov (RLM.1 a RLM.5): \n  \\mathop{\\mathrm{var}}(\\hat{\\beta}_{j} | \\boldsymbol{x}) = \\frac{\\sigma^{2}}{\\mathop{\\mathrm{SCT}}_{j}(1 - R^2_{j})}\n  \\quad (j = 1, 2, \\dots, k)\n donde:\n\n\\mathop{\\mathrm{SCT}}_j = \\sum_{i=1}^n(x_{ij} - \\bar{x}_j)^2\nR^2_j es el coeficiente de determinación de una regresión de x_j sobre el resto de explicativas.\n\n\n\nEficiencia\nTeorema de Gauss-Markov: Si se cumplen los supuestos de Gauss-Markov, el estimador MCO es el estimador lineal insesgado óptimo.\n\nUn estimador lineal es una función lineal de los valores de la variable dependiente: \n   \\hat{\\beta}_j = \\sum_{i=1}^n w_{ij} y_i\n\nDentro de un grupo de estimadores, el estimador óptimo es el que tiene menor varianza muestral.\n\n\n\nDistribución muestral\nBajo los supuestos del modelo clásico de regresión lineal (RLM.1 a RLM.6): \n  \\hat{\\beta}_{j} | \\boldsymbol{x} \\sim \\mathop{\\mathrm{Normal}}\\big(\\beta_j, \\mathop{\\mathrm{var}}(\\hat{\\beta}_j | \\boldsymbol{x})\\big)\n  \\quad (j = 0, 1, \\dots, k)"
  },
  {
    "objectID": "mcrl.html#contraste-de-hipótesis-sobre-un-parámetro",
    "href": "mcrl.html#contraste-de-hipótesis-sobre-un-parámetro",
    "title": "1  El modelo clásico de regresión lineal",
    "section": "1.4 Contraste de hipótesis sobre un parámetro",
    "text": "1.4 Contraste de hipótesis sobre un parámetro\n\nHipótesis nula e hipótesis alternativa\n\nHipótesis nula, H_0: la hipótesis para la que queremos determinar si hay o no suficiente evidencia en contra.\nHipótesis alternativa, H_1: la hipótesis que aceptaremos si rechazamos la hipótesis nula.\n\n\n\nContrastes acerca de un parámetro\n\nModelo poblacional: \n  y = \\beta_{0} + \\beta_{1} x_{1} + \\dots + \\beta_{j} x_{j} + \\dots +\n  \\beta_{k} x_{k} + u.\n\nHipótesis nula: el valor del parámetro \\beta_j es igual a b_j, \n  H_{0}\\!: \\beta_{j} = b_{j}.\n\n\n\n\nEstadístico de contraste\nEl estadístico \\boldsymbol{t} para la H_0\\!: \\beta_j = b_j es: \n  t = \\frac{\\hat{\\beta}_{j} - b_{j}}{\\mathop{\\mathrm{et}}(\\hat{\\beta}_{j})},\n donde \\mathop{\\mathrm{et}}(\\hat{\\beta}_{j}) es el error típico del estimador del parámetro \\beta_j.\n\n\nDistribución bajo la hipótesis nula\n+MATS: figcol fig-02_1027-tdens-*.pdf 0.5\nSi se cumplen los supuestos del modelo clásico de regresión lineal, cuando la hipótesis nula es cierta el estadístico t se distribuye como una variable aleatoria t con n - k - 1 grados de libertad: \n  t \\sim t_{n-k-1}.\n\n\n\nLa hipótesis alternativa\nLa hipótesis alternativa es, usualmente, la negación de la hipótesis nula: \n  H_{1}\\!: \\beta_{j} \\neq b_{j}.\n\n\n\nLa región de rechazo\n+MATS: figcol fig-02_1027-ttest-rejection-area-*.pdf 0.5\nLa hipótesis alternativa determina la región de rechazo: valores grandes en valor absoluto del estadístico t son evidencia en contra de la hipótesis nula.\n\n\nNivel de significación\n\nEl nivel de significación determina el tamaño de la región de rechazo.\nEs la probabilidad de rechazar la hipótesis nula cuando ésta es cierta: \n  \\alpha = \\Pr(\\text{rechazar $H_{0}$}\\,|\\,\\text{$H_{0}$ es cierta}).\n\n\n\n\nValores críticos\n\nLos valores críticos delimitan la región de rechazo.\nSe determinan a partir de la distribución bajo la hipótesis nula y del nivel de significación.\n\n\n\nValores críticos de un contraste de dos colas\n+MATS: figcol fig-02_1027-ttest-two-tails-*.pdf 0.5\nEn un contraste de H_0\\!: \\beta_j = b_j frente a H_1\\!: \\beta_j \\neq b_j, el valor crítico c_{\\alpha} cumple: \n  \\Pr(|t_{n - k - 1}| &gt; c_{\\alpha/2}) = \\alpha.\n\n\n\nRegla de decisión\n+MATS: figcol fig-02_1027-ttest-h0-rejected-*.pdf 0.5\nRegla de decisión: se rechaza H_0 (en favor de H_1) a un nivel de significación \\alpha cuando el estadístico t cae en la región de rechazo: \n  |t| &gt; c_{\\alpha/2}.\n\n\n\nNo rechazo\n+MATS: figcol fig-02_1027-ttest-h0-not-rejected-*.pdf 0.5\nDecimos que no se rechaza la H_0 cuando el estadístico t se sitúa fuera de la región de rechazo. Evitamos expresiones como “aceptar la hipótesis nula”.\n\n\nValor-p\nEl valor-\\boldsymbol{p} es la probabilidad, cuando H_0 es cierta, de observar un valor del estadístico de contraste al menos tan grande como el que se ha obtenido.\n\n\nValor-p de un contraste de dos colas\n+MATS: figcol fig-02_1027-ttest-p-value-*.pdf 0.5\nEl valor-p se corresponde con el área de las dos colas delimitadas por el estadístico de contraste: \n  \\text{valor-}p = \\Pr(|t_{n-k-1}| &gt; |t|).\n\n\n\nRegla de decisión con el valor-p\nRegla de decisión: se rechaza H_0 a un nivel de significación \\alpha cuando el valor-p es menor que el nivel de significación \\alpha: \n  \\text{valor-}p &lt; \\alpha.\n\n\n\nInterpretación del valor-p\n\nEl valor-p no es la probabilidad de que H_0 sea cierta.\nMide la intensidad de la evidencia en contra de H_0: cuanto más pequeño es el valor-p, menos soportan los datos la H_0.\n\n\n\nResumen\n\nContraste de H_0\\!: \\beta_j = b_j frente a H_{1}\\!: \\beta_{j} \\neq b_{j}.\nEstadístico de contraste: \n  t = \\frac{\\hat{\\beta}_{j} - b_{j}}{\\mathop{\\mathrm{et}}(\\hat{\\beta}_{j})}.\n\nSe rechaza H_0 a un nivel de significación \\alpha si:\n\n|t| &gt; c_{\\alpha/2}, donde c_{\\alpha/2} se obtiene de la distribución t_{n - k - 1}; o\n\\text{valor-}p &lt; \\alpha, donde el valor-p se obtiene de la distribución t_{n - k - 1}.\n\n\n\n\nContrastes de significación individual\n\nEs un caso especial del contraste t de dos colas: H_0\\!: \\beta_j = 0 frente a H_{1}\\!: \\beta_{j} \\neq 0.\nEl estadístico de contraste es el cociente \\boldsymbol{t}: \n  t = \\frac{\\hat{\\beta}_{j}}{\\mathop{\\mathrm{et}}(\\hat{\\beta}_{j})}.\n\nCuando se rechaza la H_0 se suele usar expresiones como x_j es estadísticamente significativa al nivel \\alpha.\n\n\n\nContrastes de una cola (I)\n+MATS: figcol fig-02_1027-ttest-one-tail-pos-*.pdf 0.5\n\nH_0\\!: \\beta_j = b_j frente a H_{1}\\!: \\beta_{j} &gt; b_{j}.\nEstadístico de contraste: \n  t = (\\hat{\\beta}_{j} - b_{j})/\\mathop{\\mathrm{et}}(\\hat{\\beta}_{j}).\n\nSe rechaza H_0 si t &gt; c_{\\alpha}, donde: \n  \\Pr(t_{n-k-1} &gt; c_{\\alpha}) = \\alpha.\n\n\n\n\nContrastes de una cola (y II)\n+MATS: figcol fig-02_1027-ttest-one-tail-neg-*.pdf 0.5\n\nH_0\\!: \\beta_j = b_j frente a H_{1}\\!: \\beta_{j} &lt; b_{j}.\nEstadístico de contraste: \n  t = (\\hat{\\beta}_{j} - b_{j})/\\mathop{\\mathrm{et}}(\\hat{\\beta}_{j}).\n\nSe rechaza H_0 si t &lt; -c_{\\alpha}, donde: \n  \\Pr(t_{n-k-1} &lt; -c_{\\alpha}) = \\alpha.\n\n\n\n\nContrastes de una cola y valores-p\n\nLos valores-p que presentan por defecto los programas estadísticos son válidos para contrastes de dos colas.\nSi el estadístico t está en la misma cola de la región de rechazo, el valor-p será la mitad del que imprime el programa informático.\nSi el estadístico t está en la cola opuesta a la de la región de rechazo, el valor-p será mayor que 0.5 y no se rechazará la H_0 a ninguno de los niveles de significación habituales."
  },
  {
    "objectID": "mcrl.html#intervalos-de-confianza",
    "href": "mcrl.html#intervalos-de-confianza",
    "title": "1  El modelo clásico de regresión lineal",
    "section": "1.5 Intervalos de confianza",
    "text": "1.5 Intervalos de confianza\n\nIntervalos de confianza\nUn intervalo de confianza para el parámetro \\beta_j a un nivel de confianza de 1-\\alpha es un intervalo (\\overline{\\beta}_j, \\underline{\\beta}_j) que contendrá el valor del parámetro \\beta_j con una probabilidad 1-\\alpha: \n\\Pr(\\underline{\\beta}_j &lt; \\beta_j &lt; \\overline{\\beta}_j) = 1 - \\alpha.  \n\n\n\nEstimación por intervalos\nEstimación por intervalos: en vez de obtener un único valor (estimación puntual), calculamos un intervalo de valores para el parámetro \\beta_j.\n+MATS: fig fig-02_1027-conf-interval-*.pdf\n\n\nContraste de hipótesis\nContraste de hipótesis: Rechazamos H_0\\!: \\beta_j = b_j frente a H_1\\!: \\beta_j \\neq b_j a un nivel de significación \\alpha si b_j no está incluido en el intervalo de confianza (\\overline{\\beta}_j, \\underline{\\beta}_j) construido a un nivel de confianza 1 - \\alpha.\n+MATS: fig fig-02_1027-conf-interval-test-*.pdf\n\n\nCálculo\nSi se cumplen los supuestos del modelo clásico de regresión lineal, RLM.1 a RLM.6, los límites de un intervalo a un nivel de confianza 1 - \\alpha para el parámetro \\beta_j son: \n  \\hat{\\beta}_j \\pm c_{\\alpha/2} \\cdot \\mathop{\\mathrm{et}}(\\hat{\\beta}_j),\n donde c_{\\alpha/2} es el valor crítico de dos colas de una distribución t_{n-k-1} a un nivel de significación \\alpha.\n\n\nSimetría\nEl intervalo de confianza del parámetro \\beta_j es simétrico y su centro es la estimación \\hat{\\beta}_j.\n+MATS: fig fig-02_1027-conf-interval-estimation-*.pdf"
  },
  {
    "objectID": "mcrl.html#contrastes-de-hipótesis-lineales-sobre-los-parámetros",
    "href": "mcrl.html#contrastes-de-hipótesis-lineales-sobre-los-parámetros",
    "title": "1  El modelo clásico de regresión lineal",
    "section": "1.6 Contrastes de hipótesis lineales sobre los parámetros",
    "text": "1.6 Contrastes de hipótesis lineales sobre los parámetros\n\nHipótesis lineales sobre los parámetros\n\nLa hipótesis nula se compone de una o más combinaciones lineales de los parámetros. Por ejemplo:\n\nH_0\\!: \\beta_3 = 0, \\beta_4 = 1.\nH_0\\!: \\beta_3 + \\beta_4 = 1, \\beta_5 = -1.\n\nLa hipótesis alternativa es la negación de la nula: al menos una de las igualdades no se cumple.\n\n\n\nRestricciones de exclusión\n\nRestricciones de exclusión múltiple: las pendientes de q regresores son nulas: \n  H_{0}\\!: \\beta_{1} = \\beta_{2} = \\dots = \\beta_{q} = 0.\n\nEn la hipótesis alternativa al menos una de las pendientes \\beta_{1}, \\beta_{2}, \\dots, \\beta_{q} es distinta de 0: \n  H_{1}\\!: \\text{no $H_0$}.\n\n\n\n\nModelo no restringido\nModelo no restringido: los parámetros pueden tomar cualquier valor y es válido tanto bajo la H_0 como bajo la H_1: \n  y = \\beta_{0} + \\beta_{1} x_{1} + \\dots + \\beta_{q} x_{q} +\n  \\beta_{q+1} x_{q+1} + \\dots +  \\beta_{k} x_{k} + u.\n\n\n\nModelo restringido\nModelo restringido: los parámetros \\beta_1, \\dots, \\beta_q son iguales a 0 y sólo es válido bajo la H_0: \n  y = \\beta_{0} + \\beta_{q+1} x_{q+1} + \\dots +  \\beta_{k} x_{k} + u.\n\n\n\nBondad del ajuste\n\nSi las restricciones de exclusión son ciertas, no debe haber mucha diferencia entre el ajuste del modelo restringido y el del modelo no restringido.\nEl contraste se basará en la comparación de las sumas de cuadrados de los residuos del modelo restringido, \\mathop{\\mathrm{SCR}}_{r}, y del modelo no restringido, \\mathop{\\mathrm{SCR}}_{nr}.\n\n\n\nEstadístico de contraste\nEl estadístico de contraste es el estadístico \\boldsymbol{F}: \n  F = \\frac{(\\mathop{\\mathrm{SCR}}_{r}- \\mathop{\\mathrm{SCR}}_{nr}) / q}{\\mathop{\\mathrm{SCR}}_{nr}/ (n - k - 1)}.\n\n\n\nDistribución bajo la hipótesis nula\nSi se cumplen los supuestos del modelo clásico de regresión lineal, cuando la hipótesis nula es cierta el estadístico F se distribuye como una variable aleatoria F con q y n - k - 1 grados de libertad: \n  F \\sim F_{q, n-k-1}.\n\n\n\nRegión de rechazo\n+MATS: figcol fig-02_1027-ftest-rejection-area-*.pdf 0.5\n\nValor crítico a un nivel de significación \\alpha: \n  \\Pr(F_{q, n-k-1} &gt; c_{\\alpha}) = \\alpha.\n\nValores del estadístico F mayores que c_{\\alpha} representan evidencia en contra de la hipótesis nula.\n\n\n\nLa regla de decisión\nRegla de decisión: se rechaza H_0 a un nivel de significación \\alpha cuando:\n\nEl estadístico F cae en la región de rechazo: \n  F &gt; c_{\\alpha}.\n\nEl valor-p es menor que el nivel de significación \\alpha. El valor-p es: \n  \\text{valor-}p = \\Pr(F_{q, n-k-1} &gt; F).\n\n\n\n\nEl contraste F y el R^2\nPara contrastar restricciones de exclusión el estadístico F puede calcularse a partir de los coeficientes de determinación del modelo restringido, R^2_r, y del modelo no restringido, R^2_{nr}: \n  F = \\frac{(R^2_{nr}- R^2_r) / q}{(1 - R^2_{nr}) / (n - k - 1)}.\n\n\n\nSignificación de la regresión\n\nSignificación conjunta de la regresión: contraste de la hipótesis nula de que todas las pendientes son nulas: \n  H_0\\!: \\beta_1 = \\beta_2 = \\dots = \\beta_k = 0.\n\nEl estadístico F en términos del coeficiente de determinación es: \n  F = \\frac{R^2/ k}{(1 - R^2) / (n - k - 1)}.\n\n\n\n\nRestricciones lineales generales\n\nSe pueden usar los estadísticos F basados en las sumas de cuadrados de los residuos para contrastar restricciones lineales generales.\nLas fórmulas basadas en los coeficientes de determinación sólo pueden usarse si los modelos restringido y no restringido tienen la misma variable dependiente.\nCualquier conjunto de restricciones lineales siempre puede transformarse en restricciones de exclusión mediante una reparametrización del modelo."
  },
  {
    "objectID": "mcrl.html#comment-predicción",
    "href": "mcrl.html#comment-predicción",
    "title": "1  El modelo clásico de regresión lineal",
    "section": "1.7 COMMENT Predicción",
    "text": "1.7 COMMENT Predicción\n\nPredicción puntual\n\nPredicción del valor medio.\nPredicción del valor.\n\n\n\nIntervalos de predicción\n\nValor medio.\nValor de la variable dependiente."
  },
  {
    "objectID": "asymp.html#introducción",
    "href": "asymp.html#introducción",
    "title": "2  Propiedades asintóticas del estimador MCO",
    "section": "2.1 Introducción",
    "text": "2.1 Introducción\n\nEl supuesto de normalidad\nEl supuesto RLM.6 del modelo clásico de regresión requiere la normalidad del término de error: \nu \\sim \\mathop{\\mathrm{Normal}}(0, \\sigma^{2}).\n\n\n\nInferencia en el modelo clásico\nEl supuesto de normalidad permite conocer la distribución de los contrates de hipótesis bajo la hipótesis nula.\n\n\nFalta de normalidad\nEn muchos casos el supuesto de normalidad no es muy razonable:\n\nVariable dependiente sólo toma valores positivos.\nVariable dependiente sólo toma algunos valores discretos.\n\n\n\nTeoría asintótica\nLa teoría asintótica estudia el comportamiento de estimadores y contrastes de hipótesis cuando el tamaño muestral crece indefinidamente.\n\n\nTeoremas sobre límites\nBajo ciertas condiciones, secuencias de variables aleatorias obtenidas por muestreo aleatorio cumplen conforme n \\to \\infty:\n\nLeyes de grandes números: establecen la convergencia de medias muestrales a la media poblacional.\nTeoremas del límite central: establecen la distribución asintótica de las medias muestrales.\n\n\n\nPropiedades asintóticas\n\nConsistencia.\nDistribución asintótica.\nEficiencia asintótica.\n\n\n\nConsistencia\nUn estimador es consistente si conforme aumenta el tamaño muestral converge en probabilidad al parámetro poblacional: \n\\mathop{\\mathrm{plim}}\\hat{\\beta}_j = \\beta_j.\n\nIntuitivamente, cuanto mayor es el tamaño muestral más cerca está un estimador consistente del parámetro poblacional.\n\n\nConsistencia e insesgadez\n\nLas propiedades de consistencia e insesgadez son independientes: una no implica la otra.\nUn estimador es consistente si es insesgado y su varianza decrece y converge a 0 conforme aumenta el tamaño muestral.\n\n\n\nDistribución asintótica\n\nLa distribución asintótica de un estimador es su distribución en el límite.\nProporciona una aproximación a la distribución del estimador para un tamaño muestral finito.\n\n\n\nNormalidad asintótica\n\nUn estimador es asintóticamente normal si su distribución asintótica es normal.\nNo es necesaria la normalidad del término de error, basta con que se cumplan las condiciones de un teorema del límite central.\n\n\n\nConsideraciones prácticas\n\nLas propiedades asintóticas no requieren el supuesto de normalidad.\nRequisitos técnicos: momentos finitos hasta orden 4. Supondremos implícitamente que se cumple.\nLa calidad de las aproximaciones asintóticas suele mejorar con el tamaño muestral. Pero, ¿cuántas observaciones son necesarias en la práctica?"
  },
  {
    "objectID": "asymp.html#propiedades-asintóticas-de-mco",
    "href": "asymp.html#propiedades-asintóticas-de-mco",
    "title": "2  Propiedades asintóticas del estimador MCO",
    "section": "2.2 Propiedades asintóticas de MCO",
    "text": "2.2 Propiedades asintóticas de MCO\n\nConsistencia\n\nMCO es consistente si se cumplen los supuestos RLM.1 a RLM.4.\nEl supuesto RLM.4, E(u|\\boldsymbol{x}) = 0, puede relajarse y basta con: \n  \\mathop{\\mathrm{E}}(u) = 0 \\text{\\ y} \\mathop{\\mathrm{cov}}(x_{j}, u) \\text{\\ para $j = 1, 2, \\dots, k$}.\n\n\n\n\nInconsistencia\n\nLa inconsistencia de \\hat{\\beta}_i es la diferencia entre el límite del estimador y el parámetro poblacional. En el caso de MCO: \n  \\mathop{\\mathrm{plim}}\\hat{\\beta}_{i} - \\beta_{i} = \\frac{\\mathop{\\mathrm{cov}}(x_{i}, u)}{\\mathop{\\mathrm{var}}(x_{i})}.\n\nMCO es inconsistente si el término de error y, al menos, una de las explicativas están correlacionados.\n\n\n\nVariable relevante omitida\n\nSe cumplen los supuestos RLM.1 a RLM.4 en el modelo poblacional: \n  y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + u.\n\nSi se omite la variable x_2, el sesgo del estimador MCO de \\beta_1 es: \n  \\mathop{\\mathrm{plim}}\\hat{\\beta}_{1} - \\beta_{1} = \\beta_2 \\delta_{21},\n donde \\delta_{21} = \\mathop{\\mathrm{cov}}(x_1, x_2) / \\mathop{\\mathrm{var}}(x_1), mide la intensidad de la asociación de las variables x_2 y x_1.\nOmitir una variable relevante, \\beta_2 \\neq 0, causa inconsistencia si la variable omitida está correlacionada con alguna de las explicativas, \\delta_{21} \\neq 0.\n\n\n\nVarianza de los estimadores MCO\nSi se cumplen los supuestos de Gauss-Markov, RLM.1 a RLM.5:\n\n\\hat{\\sigma}^2 es un estimador consistente de \\mathop{\\mathrm{var}}(u).\nLas fórmulas para estimar las varianzas de los estimadores MCO son válidas asintóticamente: \n  \\widehat{\\mathop{\\mathrm{var}}}(\\hat{\\beta}_{j}) = \\frac{\\hat{\\sigma}^2}{\\mathop{\\mathrm{SCT}}_{j}(1-R^2_{j})}.\n\n\n\n\nNormalidad asintótica del estimador MCO\nSi se cumplen los supuestos de Gauss-Markov, RLM.1 a RLM.5, la distribución asintótica del estimador MCO es normal: \n    \\frac{\\hat{\\beta}_{j} - \\beta_{j}}{\\mathop{\\mathrm{dt}}(\\hat{\\beta}_{j})} \\stackrel{\\mathrm{a}}{\\sim}\\mathop{\\mathrm{Normal}}(0, 1),\n    \\text{\\ para $j = 0, 1, 2, \\dots, k$}."
  },
  {
    "objectID": "asymp.html#inferencia-asintótica",
    "href": "asymp.html#inferencia-asintótica",
    "title": "2  Propiedades asintóticas del estimador MCO",
    "section": "2.3 Inferencia asintótica",
    "text": "2.3 Inferencia asintótica\n\nContraste de hipótesis en muestras grandes\n\nNo es necesario el supuesto de normalidad del término de error para usar los contrastes t y F.\nDisponemos de otros contrastes que sólo son válidos asintóticamente.\n\n\n\nContrastes t\n\nSi se cumplen los supuestos de Gauss-Markov, podemos usar la distribución t para determinar valores críticos y los valores-p de los contrates t. \n  \\frac{\\hat{\\beta}_{j} - \\beta_{j}}{\\mathop{\\mathrm{et}}(\\hat{\\beta}_{j})} \\stackrel{\\mathrm{a}}{\\sim}t_{n - k - 1}.\n\n\n\n\nIntervalos de confianza\n\nSi se cumplen los supuestos de Gauss-Markov, podemos usar la fórmula habitual para obtener un intervalo a un nivel de confianza 1 - \\alpha para el parámetro \\beta_j: \n  \\hat{\\beta}_j \\pm c_{\\alpha/2} \\cdot \\mathop{\\mathrm{et}}(\\hat{\\beta}_j),\n donde c_{\\alpha/2} es el valor crítico de dos colas de una distribución t_{n-k-1} a un nivel de significación \\alpha.\n\n\n\nContrastes F\n\nSi se cumplen los supuestos de Gauss-Markov, podemos usar la distribución F para determinar valores críticos y los valores-p de los contrates F. \n  \\frac{(\\mathop{\\mathrm{SCR}}_{r}- \\mathop{\\mathrm{SCR}}_{nr}) / q}{\\mathop{\\mathrm{SCR}}_{nr}/ (n - k - 1)}  \\stackrel{\\mathrm{a}}{\\sim}F_{q,\\, n - k - 1}.\n\n\n\n\nOtros contrastes asintóticos\nSe han propuesto otros principios de contrastación de restricciones generales sobre los parámetros y que sólo tienen validez asintótica:\n\nContrastes de Wald.\nContrastes de multiplicadores de Lagrange.\nContrastes de la razón de verosimilitudes.\n\n\n\nContrastes de Wald\n\nLos contrastes de Wald pueden interpretarse como una generalización de los contrastes t.\nSólo requieren la estimación del modelo no restringido.\nBajo la H_0 se distribuyen como una \\chi^2 con q grados de libertad. También se usan versiones de este contraste que se distribuyen como una F_{q,\\, n - k - 1}.\n\n\n\nContrastes de multiplicadores de Lagrange\n\nPara utilizar un contraste de multiplicadores de Lagrange, \\mathop{\\mathrm{LM}}, sólo es necesaria la estimación del modelo restringido.\nSon especialmente útiles cuando la estimación del modelo no restringido es difícil.\nFrecuentemente se calculan a partir de una regresión auxiliar.\n\n\n\nContraste de variables omitidas\n\nModelo de regresión poblacional que cumple RLM.1 a RLM.5: \n  y = \\beta_0 + \\beta_1 x_1  + \\beta_2 x_2  + \\beta_3 x_3  + \\beta_4 x_4 + u.\n\nEstamos interesados en contrastar la hipótesis nula \n  H_0\\!: \\beta_3 = \\beta_4 = 0.\n\n\n\n\nContraste \\mathop{\\mathrm{LM}} de variables omitidas\n\nEstimación del modelo restringido por MCO: \n  y = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1  + \\hat{\\beta}_2 x_2 + \\hat{u}.\n\nRegresión auxiliar usando los residuos como variable dependiente y todas las explicativas, tanto incluidas como excluidas: \n  \\hat{u}\\text{\\ sobre $x_1, x_2, x_3, x_4$}.\n\nEl estadístico de contraste es \\mathop{\\mathrm{LM}}= n R^2_{aux}: el número de observaciones multiplicado por el R^2 de la regresión auxiliar.\n\n\n\nRegla de decisión\nSi la hipótesis nula que impone q restricciones es correcta:\n\nEl estadístico \\mathop{\\mathrm{LM}} se distribuye asintóticamente como una \\chi^2 con q grados de libertad: \n  n R^2_{aux} \\stackrel{\\mathrm{a}}{\\sim}\\chi^2_q.\n\nUna versión alternativa del contraste utiliza un estadístico F para contrastar restricciones de exclusión en la regresión auxiliar.\n\n\n\nContrastes de razón de verosimilitudes\n\nSe pueden interpretar como una generalización del contraste F.\nRequieren la estimación del modelo restringido y del modelo no restringido.\nSon útiles cuando en modelos no lineales que se estiman con métodos diferentes a MCO."
  },
  {
    "objectID": "het.html#introducción",
    "href": "het.html#introducción",
    "title": "3  Heteroscedasticidad",
    "section": "3.1 Introducción",
    "text": "3.1 Introducción\n\nHomoscedasticidad (I)\n\n\n\n\n\n\n\n(a) Homoscedasticidad\n\n\n\n\n\n\n\n(b) Heteroscedasticidad\n\n\n\n\nFigura 3.1: Dispersión alrededor de la función de regresión poblacional\n\n\nHomoscedasticidad. La dispersión alrededor de la función de regresión poblacional, FRP, es constante: \n  \\mathop{\\mathrm{var}}(u_i | x_i) = \\sigma^2.\n\n\n\nHomoscedasticidad (y II)\n\n\n\n\n\n\n\n(a) Homoscedasticidad\n\n\n\n\n\n\n\n(b) Heteroscedasticidad\n\n\n\n\nFigura 3.2: Dispersión alrededor de la función de regresión poblacional\n\n\nHomoscedasticidad. Todas las observaciones son igual de informativas a la hora de determinar por donde pasa la FRP.\n\n\nHeteroscedasticidad (I)\nHeteroscedasticidad. La dispersión alrededor de la FRP cambia con los valores de la explicativa: \n  \\mathop{\\mathrm{var}}(u_i | x_i) = \\sigma^2_i.\n\n\n\nHeteroscedasticidad (y II)\nHeteroscedasticidad. Algunas observaciones contienen menos información para estimar la FRP."
  },
  {
    "objectID": "het.html#consecuencias-de-la-heteroscedasticidad",
    "href": "het.html#consecuencias-de-la-heteroscedasticidad",
    "title": "3  Heteroscedasticidad",
    "section": "3.2 Consecuencias de la heteroscedasticidad",
    "text": "3.2 Consecuencias de la heteroscedasticidad\n\nInsesgadez y consistencia\n\nLa heteroscedasticidad no afecta al supuesto RLM.4.\nSi se cumplen los supuestos RLM.1 a RLM.4, el estimador MCO es insesgado y consistente aunque exista heteroscedasticidad.\n\n\n\nVarianza de los estimadores de MCO\nCuando hay heteroscedasticidad, las fórmulas usuales de las varianzas de los estimadores de MCO, \\mathop{\\mathrm{var}}(\\hat{\\beta}), no son válidas incluso en muestras grandes.\n\n\nContraste de hipótesis\nCuando el término de error es heteroscedástico, los procedimientos habituales de contraste de hipótesis dejan de ser válidos incluso en muestras grandes.\n\n\nEficiencia\n\nCon heteroscedasticidad, MCO no es el estimador lineal insesgado óptimo.\nSi se conoce el patrón de la heteroscedasticidad es posible construir estimadores más eficientes que MCO."
  },
  {
    "objectID": "het.html#inferencia-robusta-a-heteroscedasticidad",
    "href": "het.html#inferencia-robusta-a-heteroscedasticidad",
    "title": "3  Heteroscedasticidad",
    "section": "3.3 Inferencia robusta a heteroscedasticidad",
    "text": "3.3 Inferencia robusta a heteroscedasticidad\n\nErrores típicos robustos\nUn procedimiento común con datos de corte transversal:\n\nEstimar los parámetros por MCO.\nModificar el cálculo de los errores típicos de los estimadores para que sean válidos (asintóticamente) haya o no haya heteroscedasticidad.\n\n\n\nVarianza del estimador MCO\n\nSe cumplen los supuestos RLM.1 a RLM.4 en el modelo de regresión simple: \n  y_i = \\beta_0 + \\beta_1 x_i + u_i.\n\nHeteroscedasticidad: \n  \\mathop{\\mathrm{var}}(u_{i} | x_{1i}) = \\sigma^{2}_{i}.\n\nLa varianza del estimador MCO es: \n  \\mathop{\\mathrm{var}}(\\hat{\\beta}_{1}) = \\frac{\\sum(x_{i} - \\bar{x})^{2}\\sigma^2_{i}}{%\n    \\Big[\\sum(x_{i} - \\bar{x})^{2}\\Big]^{2}}.\n\n\n\n\nEstimación robusta de \\mathop{\\mathrm{var}}(\\hat{\\beta})\n\nHalbert White mostró que se puede obtener una estimación consistente de \\mathop{\\mathrm{var}}(\\hat{\\beta}) exista o no heteroscedasticidad: \n  \\widehat{\\mathop{\\mathrm{var}}}_R(\\hat{\\beta}_{1}) =\n  \\frac{\\sum(x_{i} - \\bar{x})^{2} \\hat{u}^2_i}{\n    \\Big[\\sum(x_{i} - \\bar{x})^{2}\\Big]^{2}}.\n\nSe han descrito alternativas a la propuesta original de White que son equivalentes asintóticamente pero que, bajo ciertas condiciones, son superiores en muestras pequeñas.\n\n\n\nContrastes t robustos\nUn estadístico asintóticamente válido para contrastar la hipótesis nula \\beta_j = b se construye con la estimación de MCO, \\hat{\\beta}_j, y el error típico robusto a heteroscedasticidad, \\mathop{\\mathrm{et}}_R(\\hat{\\beta}_j): \n  t_{j} = \\frac{\\hat{\\beta}_{j} - b}{\\mathop{\\mathrm{et}}_{R}(\\hat{\\beta}_{j})}.\n\n\n\nContrastes de hipótesis lineales\n\nEn presencia de heteroscedasticidad no es válido el estadístico F que compara las \\mathop{\\mathrm{SCR}} o los R^2 de los modelos restringido y no restringido. Tampoco existe una versión robusta de este contraste.\nPara contrastar restricciones lineales generales sobre los parámetros del modelo, utilizamos contrastes de Wald construidos con estimaciones de \\mathop{\\mathrm{var}}(\\hat{\\boldsymbol{\\beta}}) robustas a heteroscedasticidad."
  },
  {
    "objectID": "het.html#contrastes-de-heteroscedasticidad",
    "href": "het.html#contrastes-de-heteroscedasticidad",
    "title": "3  Heteroscedasticidad",
    "section": "3.4 Contrastes de heteroscedasticidad",
    "text": "3.4 Contrastes de heteroscedasticidad\n\n¿Por qué?\n\nSi no hay grandes problemas de heteroscedasticidad no sería necesario usar errores típicos robustos.\nMCO es el estimador óptimo (en cierto sentido) cuando el término de error es homoscedástico. Si hay heteroscedasticidad quizá queramos emplear estimadores más eficientes que MCO.\n\n\n\nContrastes de especificación\n\nTratamos de verificar si se cumple alguno de los supuestos sobre el modelo poblacional.\nFrecuentemente la hipótesis nula afirma que no hay problemas de especificación y el estimador MCO tiene buenas propiedades.\nA menudo se utiliza un contraste de multiplicadores de Lagrange.\n\n\n\nContrastes de heteroscedasticidad\n\nLa hipótesis nula es el supuesto RLM.5: \n  H_0\\!: \\mathop{\\mathrm{var}}(u_i | x_{1i}, \\dots, x_{ki}) = \\sigma^2.\n\nBajo la hipótesis alternativa hay heteroscedasticidad: \n  H_1\\!: \\mathop{\\mathrm{var}}(u_i | x_{1i}, \\dots, x_{ki}) = \\sigma^2_i.\n\n\n\n\nEnfoque general\n\nPodemos reescribir la hipótesis nula como: \n  H_0\\!: \\mathop{\\mathrm{E}}(u^2_i | x_{1i}, \\dots, x_{ki}) = \\sigma^2.\n\nSi no se cumple H_0 la esperanza condicional de u^2 depende de los regresores. Si la relación fuera lineal: \n   u^2  = \\delta_0   + \\delta_1 x_{1} +  \\dots + \\delta_k x_{k} + \\text{error}.\n\nSi observáramos u_i podríamos contrastar homoscedasticidad por medio de la hipótesis nula: \n  H_0\\!: \\delta_1 = \\delta_2 = \\dots = \\delta_k = 0\n\n\n\n\nRegresión auxiliar\n\nLos contrastes modernos de heteroscedasticidad utilizan una regresión auxiliar cuya variable dependiente es \\hat{u}^2.\nCada contraste se diferencia por los k_{aux} regresores que se incluyen.\nEl estadístico \\mathop{\\mathrm{LM}}= n R^2_{aux} se distribuye bajo la H_0 como una \\chi^2_{k_{aux}}.\nTambién puede usarse el contraste de significación de la regresión: \n  F = \\frac{R^2_{aux} / k_{aux}}{(1 - R^2_{aux}) / (n - k_{aux} - 1)}.\n\n\n\n\nContraste de Breusch-Pagan\n\nEn la actualidad, se usa comúnmente la variante, debida a Koenker, del contraste que propusieron Breusch y Pagan.\nEn la regresión auxiliar se usan como regresores las k variables explicativas del modelo de regresión original.\n\n\n\nContraste de Breusch-Pagan: ejemplo\n\nModelo de regresión con 3 explicativas: \n  y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{3i} + u_i.\n\nDespués de estimar por MCO se obtienen los residuos \\hat{u}.\nRegresión auxiliar: \n  \\hat{u}^2_i = \\delta_0 + \\delta_1 x_{1i} + \\delta_2 x_{2i} + \\delta_3 x_{3i} + \\text{error}.\n\nSe estima por MCO la regresión auxiliar y se calcula el estadístico \\mathop{\\mathrm{LM}} o el estadístico F de la regresión auxiliar.\n\n\n\nContraste de White: motivación\nWhite demuestra que la heteroscedasticidad sólo es problemática si la varianza condicional de u depende de:\n\nlas variables explicativas: x_1, x_2, \\dots, x_k;\nsus cuadrados: x^2_1, x^2_2, \\dots, x^2_k; o\nsus productos cruzados: x_1 \\cdot x_2, x_1 \\cdot x_3, \\dots\n\n\n\nContraste de White\n\nLa regresión auxiliar incluye las explicativas, sus cuadrados y sus productos cruzados.\nPueden aparecer problemas de multicolinealidad perfecta en la regresión auxiliar, especialmente si hay variables ficticias. En ese caso, habría que eliminar de la regresión auxiliar los regresores redundantes.\n\n\n\nContraste de White: ejemplo\n\nModelo de regresión con 3 explicativas: \n  y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{3i} + u_i.\n\nDespués de estimar por MCO se obtienen los residuos \\hat{u}.\nRegresión auxiliar: \n\\begin{align*}\n  \\hat{u}^2_i\n  = \\delta_0 &+ \\delta_1 x_{1i} + \\delta_2 x_{2i} + \\delta_3 x_{3i}\n  + \\delta_4 x^{2}_{1i} + \\delta_{5} x^{2}_{2i} + \\delta_6 x^{2}_{3i} \\\\\n  &+ \\delta_7 x_{1i} x_{2i} + \\delta_8 x_{1i} x_{3i} + \\delta_9 x_{2i}x_{3i}\n   +  \\text{error}.\n\\end{align*}\n\nSe estima por MCO la regresión auxiliar y se calcula el estadístico \\mathop{\\mathrm{LM}} o el estadístico F de la regresión auxiliar.\n\n\n\nContraste de White: alternativas (I)\n\nEn la regresión auxiliar del contraste de White hay un gran número de parámetros, lo que puede traducirse en baja potencia (capacidad de detectar heteroscedasticidad cuando realmente está presente).\nPara mitigar ese problema, en ocasiones se omiten los productos cruzados en la regresión auxiliar: \n  \\hat{u}^2_i\n  = \\delta_0 + \\delta_1 x_{1i} + \\delta_2 x_{2i} + \\delta_3 x_{3i}\n  + \\delta_4 x^{2}_{1i} + \\delta_{5} x^{2}_{2i} + \\delta_6 x^{2}_{3i}\n    +  \\text{error}.\n\n\n\n\nContraste de White: alternativas (y II)\nWooldridge propone una variante del contraste de White donde sólo se incluyen en la regresión auxiliar las predicciones de MCO, \\hat{y}, y sus cuadrados, \\hat{y}^2: \n  \\hat{u}^2_i = \\delta_0 + \\delta_1 \\hat{y}_{i} + \\delta_2 \\hat{y}^2_{i} +  \\text{error}."
  },
  {
    "objectID": "het.html#ineficiencia-de-mco",
    "href": "het.html#ineficiencia-de-mco",
    "title": "3  Heteroscedasticidad",
    "section": "3.5 Ineficiencia de MCO",
    "text": "3.5 Ineficiencia de MCO\n\nHeteroscedasticidad\nAquí iba la figura Figura 3.2 (a).\nCon heteroscedasticidad la dispersión alrededor de la FRP cambia con los valores de la explicativa.\n\n\nIneficiencia de MCO\nAquí iba la figura Figura 3.2 (b).\nLa ineficiencia de MCO se debe a que se tratan por igual todas las observaciones, aunque no todas contienen información igual de precisa para estimar la FRP.\n\n\nEstimación eficiente\nEstimación eficiente con heteroscedasticidad:\n\nSe asigna un peso diferente a cada observación.\nLas observaciones más imprecisas reciben ponderaciones menores.\n\n\n\nMínimos Cuadrados Generalizados\nEl estimador eficiente cuando existe heteroscedasticidad pertenece a la familia de Mínimos Cuadrados Generalizados (MCG). Los estimadores MCG consisten en:\n\nTransformar el modelo de forma que se cumplan los supuestos de Gauss-Markov.\nEstimar el modelo transformado por MCO."
  },
  {
    "objectID": "het.html#mínimos-cuadrados-ponderados",
    "href": "het.html#mínimos-cuadrados-ponderados",
    "title": "3  Heteroscedasticidad",
    "section": "3.6 Mínimos cuadrados ponderados",
    "text": "3.6 Mínimos cuadrados ponderados\n\nModelo de regresión\nEl modelo de regresión cumple los supuestos RLM.1 a RLM.4: \n  y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}\n  + \\dots + \\beta_k x_{ki} + u_i.\n\n\n\nHeteroscedasticidad\n\nLa varianza del término de error puede expresarse como: \n  \\mathop{\\mathrm{var}}(u_i | x_{1i}, x_{2i}, \\dots, x_{ki}) =\n  \\sigma^2 h(x_{1i}, x_{2i}, \\dots, x_{ki}) =\n  \\sigma^2 h(\\boldsymbol{x}_i).\n\nLa función h(\\boldsymbol{x}_i) toma siempre valores positivos y expresa la relación entre las explicativas y la varianza del término de error.\nLa constante desconocida \\sigma^2 es positiva.\n\n\n\nHeteroscedasticidad conocida\n\nSupondremos que la función h(\\boldsymbol{x}_i) es conocida y no depende de parámetros desconocidos.\n\n\n\nTransformación del modelo\n\nSi conocemos h_i = h(\\boldsymbol{x}_i) podemos dividir el modelo original por \\sqrt{h_i}.\nEl término de error del modelo transformado, u^*_i = u_i / \\sqrt{h_i}, es homoscedástico: \n\\begin{align*}\n  \\mathop{\\mathrm{var}}(u_i^*| \\boldsymbol{x}_i)\n  &= \\mathop{\\mathrm{E}}\\big((u_i/\\sqrt{h_i})^2| \\boldsymbol{x}_i\\big) \\\\\n  &= (1/h_{i})\\mathop{\\mathrm{E}}(u^2| \\boldsymbol{x}_i) \\\\\n  &= (1/h_i) \\sigma^2 h_i \\\\\n  &= \\sigma^2.\n\\end{align*}\n\n\n\n\nEjemplo\n\nModelo original. Si x_{2i} siempre toma valores positivos y h(x_{1i}, x_{2i}) = x_{2i}: \n\\begin{gather*}\n  y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + u_i, \\\\\n  \\mathop{\\mathrm{var}}(u_i | x_{1i}, x_{2i}) = \\sigma^2 x_{2i}.\n\\end{gather*}\n\nModelo transformado: \n\\begin{gather*}\n  y^*_i = \\beta_0 x^*_{0i} + \\beta_1 x^*_{1i} + \\beta_2 x^*_{2i} + u^*_i, \\\\\n  \\mathop{\\mathrm{var}}(u^*_i | x_{1i}, x_{2i}) = \\sigma^2.\n\\end{gather*}\n donde \n\\begin{equation*}\n  y^*_i = y_i / \\sqrt{x_{2i}},\\quad\n  x^*_{0i} = 1 / \\sqrt{x_{2i}},\\quad\n  x^*_{1i} = x_{1i} / \\sqrt{x_{2i}},\\quad\n  x^*_{2i} = x_{2i} / \\sqrt{x_{2i}}.\n\\end{equation*}\n\n\n\n\nMínimos Cuadrados Ponderados\nEstimador de Mínimos Cuadrados Ponderados, (MCP):\n\nLa varianza del término de error es proporcional a h_i que es una función conocida de variables observables: \n  \\mathop{\\mathrm{var}}(u_i | x_{1i}, x_{2i}, \\dots, x_{ki}) = \\sigma^2 h_i.\n\nSe transforma el modelo dividiendo por \\sqrt{h_i}.\nSe estima el modelo transformado por MCO.\n\n\n\nPropiedades de MCP\n\nSi se cumplen los supuestos RLM.1 a RLM.4 el estimador MCP es insesgado y consistente.\nEl modelo transformado cumple con los supuestos de Gauss-Markov, por lo que MCP es el estimador lineal insesgado óptimo.\n\n\n\nPredicciones y residuos\n\nEl modelo transformado sólo sirve para obtener las estimaciones de MCP de los parámetros: \\tilde{\\beta}_0, \\tilde{\\beta}_1, \\dots, \\tilde{\\beta}_k.\nPara calcular las predicciones usamos las variables originales sin transformar: \n  \\tilde{y}_i = \\tilde{\\beta}_{0}  + \\tilde{\\beta}_{1} x_{1i} + \\dots + \\tilde{\\beta}_{k} x_{ki}.\n\nTambién usamos las variables originales para calcular los residuos: \n  \\tilde{u}_i = y_i - \\tilde{y}_i = y_i - \\tilde{\\beta}_{0}  - \\tilde{\\beta}_{1} x_{1i} - \\dots - \\tilde{\\beta}_{k} x_{ki}.\n\n\n\n\nSuma ponderada de los cuadrados de los residuos\n\nEl estimador MCP minimiza la suma ponderada de residuos al cuadrado: \n  \\sum_{i} \\tilde{u}^{2}_{i}/h_{i} =\n  \\sum_{i} \\big(\n  y_{i} - \\tilde{\\beta}_{0} - \\tilde{\\beta}_{1} x_{1i} - \\dots - \\tilde{\\beta}_{k} x_{ki}\n  \\big)^{2}/h_{i}.\n\nEl peso o ponderación, w_i, que se asigna a cada observación es la inversa de h_i, w_i = 1 / h_i.\n\n\n\nBondad del ajuste\nEn general, no es posible comparar el R^2 obtenido con MCP y el obtenido con MCO:\n\nDiferentes programas informáticos calculan de forma diferente el R^2 de MCP y los distintos métodos no son equivalentes.\nLos R^2 que se calculan para MCP no se pueden interpretar como medidas de bondad del ajuste."
  },
  {
    "objectID": "het.html#mínimos-cuadrados-ponderados-factibles",
    "href": "het.html#mínimos-cuadrados-ponderados-factibles",
    "title": "3  Heteroscedasticidad",
    "section": "3.7 Mínimos cuadrados ponderados factibles",
    "text": "3.7 Mínimos cuadrados ponderados factibles\n\nHeteroscedasticidad desconocida\n\nHasta ahora hemos supuesto que conocemos h_i. ¿Qué podemos hacer en caso contrario?\nAunque no conozcamos la forma exacta de la heteroscedasticidad, con frecuencia sabemos que está relacionada con algunas de las variables del modelo.\nEl estimador de Mínimos Cuadrados Ponderados Factibles (MCPF) se obtiene de forma análoga al estimador de MCP salvo que utiliza una estimación de h_i para obtener el modelo transformado.\n\n\n\nModelización de la heteroscedasticidad\nEspecificaremos un modelo para la varianza del término de error que dependa de variables observables y de parámetros desconocidos. Por ejemplo, podemos escribir: \n  \\mathop{\\mathrm{var}}(u_i | \\boldsymbol{x}_{i}) = \\sigma^2 h(\\boldsymbol{x}_{i})\n donde \n  h(\\boldsymbol{x}_{i}) = \\exp(\\delta_{0} + \\delta_{1} x_{1i} + \\dots + \\delta_{k} x_{ki}),\n y \\delta_0, \\delta_1, \\dots, \\delta_k son parámetros desconocidos. La función exponencial garantiza que h(\\boldsymbol{x}_{i}) &gt; 0 sean cuales sean los valores de los parámetros.\n\n\nEstimación de h_i (I)\nVarianza condicional del término de error: \n  \\mathop{\\mathrm{E}}(u_i^2 | \\boldsymbol{x}_{i}) = \\sigma^2\n  \\exp(\\delta_{0} + \\delta_{1} x_{1i} + \\dots + \\delta_{k} x_{ki}).\n Si observásemos el término de error, u_i, podríamos estimar: \n  \\log(u_i^2) =\n  \\alpha_0 + \\delta_{1} x_{1i} + \\dots +\n  \\delta_{k} x_{ki} + \\text{error}.\n Para poder estimar los parámetros de la regresión anterior, reemplazamos u_i por los residuos de MCO, \\hat{u}_i.\n\n\nEstimación de h_i (y II)\n\nEstimamos por MCO los parámetros de la función de regresión: \n\\log(\\hat{u}_i^2) = \\alpha_0 + \\delta_{1} x_{1i} +\n\\dots + \\delta_{k} x_{ki} + \\text{error}.\n  \nObtenemos \\hat{h}_i a partir de las predicciones de la regresión anterior: \n  \\hat{h}_i = \\exp(\\hat{\\alpha}_0 + \\hat{\\delta}_{1} x_{1i} + \\dots + \\hat{\\delta}_{k} x_{ki}).\n\n\n\n\nMínimos Cuadrados Ponderados Factibles\nProcedimiento para la estimación por Mínimos Cuadrados Ponderados Factibles (MCPF):\n+ATTR_LATEX: :options [label=:, wide=0pt, leftmargin=1em]\n\nSe estima el modelo por MCO y se guardan los residuos \\hat{u}_i.\nSe estima por MCO una regresión de \\log(\\hat{u}^2_i) sobre las explicativas.\nSe obtiene \\hat{h}_i tomando la exponencial de las predicciones de la regresión del paso anterior.\nSe estima el modelo por MCP usando 1/\\hat{h}_i como ponderaciones.\n\n\n\nPropiedades de MCPF\n\nAl usar una estimación de h_i, MCPF no es insesgado ni lineal.\nSi se cumplen los supuestos RLM.1 a RLM.4 el estimador de MCPF es consistente.\nSi la especificación de la heteroscedasticidad es correcta, MCPF es asintóticamente más eficiente que MCO.\n\n\n\nEspecificaciones alternativas\nSe han usado especificaciones alternativas de la heteroscedasticidad. En consecuencia se modificaría el Paso 2 del procedimiento para obtener el estimador MCPF descrito antes:\n\nRegresar \\log(\\hat{u}^2_i) sobre un subconjunto de las explicativas.\nRegresar \\log(\\hat{u}^2_i) sobre las explicativas, sus cuadrados y productos cruzados.\nRegresar \\log(\\hat{u}^2_i) sobre las predicciones, \\hat{y}_i, y sus cuadrados, \\hat{y}^2_i.\n\nLos restantes pasos del procedimiento de estimación no se alterarían."
  },
  {
    "objectID": "het.html#otras-cuestiones",
    "href": "het.html#otras-cuestiones",
    "title": "3  Heteroscedasticidad",
    "section": "3.8 Otras cuestiones",
    "text": "3.8 Otras cuestiones\n\nComparación con MCO\n\nMCO y MCP(F) son estimadores consistentes cuando se cumplen los supuestos RLM.1 a RLM.4.\nGrandes diferencias en las estimaciones de MCO y de MCP(F) indicarían el posible incumplimiento de otras de las hipótesis de Gauss-Markov.\n\n\n\nInferencia robusta\n\nSi el supuesto sobre la naturaleza de la heteroscedasticidad es correcto, los contrastes t y F calculados a partir de las estimaciones MCP(F) tienen validez asintótica.\nSi el supuesto no captura toda la heteroscedasticidad, sería necesario usar un estimador robusto de \\mathop{\\mathrm{var}}(\\tilde{\\beta}) después de la estimación MCP(F).\nEn general, es recomendable el uso de métodos de inferencia robustos a heteroscedasticidad con los estimadores de MCO y de MCP(F)."
  },
  {
    "objectID": "lpm.html#introducción",
    "href": "lpm.html#introducción",
    "title": "4  El modelo de probabilidad lineal",
    "section": "4.1 Introducción",
    "text": "4.1 Introducción\n\nEventos cuantitativos\n¿Es posible usar el modelo de regresión para analizar los determinantes de eventos cuantitativos?\n\n\nVariable dependiente binaria\nLa variable dependiente sólo toma dos valores:\n\ny_i = 1, cuando se produce el evento que estamos estudiando;\ny_i = 0, cuando no se produce.\n\n\n\nModelo de regresión múltiple\n\nModelo de regresión: \n    y = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_k x_k + u.\n\n¿Cuál es la interpretación de los parámetros cuando y es una variable binaria?\n\n\n\nProbabilidad condicional\n\nSi y es una variable binaria, su esperanza condicional coincide con la probabilidad condicional de que y = 1: \n    \\mathop{\\mathrm{E}}(y | \\boldsymbol{x}) = \\Pr(y = 1 | \\boldsymbol{x}).\n\nModelo de probabilidad lineal (MLP): la probabilidad de que y tome el valor 1 es una función lineal en los parámetros \\beta_0, \\beta_1, \\dots, \\beta_k: \n    \\Pr(y = 1 | \\boldsymbol{x}) = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_k x_k.\n\n\n\n\nInterpretación de los parámetros\n\nPodemos expresar el MLP como: \n    p(\\boldsymbol{x}) = \\beta_0 + \\beta_1 x_1  + \\dots + \\beta_k x_k,\n donde p(\\boldsymbol{x}) es una forma abreviada de escribir \\Pr(y = 1 | \\boldsymbol{x}).\nLa pendiente \\beta_j mide cómo cambia la probabilidad de que y = 1 cuando cambia x_j, manteniendo inalteradas el resto de explicativas: \n    \\beta_j = \\frac{\\Delta p(\\boldsymbol{x})}{\\Delta x_j}.\n\n\n\n\nVentajas y limitaciones\n\nEn comparación a otros modelos más sofisticados, el MLP:\n\nes más fácil de usar e interpretar;\nproporciona resultados similares cuando las variables explicativas toman valores cercanos a sus medias.\n\nEl MLP puede generar probabilidades fuera del intervalo (0, 1), por lo que no es adecuado para algunas aplicaciones."
  },
  {
    "objectID": "lpm.html#estimación-por-mco",
    "href": "lpm.html#estimación-por-mco",
    "title": "4  El modelo de probabilidad lineal",
    "section": "4.2 Estimación por MCO",
    "text": "4.2 Estimación por MCO\n\nEstimación\n\nLos parámetros del MLP se pueden estimar por MCO.\nLos residuos se obtienen de la forma usual: \n    \\hat{u}_i = y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_{1i} - \\dots - \\hat{\\beta}_k x_{ki}.\n\n\n\n\nPredicción\n\nLas predicciones de MCO se pueden interpretar como estimaciones de la probabilidad de y = 1: \n    \\hat{p}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{1i} + \\dots + \\hat{\\beta}_k x_{ki}\n\nPara predecir la variable dependiente se suele usar la regla: \n    \\tilde{y}_{i} =\n    \\begin{cases}\n      1 & \\text{si $\\hat{p}_{i} \\geq 0.5$}, \\\\\n      0 & \\text{si $\\hat{p}_{i} &lt; 0.5$}.\n    \\end{cases}\n\n\n\n\nBondad del ajuste\nTabulación cruzada de y e \\tilde{y}:\n\n\n\n\n\\tilde{y}_i = 0\n\\tilde{y}_i = 1\n\n\n\n\ny_i = 0\nn_{00}\nn_{01}\n\n\ny_i = 1\nn_{10}\nn_{11}\n\n\n\n\nNúmero de predicciones correctas: n_{00} + n_{11}.\nNúmero total de observaciones: n = n_{00} + n_{01} + n_{10} + n_{11}.\nPorcentaje de predicciones correctas: 100 (n_{00} + n_{11}) / n. Se usa para medir la bondad del ajuste de modelos con variable dependiente binaria.\n\n\n\nInsesgadez y consistencia\n\nLos supuestos RLM.1 a RLM.4 no son incompatibles con que la variable dependiente sea binaria.\nSi se cumplen los supuestos RLM.1 a RLM.4 el estimador MCO es insesgado y consistente.\n\n\n\nHeteroscedasticidad\n\nEn el modelo lineal de probabilidad se incumple el supuesto RLM.5.\nLa varianza condicional del término de error puede escribirse como: \n    \\mathop{\\mathrm{var}}(u|\\boldsymbol{x}) = p(\\boldsymbol{x})\\big(1 - p(\\boldsymbol{x})\\big).\n\n\n\n\nUso de MCO\n\nLos parámetros del modelo lineal de probabilidad pueden estimarse por MCO.\nPara contrastar hipótesis o construir intervalos de confianza es necesario usar estimaciones robustas a heteroscedasticidad de \\mathop{\\mathrm{var}}(\\hat{\\beta})."
  },
  {
    "objectID": "lpm.html#estimación-eficiente",
    "href": "lpm.html#estimación-eficiente",
    "title": "4  El modelo de probabilidad lineal",
    "section": "4.3 Estimación eficiente",
    "text": "4.3 Estimación eficiente\n\nVarianza del término de error\n\nLa varianza condicional del término de error del MLP puede escribirse como: \n    \\mathop{\\mathrm{var}}(u_i|\\boldsymbol{x_i}) = p(\\boldsymbol{x_i})\\big(1 - p(\\boldsymbol{x_i})\\big).\n\nLa probabilidad p_i = p(\\boldsymbol{x_i}) depende de parámetros desconocidos.\nSe podría aplicar MCPF si se reemplaza p_i por una estimación, \\hat{p}_i.\n\n\n\nMínimos Cuadrados Ponderados Factibles\n\nSi se cumplen los supuestos RLM.1 a RLM.4, MCO es un estimador insesgado.\nCon las estimaciones \\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_k se estima la probabilidad de que y = 1: \n    \\hat{p}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{1i} + \\dots + \\hat{\\beta}_k x_{ki}\n\nLos pesos para obtener el estimador MCPF son w_i = 1 / \\hat{h}_i, donde: \n    \\hat{h}_i = \\hat{p}_i (1 - \\hat{p}_i).\n\n\n\n\nLimitaciones\n\nEl estimador MCPF no puede obtenerse si para algunas observaciones se obtiene \\hat{p}_i \\leq 0 o \\hat{p}_i \\geq 1.\nAunque se han propuesto alternativas, en esos casos es mejor usar MCO y contrastes de hipótesis robustos a heteroscedasticidad."
  },
  {
    "objectID": "other-cs.html#forma-funcional",
    "href": "other-cs.html#forma-funcional",
    "title": "5  Forma funcional y problemas con los datos",
    "section": "5.1 Forma funcional",
    "text": "5.1 Forma funcional\n\nForma funcional\nPor ejemplo, supongamos que la siguiente ecuación de salarios satisface los supuestos RLM.1 a RLM.4: \n  \\begin{aligned}\n  \\log(\\mathit{wage}) = &\\beta_{0}\n  + \\beta_{1} \\mathit{educ}\n  + \\beta_{2} \\mathit{exper}\n  + \\beta_{3} \\mathit{exper}^{2} \\\\\n  &+ \\beta_{4} \\mathit{female}\n  + \\beta_{5} \\mathit{female} \\cdot \\mathit{educ}\n  + u\n  \\end{aligned}\n ¿Qué consecuencias tendría estimar el modelo sin \\mathit{exper}^2 o sin el término de interacción \\mathit{female} \\cdot \\mathit{educ}? ¿Cómo podemos detectar errores de especificación de la forma funcional?\n\n\nConsecuencias\n\nAl omitir términos relevantes, es probable que obtengamos estimaciones sesgadas de los restantes parámetros.\nNo está claro como interpretar los efectos marginales de las explicativas.\n\n\n\nDetección\n\nSe han propuesto muchas alternativas para detectar errores de especificación de la forma funcional. En este curso consideraremos:\n\nMedidas de bondad del ajuste.\nContrastes F.\nContrastes de hipótesis no anidadas.\n\nTodos estos contrastes suponen que la especificación de la variable dependiente es correcta. No sirven, por ejemplo, para elegir entre tomar logaritmos de la variable dependiente o no.\n\n\n\nBondad del ajuste\n\nUna forma simple de comparar modelos alternativos es usar medidas de bondad del ajuste.\nDebe usarse el R^2 ajustado cuando los modelos tienen un número diferente de variables explicativas.\n\n\n\nContrastes F\n\nPodemos probar diferentes especificaciones alternativas incluyendo potencias, términos de interacción y otras transformaciones de las explicativas.\nLos contrastes F se pueden utilizar para determinar la significación de los términos que se añaden al modelo original.\n\n\n\nCOMMENT Contraste RESET\n\nEl contraste RESET se diseñó para detectar problemas de mala especificación de la forma funcional.\nBajo la hipótesis nula, la forma funcional es correcta. No es necesario especificar la forma funcional bajo la hipótesis alternativa.\n\n\n\nCOMMENT Cálculo del contraste RESET\n\nSe estima por MCO el modelo original: \n  y = \\beta_{0} + \\beta_{1} x_{1} + \\dots + \\beta_{k} x_{k} + u\n\nSe obtienen las predicciones de la variable dependiente, \\hat{y}, y se añaden al modelo potencias de \\hat{y}: \n  y = \\beta_{0} + \\beta_{1} x_{1} + \\dots + \\beta_{k} x_{k}\n  + \\delta_1 \\hat{y}^2 + \\delta_2 \\hat{y}^3 +  u\n\nBajo la hipótesis nula, el modelo original estaba bien especificado y los términos adicionales no son significativos: H_0\\!: \\delta_1 = \\delta_2 = 0. Para verificar esta condición se usa un contraste F.\n\n\n\nCOMMENT Variaciones del contraste RESET\n\nAlgunos autores recomiendan añadir solamente los cuadrados de las predicciones: \n  y = \\beta_{0} + \\beta_{1} x_{1} + \\dots + \\beta_{k} x_{k}\n  + \\delta_1 \\hat{y}^2  +  u\n En este caso podemos usar un contraste t para la H_0\\!: \\delta_1 = 0.\nSi la varianza condicional de término de error no fuera constante, se utilizarían contrates F y t robustos a heteroscedasticidad.\n\n\n\nModelos no anidados\n\n¿Cómo podemos elegir entre los siguientes modelos? \n\\begin{gather}\n  y = \\alpha_{0}\n  + \\alpha_{1} x_{1}\n  + \\alpha_{2} x_{2}\n  + u \\tag{Modelo A} \\\\\n  y = \\beta_{0}\n  + \\beta_{1} \\log(x_{1})\n  + \\beta_{2} \\log(x_{2})\n  + v \\tag{Modelo B}\n\\end{gather}\n\nEl ejemplo anterior es un caso de modelos no anidados: ninguno de los dos modelos es un caso especial del otro.\nNo podemos usar contrastes F para comparar modelos no anidados.\n\n\n\nContrastes de abarcamiento\n\nLos contrastes de abarcamiento (encompassing tests) se basan en la construcción de un tercer modelo que contenga como casos especiales los modelos que se quieren comparar.\n\n\n\nContrastes de abarcamiento: ejemplo\n\nEl siguiente modelo tiene como casos especiales el Modelo A y el Modelo B de nuestro ejemplo: \n  y = \\gamma_{0}\n  + \\gamma_{1} x_{1}\n  + \\gamma_{2} x_{2}\n  + \\gamma_{3} \\log(x_{1})\n  + \\gamma_{4} \\log(x_{2})\n  + \\text{error}\n\nPara contrastar la validez de un modelo frente a otro podemos usar los contrastes F correspondientes a las siguientes hipótesis:\n\n\n\n\n\n\n\nModelo A frente a Modelo B:\nH_0\\!: \\gamma_3 = \\gamma_4 = 0\n\n\nModelo B frente a Modelo A:\nH_0\\!: \\gamma_1 = \\gamma_2 = 0\n\n\n\n\n\n\nContrastes de Davidson-MacKinnon\nDavidson y MacKinnon proponen añadir a un modelo las predicciones obtenidas con otro modelo y contrastar si ese término adicional tiene capacidad explicativa.\n\n\nContrastes de Davidson-MacKinnon: ejemplo\n\nPara contrastar la validez del Modelo A:\n\nSe estima el Modelo B y se obtienen las predicciones: \\hat{y}_B.\nSe contrasta H_0\\!: \\theta_B = 0 en la regresión: \n  y = \\alpha_{0}\n  + \\alpha_{1} x_{1}\n  + \\alpha_{2} x_{2}\n  + \\theta_{B} \\hat{y}_{B}\n  + \\text{error}\n\n\nEn el caso del Modelo B, se contrasta H_0\\!: \\theta_A = 0 en la regresión: \n    y = \\beta_{0}\n    + \\beta_{1} \\log(x_{1})\n    + \\beta_{2} \\log(x_{2})\n    + \\theta_{A} \\hat{y}_{A}\n    + \\text{error}\n   donde \\hat{y}_A son las predicciones obtenidas con el Modelo A.\n\n\n\nContrastes de hipótesis no anidadas y heteroscedasticidad\n\nLas hipótesis no anidadas se contrastan con estadísticos t o F en modelos de regresión que se construyen con el único objetivo del contraste de esas hipótesis.\nEn caso de heteroscedasticidad se utilizarían las versiones robustas de los contrastes t y F.\n\n\n\nContrastes de hipótesis no anidadas: limitaciones\n\nEstos contrastes no producen siempre una respuesta clara: es posible que se acepten los dos modelos o que se rechacen ambos.\nRechazar un modelo no quiere decir que el modelo rival sea el correcto."
  },
  {
    "objectID": "other-cs.html#observaciones-incompletas",
    "href": "other-cs.html#observaciones-incompletas",
    "title": "5  Forma funcional y problemas con los datos",
    "section": "5.2 Observaciones incompletas",
    "text": "5.2 Observaciones incompletas\n\nObservaciones incompletas\n\nUn problema relativamente frecuente es la presencia de observaciones incompletas para las que faltan los valores de algunas variables.\nUsualmente se ignoran las observaciones incompletas en la estimación.\n\n\n\nObservaciones incompletas: consecuencias\n\nSi se eliminan las observaciones incompletas para la estimación, se reduce el número de observaciones y la precisión de los estimadores empeora.\nLas estimaciones pueden estar sesgadas si la falta de información responde a razones sistemáticas (por ejemplo: las familias más ricas no revelan su renta con mayor frecuencia que familias más pobres).\nNo se introducen sesgo si las observaciones incompletas se producen por puro azar."
  },
  {
    "objectID": "other-cs.html#muestreo-no-aleatorio",
    "href": "other-cs.html#muestreo-no-aleatorio",
    "title": "5  Forma funcional y problemas con los datos",
    "section": "5.3 Muestreo no aleatorio",
    "text": "5.3 Muestreo no aleatorio\n\nMuestreo no aleatorio\nEl muestreo no aleatorio se puede deber a diversas razones:\n\nObservaciones incompletas.\nEsquemas de obtención de los datos.\n\n\n\nSelección muestral\nSelección muestral: El mecanismo que determina qué observaciones forman parte de la muestra no es aleatorio. Distinguimos entre:\n\nSelección muestral exógena: la selección de la muestra se basa en los valores que toman las variables explicativas.\nSelección muestral endógena: la muestra no contiene observaciones para algunos valores de la variable dependiente.\n\n\n\nSelección muestral: consecuencias\n\nSelección muestral exógena: Los estimadores siguen siendo insesgados y consistentes mientras se cumpla el supuesto de media condicional nula del término de error.\nSelección muestral endógena: La selección en base a la variable dependiente implica que el supuesto RLM.4 no se cumple, por lo que MCO sería en este caso sesgado e inconsistente.\n\n\n\nMuestreo estratificado\n\nEstratos: subgrupos de una población que no se solapan y cuya unión es la población total.\nMuestreo estratificado: la frecuencia con que se muestrean los diferentes estratos no es proporcional a su tamaño.\nSe trata de garantizar que en la muestra final estén suficientemente representados grupos minoritarios.\n\n\n\nMuestreo estratificado: consecuencias\n\nSi la estratificación se basa en una o más variables explicativas, el estimador de MCO será insesgado y consistente.\nPor el contrario si la estratificación se realiza en base a los valores que toma la variable dependiente, MCO será sesgado e inconsistente."
  },
  {
    "objectID": "other-cs.html#observaciones-atípicas-y-observaciones-influyentes",
    "href": "other-cs.html#observaciones-atípicas-y-observaciones-influyentes",
    "title": "5  Forma funcional y problemas con los datos",
    "section": "5.4 Observaciones atípicas y observaciones influyentes",
    "text": "5.4 Observaciones atípicas y observaciones influyentes\n\nObservaciones atípicas y observaciones influyentes\n\nObservaciones atípicas (outliers): observaciones para las que una variable toma un valor inusualmente grande.\nObservaciones influyentes: son aquellas que si se eliminan de la muestra provocan grandes cambios en las estimaciones de MCO.\n\n\n\nObservaciones atípicas\n\nPueden deberse a errores en la introducción de los datos. Si la corrección del error no es obvia, se deberían eliminar esas observaciones.\nOtra posible razón es la inclusión en la muestra de observaciones con características muy diferentes al resto de la población. Con frecuencia, la mejor estrategia en estos casos es presentar los resultados con y sin las observaciones atípicas.\n\n\n\nAnálisis de los residuos\n\nEl tamaño de los residuos de MCO no es un buen indicador para detectar observaciones atípicas. En general, los residuos de estas observaciones no son especialmente grandes.\n\n\n\nResiduos estudentizados\n\nEn la detección de observaciones atípicas se suelen emplear los residuos estudentizados, \\hat{\\epsilon}.\nEl residuo estudentizado para la observación i, \\hat{\\epsilon}_i, puede obtenerse añadiendo al modelo una ficticia que sólo toma el valor 1 en esa observación: \\hat{\\epsilon}_i es el estadístico t de esa ficticia.\nSe considerarían atípicas aquellas observaciones cuyos residuos estudentizados sean muy grandes de acuerdo con la distribución t_{n-k-2}."
  },
  {
    "objectID": "intro-ts.html#introducción",
    "href": "intro-ts.html#introducción",
    "title": "6  Introducción a las series temporales",
    "section": "6.1 Introducción",
    "text": "6.1 Introducción\n\nDatos de series temporales\n\nUna muestra de datos de series temporales está formada por observaciones de una o más variables en periodos de tiempo consecutivos.\n\n\n\nComparación con datos de corte transversal\n\nLos datos no provienen de un muestreo aleatorio.\nEl paso del tiempo proporciona un orden natural a las observaciones de series temporales.\n\n\n\nSeries temporales regulares\n\nSeries temporales regulares: el intervalo de tiempo que transcurre entre una observación y la siguiente es siempre el mismo: un año, un semestre, un cuatrimestre, un mes, etc.\nDe acuerdo con la frecuencia con que se observan los valores de una variable, hablamos de series temporales anuales, trimestrales, mensuales, etc.\n\n\n\nEjemplo: inflación interanual en España\n\n\nTabla 6.1: Tasa interanual de inflación calculada a partir del IPC armonizado general de España. Fuente: INE.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEne\nFeb\nMar\nAbr\nMay\nJun\nJul\nAgo\nSep\nOct\nNov\nDic\n\n\n\n\n2016\n-0{,}4\n-1{,}0\n-1{,}0\n-1{,}2\n-1{,}1\n-0{,}9\n-0{,}7\n-0{,}3\n0{,}0\n0{,}5\n0{,}5\n1{,}4\n\n\n2017\n2{,}9\n3{,}0\n2{,}1\n2{,}6\n2{,}0\n1{,}6\n1{,}7\n2{,}0\n1{,}8\n1{,}7\n1{,}8\n1{,}2\n\n\n2018\n0{,}7\n1{,}2\n1{,}3\n1{,}1\n2{,}1\n2{,}3\n2{,}3\n2{,}2\n2{,}3\n2{,}3\n1{,}7\n1{,}2\n\n\n2019\n1{,}0\n1{,}1\n1{,}3\n1{,}6\n0{,}9\n0{,}6\n0{,}6\n0{,}4\n0{,}2\n0{,}2\n0{,}5\n0{,}8\n\n\n2020\n1{,}1\n0{,}9\n0{,}1\n-0{,}7\n-0{,}9\n-0{,}3\n-0{,}7\n-0{,}6\n-0{,}6\n-0{,}9\n-0{,}8\n-0{,}6\n\n\n2021\n0{,}4\n-0{,}1\n1{,}2\n2{,}0\n2{,}4\n2{,}5\n2{,}9\n3{,}3\n4{,}0\n5{,}4\n5{,}5\n6{,}6\n\n\n2022\n6{,}2\n7{,}6\n9{,}8\n8{,}3\n8{,}5\n10{,}0\n10{,}7\n10{,}5\n9{,}0\n7{,}3\n6{,}7\n5{,}5\n\n\n2023\n5{,}9\n6{,}0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGráfico de series temporales\nTasa interanual de inflación calculada a partir del IPC armonizado general de España. Fuente: INE.\n&lt;+MATS: fig fig-08a_1027-infl.pdf\n\n\nPrincipales dificultades\n\nMuestras relativamente pequeñas.\nLos procesos que determinan los valores que toma una serie temporal pueden cambiar con el paso del tiempo.\nUn hecho que ocurra en un momento del tiempo puede tener influencia en sucesivos periodos, creando dependencia entre observaciones consecutivas de una serie temporal."
  },
  {
    "objectID": "intro-ts.html#conceptos-básicos",
    "href": "intro-ts.html#conceptos-básicos",
    "title": "6  Introducción a las series temporales",
    "section": "6.2 Conceptos básicos",
    "text": "6.2 Conceptos básicos\n\nNotación\n\ny_t: observación de la serie temporal y en el periodo t.\nEl índice t va desde 1, para el primer periodo en la muestra, hasta T, para el último.\n\n\n\nRetardos\n\nEl primer retardo (first lag) de la serie y es y_{t-1}, el valor que tomaba la variable el período anterior.\nDe forma análoga, podemos definir el segundo y sucesivos retardos de la serie y: y_{t-2}, y_{t-3}, \\dots\n\n\n\nEjemplo: retardos\nLa serie anual Y_t contiene los valores del PIB español a precios de mercado desde el año 2000 hasta el 2008. Se mide en millones de euros.\n\n\n\n\n\n\n\n\n\n\n\nAño\nt\nY_{t}\nY_{t-1}\nY_{t-2}\nY_{t-3}\n\n\n\n\n2000\n1\n647{.}851\n–\n–\n–\n\n\n2001\n2\n700{.}993\n647{.}851\n–\n–\n\n\n2002\n3\n749{.}552\n700{.}993\n647{.}851\n–\n\n\n2003\n4\n802{.}266\n749{.}552\n700{.}993\n647{.}851\n\n\n2004\n5\n859{.}437\n802{.}266\n749{.}552\n700{.}993\n\n\n2005\n6\n927{.}357\n859{.}437\n802{.}266\n749{.}552\n\n\n2006\n7\n1{.}003{.}823\n927{.}357\n859{.}437\n802{.}266\n\n\n2007\n8\n1{.}075{.}539\n1{.}003{.}823\n927{.}357\n859{.}437\n\n\n2008\n9\n1{.}109{.}541\n1{.}075{.}539\n1{.}003{.}823\n927{.}357\n\n\n\n\n\nDiferencias\n\nLa diferencia de la serie y_t es \\mathop{}\\!\\Delta y_t = y_{t} - y_{t-1}, el incremento de la variable de un periodo a otro.\nPodemos calcular \\mathit{gy}_t, la tasa porcentual de crecimiento de y_t, a partir de la diferencia y del primer retardo: \n  \\mathit{gy}_t = 100 \\frac{\\mathop{}\\!\\Delta y_t}{y_{t-1}}\n\n\n\n\nEjemplo: diferencias\nDiferencia y tasa de crecimiento de Y_t.\n\n\n\n\n\n\n\n\n\n\n\nAño\nt\nY_{t}\nY_{t-1}\n\\mathop{}\\!\\Delta Y_{t}\ngY_{t}\n\n\n\n\n2000\n1\n647{.}851\n–\n–\n–\n\n\n2001\n2\n700{.}993\n647{.}851\n53{.}142\n8{,}2\n\n\n2002\n3\n749{.}552\n700{.}993\n48{.}559\n6{,}9\n\n\n2003\n4\n802{.}266\n749{.}552\n52{.}714\n7{,}0\n\n\n2004\n5\n859{.}437\n802{.}266\n57{.}171\n7{,}1\n\n\n2005\n6\n927{.}357\n859{.}437\n67{.}920\n7{,}9\n\n\n2006\n7\n1{.}003{.}823\n927{.}357\n76{.}466\n8{,}2\n\n\n2007\n8\n1{.}075{.}539\n1{.}003{.}823\n71{.}716\n7{,}1\n\n\n2008\n9\n1{.}109{.}541\n1{.}075{.}539\n34{.}002\n3{,}2\n\n\n\n\n\nLogaritmos y tasas de crecimiento\n\nEn el análisis de series temporales es muy común usar las variables en logaritmos.\nEs muy frecuente calcular la tasa de crecimiento de y_t usando la diferencia del logaritmo de y_t: \n  \\mathit{gy}_t \\approx 100 \\mathop{}\\!\\Delta\\log(y_t)\n\n\n\n\nEjemplo: logaritmos y tasas de crecimiento\nNo hay grandes diferencias entre las diferentes formas de calcular las tasas de crecimiento.\n\n\n\n\n\n\n\n\n\n\n\nAño\nt\nY_{t}\n100 \\mathop{}\\!\\Delta Y_{t} / Y_{t-1}\n\\log(Y_{t})\n100 \\mathop{}\\!\\Delta\\log(Y_{t})\n\n\n\n\n2000\n1\n647{.}851\n–\n13{,}38\n–\n\n\n2001\n2\n700{.}993\n8{,}2\n13{,}46\n7{,}9\n\n\n2002\n3\n749{.}552\n6{,}9\n13{,}53\n6{,}7\n\n\n2003\n4\n802{.}266\n7{,}0\n13{,}60\n6{,}8\n\n\n2004\n5\n859{.}437\n7{,}1\n13{,}66\n6{,}9\n\n\n2005\n6\n927{.}357\n7{,}9\n13{,}74\n7{,}6\n\n\n2006\n7\n1{.}003{.}823\n8{,}2\n13{,}82\n7{,}9\n\n\n2007\n8\n1{.}075{.}539\n7{,}1\n13{,}89\n6{,}9\n\n\n2008\n9\n1{.}109{.}541\n3{,}2\n13{,}92\n3{,}1\n\n\n\n\n\nProcesos estocásticos\n\nLos modelos estadísticos para estudiar series temporales se basan en el concepto de proceso estocástico.\nUn proceso estocástico es una secuencia de variables aleatorias observadas en periodos consecutivos de tiempo: \\{y_1, y_2, \\dots, y_T\\}.\nNotación abreviada: \\{y_t\\!: t = 1, 2, \\dots, T\\}.\n\n\n\nEsperanza\n\nEn general, la esperanza del proceso estocástico es la secuencia de esperanzas en cada momento del tiempo: \\{\\mathop{\\mathrm{E}}(y_t)\\!: t = 1, \\dots, T\\}.\nLa esperanza de un proceso estacionario en media es constante: \n  \\mathop{\\mathrm{E}}(y_t) = \\mu, \\quad t = 1, \\dots, T.\n\n\n&lt;+latex: \n\n\nTendencias\n\nEn series económicas, es frecuente encontrar tendencias.\nPIB a precios de mercado de España. Miles de millones de euros. Fuente: INE.\n\n&lt;+MATS: fig fig-08a_1027-gdp.pdf\n\n\nEstacionalidad\n\nAlgunas series con frecuencia superior a la anual presentan variaciones estacionales: variaciones cíclicas que se repiten año a año.\nÍndice de cifra de negocios del comercio al por menor. Precios corrientes. Base 2015. Fuente: INE.\n\n&lt;+MATS: fig fig-08a_1027-retail.pdf\n\n\nVarianza\n\nUsamos la varianza para describir la dispersión de y_t en cada momento del tiempo: \\mathop{\\mathrm{var}}(y_t) = \\sigma^2_t.\nA menudo las variables económicas que presentan tendencias crecientes también tienen varianzas crecientes. En esos casos, la transformación logarítmica puede estabilizar las varianzas.\n\n\n\nAutocovarianzas\n\nUsamos las autocovarianzas para medir la dependencia de las variables que forman un proceso estocástico.\nLa autocovarianza de orden h se define como \n  \\mathop{\\mathrm{cov}}(y_t, y_{t-h}) =\n  \\mathop{\\mathrm{E}}\\!\\big[\\big(y_{t} - \\mathop{\\mathrm{E}}(y_{t})\\big) \\big(y_{t-h} - \\mathop{\\mathrm{E}}(y_{t-h})\\big)\\big]\n\n\n\n\nEstacionariedad\n\nEl concepto de estacionariedad se refiere a la estabilidad de la distribución de un proceso estocástico.\nEn su versión estricta requiere que la distribución conjunta de \\{y_t\\} no cambie conforme transcurre el tiempo.\n\n\n\nEstacionariedad débil\n\nEstacionariedad débil (o estacionariedad en covarianza) de y_t:\n\nEsperanza constante: \\mathop{\\mathrm{E}}(y_t) = \\mu.\nVarianza constante: \\mathop{\\mathrm{var}}(y_t) = \\sigma^2 &lt; \\infty.\nLa autocovarianza entre y_t e y_{t-h} sólo depende de h y no cambia con t: \\mathop{\\mathrm{cov}}(y_t, y_{t-h}) = \\gamma_h, para h \\geq 1 y cualquier t.\n\nLa estacionariedad estricta implica la estacionariedad débil, pero no al revés.\n\n\n\nNo estacionariedad\nEn economía es frecuente encontrar series no estacionarias:\n\nTendencias.\nVariaciones estacionales.\nAlternancia de periodos con alta y baja dispersión.\nCambio estructural (structural breaks).\n\n\n\nCoeficientes de autocorrelación\n\nLas autocovarianzas dependen de las unidades de y_t, lo que dificulta su interpretación.\nEs más frecuente usar los coeficientes de autocorrelación cuyo rango de variación está acotado entre -1 y 1.\nEl coeficiente de autocorrelación de orden j se define como \n  \\rho_j = \\mathop{\\mathrm{corr}}(y_t, y_{t-j}) =\n  \\frac{\\mathop{\\mathrm{cov}}(y_t, y_{t-j})}{\\sqrt{\\mathop{\\mathrm{var}}(y_t) \\mathop{\\mathrm{var}}(y_{t-j})}}\n\n\n\n\nCorrelación serial\n\nUna serie está autocorrelacionada cuando sus coeficientes de autocorrelación son distintos de 0.\nEn economía las variables suelen presentar autocorrelación positiva: el valor que toman en un periodo es similar al que tomó en periodos inmediatamente anteriores.\nEn una variable con autocorrelación negativa se suelen alternar en periodos consecutivos valores altos con valores bajos.\n\n\n\nCOMMENT Correlograma\n\nUn correlograma es la representación gráfica de los coeficientes de autocorrelación de una serie temporal.\n\n\n\nDependencia débil\n\nLa dependencia débil implica que la dependencia entre y_t e y_{t+h} decrece conforme aumenta h y que decrece a un ritmo rápido.\nUn proceso débilmente dependiente está asintóticamente incorrela-cionado si \\rho_h \\to 0 conforme h \\to \\infty. La dependencia débil exige, además, que la convergencia sea lo suficientemente rápida."
  },
  {
    "objectID": "intro-ts.html#algunos-procesos-estocásticos",
    "href": "intro-ts.html#algunos-procesos-estocásticos",
    "title": "6  Introducción a las series temporales",
    "section": "6.3 Algunos procesos estocásticos",
    "text": "6.3 Algunos procesos estocásticos\n\nAlgunos procesos estocásticos\nExaminaremos las principales características de algunos procesos estocásticos importantes:\n\nRuido blanco.\nProcesos autorregresivos.\nPaseo aleatorio.\nProcesos integrados."
  },
  {
    "objectID": "intro-ts.html#ruido-blanco",
    "href": "intro-ts.html#ruido-blanco",
    "title": "6  Introducción a las series temporales",
    "section": "6.4 Ruido blanco",
    "text": "6.4 Ruido blanco\n\nDefinición\nEl proceso estocástico \\{e_t\\} es un ruido blanco si:\n\n\\mathop{\\mathrm{E}}(e_t) = 0.\n\\mathop{\\mathrm{var}}(e_t) = \\sigma^2_e &lt; \\infty.\n\\mathop{\\mathrm{cov}}(e_{t+h}, e_t) = 0, para todo h &gt; 0.\n\n\n\nEjemplo\nRuido blanco, \\sigma^2_e = 4, T = 60.\n&lt;+MATS: fig fig-08b_1027-wn-ts-*.pdf\n\n\nCaracterísticas\n\nUn ruido blanco es un proceso estacionario y débilmente estacionario.\nConocer los valores pasados de un ruido blanco no ayuda a predecir mejor e_t: \n  \\mathop{\\mathrm{E}}(e_t | e_{t-1}, e_{t-2}, \\dots) = \\mathop{\\mathrm{E}}(e_t) = 0.\n\nUn ruido blanco no tiene memoria: el valor que toma en un periodo no tienen ninguna influencia sobre los valores futuros.\n\n\n\nAutocorrelación\n&lt;+MATS: figcol fig-08b_1027-wn-lag-*.pdf 0.5\n\ne_t no está correlacionado con sus retardos.\nLos coeficientes de autocorrelación de un ruido blanco son todos iguales a 0: \\rho_h = 0, para todo h &gt; 0.\n\n\n\nRelación con otros conceptos\n\nUn ruido blanco es un proceso i.i.d. (independiente e idénticamente distribuido).\nEn algunos campos coincide con el concepto de innovaciones.\nUn ruido blanco gaussiano, además de las anteriores, cumple la condición: \n  e_t \\sim \\mathop{\\mathrm{Normal}}(0, \\sigma^2_e)"
  },
  {
    "objectID": "intro-ts.html#procesos-autorregresivos",
    "href": "intro-ts.html#procesos-autorregresivos",
    "title": "6  Introducción a las series temporales",
    "section": "6.5 Procesos autorregresivos",
    "text": "6.5 Procesos autorregresivos\n\nProcesos autorregresivos\n\nEl valor presente de un proceso autorregresivo depende de los valores que tomó el proceso en periodos anteriores.\nUn proceso autorregresivo de orden p, AR(p), cumple la siguiente ecuación: \n  y_{t} = \\rho_{1} y_{t-1} + \\rho_{2} y_{t-2} + \\dots \\rho_{p} y_{t-p} + e_{t},\n donde \\rho_1, \\rho_2, \\dots, \\rho_p son parámetros y \\{e_t\\} es un ruido blanco. El valor inicial de la secuencia es 0: y_0 = 0.\n\n\n\nProceso AR(1)\n\nNos centraremos en el proceso autorregresivo de orden 1, AR(1): \n  y_{t} = \\rho y_{t-1} +  e_{t},\n\nLa condición de estabilidad de un proceso AR(1) es: \n  \\lvert\\rho\\rvert &lt; 1\n\nUn proceso AR estable es estacionario y débilmente dependiente.\n\n\n\nEsperanza de un proceso AR(1)\n\ny_t sigue un proceso AR(1): y_{t} = \\rho y_{t-1} + e_{t}.\nTomando esperanzas: \n  \\mathop{\\mathrm{E}}(y_{t}) = \\rho \\mathop{\\mathrm{E}}(y_{t-1}) +  \\mathop{\\mathrm{E}}(e_{t}).\n\nSi y_t es estacionario: \\mathop{\\mathrm{E}}(y_t) = \\mathop{\\mathrm{E}}(y_{t-1})œ. Por otro lado, \\mathop{\\mathrm{E}}(e_t) = 0. Por tanto, la expresión anterior se reduce a: \n   \\mathop{\\mathrm{E}}(y_{t}) = \\rho \\mathop{\\mathrm{E}}(y_{t}).\n\nPara que la ecuación anterior sea cierta para cualquier \\rho, se tiene que cumplir que \\mathop{\\mathrm{E}}(y_t) = 0.\n\n\n\nVarianza de un proceso AR(1)\n\ny_t sigue un proceso AR(1): y_{t} = \\rho y_{t-1} + e_{t}.\nLa varianza de y_t es: \n   \\mathop{\\mathrm{var}}(y_{t}) = \\rho^2 \\mathop{\\mathrm{var}}(y_{t-1}) + \\mathop{\\mathrm{var}}(e_t).\n\nSi y_t es estacionario, \\mathop{\\mathrm{var}}(y_{t}) = \\mathop{\\mathrm{var}}(y_{t-1}) = \\sigma^2_y. Podemos escribir la expresión anterior como: \n   \\sigma^2_y = \\rho^2 \\sigma^2_y + \\sigma^2_e.\n\nFinalmente, la varianza de y_t es: \n   \\sigma^2_y = \\frac{\\sigma^2_e}{1 - \\rho^2}.\n\n\n\n\nAutocovarianzas de un proceso AR(1) (I)\n\nYa que \\mathop{\\mathrm{E}}(y_t) = 0, la autocovarianza de y_{t+h} e y_ {t} es: \n  \\mathop{\\mathrm{cov}}(y_{t+h}, y_t) = \\mathop{\\mathrm{E}}(y_{t+h}  y_t)\n\nCuando h = 1: \n  \\mathop{\\mathrm{cov}}(y_{t+1}, y_t) = \\mathop{\\mathrm{E}}(y_{t+1}  y_t)\n\nSustituyendo y_{t+1} = \\rho y_t + e_{t+1}: \n  \\mathop{\\mathrm{cov}}(y_{t+1}, y_t) =\n  \\mathop{\\mathrm{E}}\\!\\big[(\\rho y_t + e_{t+1}) y_t\\big] =\n  \\mathop{\\mathrm{E}}\\!\\big[\\rho y^{2}_t + y_t e_{t+1} \\big]\n\nTeniendo en cuenta que \\mathop{\\mathrm{E}}(y^2_t) = \\sigma^2_y y que \\mathop{\\mathrm{E}}(y_t e_s) = 0 para t \\neq s: \n  \\mathop{\\mathrm{cov}}(y_{t+1}, y_t) = \\rho \\sigma^2_y.\n\n\n\n\nAutocovarianzas de un proceso AR(1) (II)\n\nAhora escribimos el proceso autorregresivo para t+2: \n  y_{t + 2} = \\rho y_{t + 1} +  e_{t + 2}.\n\nSustituimos y_{t+1} = \\rho y_t + e_{t+1}: \n  y_{t + 2} = \\rho^2 y_{t} + \\rho e_{t+1}+  e_{t + 2}.\n\nLa autocovarianza de y_{t+2} e y_ {t} es: \n  \\mathop{\\mathrm{cov}}(y_{t+2}, y_t) = \\mathop{\\mathrm{E}}(y_{t+2}  y_t) =\n  \\mathop{\\mathrm{E}}\\!\\big[(\\rho^2 y_{t} + \\rho e_{t+1}+  e_{t + 2}) y_t\\big].\n\nFinalmente: \n  \\mathop{\\mathrm{cov}}(y_{t+2}, y_t) = \\rho^2 \\sigma^2_y.\n\n\n\n\nAutocovarianzas de un proceso AR(1) (y III)\n\nMediante sustitución recursiva obtenemos: \n  y_{t + h} =\n  \\rho^h y_{t} +\n  \\rho^{h-1} e_{t+1}  +\n  \\rho^{h-2} e_{t+2} + \\dots +\n  \\rho e_{t+h-1} +\n  e_{t + h}.\n\nLa autocovarianza de y_{t+h} e y_ {t} es: \n  \\mathop{\\mathrm{cov}}(y_{t+h}, y_t) =\n  \\mathop{\\mathrm{E}}\\!\\big[(\\rho^h y_{t} +\n  \\rho^{h-1} e_{t+1}  +\n  \\dots +\n  e_{t + h})\n  y_t\\big].\n\nDado que \\mathop{\\mathrm{E}}(y^2_t) = \\sigma^2_y y que \\mathop{\\mathrm{E}}(y_t e_s) = 0 para t \\neq s: \n  \\mathop{\\mathrm{cov}}(y_{t+h}, y_t) = \\rho^h \\sigma^2_y.\n\n\n\n\nAutocorrelaciones de un proceso AR(1)\n\nAutocorrelación entre y_{t+h} e y_ {t}: \n  \\mathop{\\mathrm{corr}}(y_{t+h}, y_{t}) = \\frac{\\mathop{\\mathrm{cov}}(y_{t+h}, y_{t})}{\n    \\sqrt{\\mathop{\\mathrm{var}}(y_{t+h})\\mathop{\\mathrm{var}}(y_{t})}\n  }\n\nTeniendo en cuenta que \n  \\mathop{\\mathrm{var}}(y_{t+h}) = \\mathop{\\mathrm{var}}(y_{t}) = \\sigma^2_y, \\\\\n y que \n  \\mathop{\\mathrm{cov}}(y_{t+h}, y_{t}) = \\rho^h \\sigma^2_y, \\\\\n se obtiene: \n  \\mathop{\\mathrm{corr}}(y_{t+h}, y_{t}) = \\rho^h.\n\n\n\n\nEstacionariedad y dependencia débil\n\ny_t sigue un proceso AR(1): \n  y_{t} = \\rho y_{t-1} +  e_{t},\n\nSi \\lvert\\rho\\rvert &lt; 1, y_t es un proceso estacionario:\n\n\\mathop{\\mathrm{E}}(y_t) = 0.\n\\mathop{\\mathrm{var}}(y_t) = \\sigma^2_e / (1 - \\rho^2).\n\\mathop{\\mathrm{cov}}(y_{t+h}, y_{t}) = \\rho^h \\sigma^2_y, para h &gt; 0.\n\nSi \\lvert\\rho\\rvert &lt; 1 el proceso AR(1) es débilmente dependiente, puesto que \\mathop{\\mathrm{corr}}(y_{t+h}, y_t) = \\rho^h converge a 0 conforme h \\to \\infty y la convergencia es geométrica.\n\n&lt;+latex: \n\n\nAlgunos ejemplos (I)\nProceso AR(1), \\rho = 0.8, \\sigma^2_e = 4, T = 60. &lt;+MATS: fig fig-08b_1027-ar8-ts-*.pdf\nProceso AR(1), \\rho = 0.2, \\sigma^2_e = 4, T = 60. &lt;+MATS: fig fig-08b_1027-ar2-ts-*.pdf\n\n\nAlgunos ejemplos (y II)\n&lt;+MATS: col 0.5 Proceso AR(1), \\rho = 0.8, \\sigma^2_e = 4, T = 60. &lt;+MATS: fig fig-08b_1027-ar8-lag-*.pdf\n&lt;+MATS: col 0.5 Proceso AR(1), \\rho = 0.2, \\sigma^2_e = 4, T = 60. &lt;+MATS: fig fig-08b_1027-ar2-lag-*.pdf"
  },
  {
    "objectID": "intro-ts.html#paseo-aleatorio",
    "href": "intro-ts.html#paseo-aleatorio",
    "title": "6  Introducción a las series temporales",
    "section": "6.6 Paseo aleatorio",
    "text": "6.6 Paseo aleatorio\n\nDefinición\nUn paseo aleatorio es un proceso estocástico que se puede expresar como: \n   y_t = y_{t-1} + e_t,\n donde e_t es un ruido blanco.\n\n\nSustitución recursiva\nSustituyendo recursivamente se obtiene: \n  y_t = e_t + e_{t-1} + \\dots + e_1 + y_0,\n donde y_0 es el valor inicial del proceso. Supondremos que y_0 = 0, por lo que: \n  y_t = e_t + e_{t-1} + \\dots + e_1.\n\n\n\nEsperanza y varianza\n\nLa esperanza de un paseo aleatorio es 0: \n  \\mathop{\\mathrm{E}}(y_t) =  \\mathop{\\mathrm{E}}(e_t) +  \\mathop{\\mathrm{E}}(e_{t-1}) + \\dots +  \\mathop{\\mathrm{E}}(e_1) = 0.\n\nLa varianza de un paseo aleatorio es creciente en el tiempo: \n  \\mathop{\\mathrm{var}}(y_t) =  \\mathop{\\mathrm{var}}(e_t) +  \\mathop{\\mathrm{var}}(e_{t-1}) + \\dots +  \\mathop{\\mathrm{var}}(e_1) = t \\sigma^2_e.\n\n\n\n\nAutocovarianzas y autocorrelaciones\n\nLa autocovarianza entre y_{t+h} e y_t es: \n  \\mathop{\\mathrm{cov}}(y_{t+h}, y_t) = \\mathop{\\mathrm{E}}(y_{t+h} y_t) =\n  \\mathop{\\mathrm{E}}\\!\\big[( y_{t} + e_{t+1}  + \\dots + e_{t + h}) y_t\\big]  = t \\sigma^2_e.\n\nLa autocorrelación entre y_{t+h} e y_t es: \n  \\mathop{\\mathrm{corr}}(y_{t+h}, y_t) = \\sqrt{\\frac{t}{t+h}}\n\n\n\n\nNo estacionariedad\nUn paseo aleatorio no cumple las condiciones de la estacionariedad débil:\n\n\\mathop{\\mathrm{E}}(y_t) = 0.\n\\mathop{\\mathrm{var}}(y_t) = t \\sigma^2_e.\n\\mathop{\\mathrm{cov}}(y_{t+h}, y_{t}) = t \\sigma^2_e, para h &gt; 0.\n\n\n\nDependencia fuerte\n\nLas autocorrelaciones de un paseo aleatorio son: \n  \\mathop{\\mathrm{corr}}(y_{t+h}, y_t) = \\sqrt{\\frac{t}{t+h}}\n\n\\mathop{\\mathrm{corr}}(y_{t+h}, y_t) converge a 0 conforme h \\to \\infty, pero la convergencia es muy lenta.\n\n\n\nEjemplo\nPaseo aleatorio, \\sigma^2_e = 4, T = 60.\n&lt;+MATS: fig fig-08b_1027-rw-ts-*.pdf"
  },
  {
    "objectID": "intro-ts.html#tendencias-1",
    "href": "intro-ts.html#tendencias-1",
    "title": "6  Introducción a las series temporales",
    "section": "6.7 Tendencias",
    "text": "6.7 Tendencias\n\nPaseo aleatorio con deriva\n\nUn paseo aleatorio con deriva se puede expresar como: \n   y_t = \\alpha_0 + y_{t-1} + e_t,\n donde e_t es un ruido blanco y el parámetro \\alpha_0 es el término de deriva.\nSustituyendo recursivamente y teniendo en cuenta que y_0 = 0: \n  y_t = \\alpha_0 t + e_t + e_{t-1} + \\dots + e_1.\n\nEl valor esperado de y es función de t: \n   \\mathop{\\mathrm{E}}(y_t) = \\alpha_0 t\n\n\n\n\nEjemplo\nPaseo aleatorio con deriva, \\alpha_0 = 1, \\sigma^2_e = 4, T = 60.\n&lt;+MATS: fig fig-08b_1027-rwd-ts-*.pdf\n\n\nProcesos estacionarios con tendencia\n\nLos paseos aleatorios con deriva no son los únicos procesos que presentan tendencias.\nUn proceso estacionario con tendencia puede escribirse como: \n  y_t = \\beta_0 + \\beta_1 t + u_t,\n donde u_t es un proceso estacionario y débilmente dependiente.\n\n\n\nEjemplo\ny_t = \\beta_0 + \\beta_1 t + u_t, \\beta_1 = 1, u_t \\sim AR(1), T = 60.\n&lt;+MATS: fig fig-08b_1027-tst-ts-*.pdf"
  },
  {
    "objectID": "intro-ts.html#procesos-integrados",
    "href": "intro-ts.html#procesos-integrados",
    "title": "6  Introducción a las series temporales",
    "section": "6.8 Procesos integrados",
    "text": "6.8 Procesos integrados\n\nLa diferencia de un paseo aleatorio\n\nUn paseo aleatorio puede escribirse como: \n  y_t = y_{t-1} + e_t\n\nRestando y_{t-1} de ambos lados: \n  y_t - y_{t-1} = e_t\n\nLa diferencia de un paseo aleatorio es un ruido blanco: \n   \\mathop{}\\!\\Delta y_t = e_t\n\n\n\n\nDiferencias y dependencia débil\n\nLa diferencia de un paseo aleatorio es un ruido blanco, un proceso débilmente dependiente.\nExisten otros procesos estocásticos que, después de diferenciarlos, son débilmente dependientes.\n\n\n\nProcesos integrados\n\nEl proceso y_t es integrado de orden 1, I(1), si y_t es un proceso altamente persistente, pero su diferencia, \\mathop{}\\!\\Delta y_t, es débilmente dependiente.\nSi es necesario diferenciar dos veces para obtener un proceso débilmente dependiente, el proceso y_t es integrado de orden 2, I(2).\nUn proceso débilmente dependiente es integrado de orden 0, I(0), puesto que no es necesario aplicar ninguna diferencia.\n\n\n\nEjemplo: IPC armonizado\nLogaritmo del IPC armonizado de España, febrero de 1998 hasta febrero de 2023. Fuente: INE.\n&lt;+MATS: fig fig-08b_1027-lp-ts-*.pdf\n\n\nEjemplo: Inflación interanual\nTasa de inflación interanual: \n  \\pi_t = 100 \\mathop{}\\!\\Delta_{12} \\log(p_t) = 100 \\big(\\log(p_t) - \\log(p_{t-12})\\big).\n &lt;+MATS: fig fig-08b_1027-infl-ts-*.pdf\n\n\nEjemplo: Variación mensual de la inflación\nVariación mensual de la inflación: \n  \\mathop{}\\!\\Delta\\pi_t = \\pi_t - \\pi_{t-1}.\n &lt;+MATS: fig fig-08b_1027-dinfl-ts-*.pdf\n\n\nEjemplo: Procesos integrados\n\nEl índice de precios es altamente persistente. Al tomar diferencias se obtiene la tasa de inflación que también es altamente persistente. La variación de la tasa de inflación se comporta como un proceso débilmente dependiente.\nLas tres series tienen diferentes órdenes de integración: \n\\begin{gather*}\n  \\log(p_{t}) \\sim \\text{I}(2), \\\\\n  \\pi_{t} \\sim \\text{I}(1), \\\\\n  \\mathop{}\\!\\Delta\\pi_{t} \\sim \\text{I}(0).\n\\end{gather*}"
  },
  {
    "objectID": "ols-ts.html#modelos-de-regresión-con-series-temporales",
    "href": "ols-ts.html#modelos-de-regresión-con-series-temporales",
    "title": "7  El estimador MCO con series temporales",
    "section": "7.1 Modelos de regresión con series temporales",
    "text": "7.1 Modelos de regresión con series temporales\n\nIntroducción\n\nCon series temporales es posible investigar las relaciones dinámicas entre variables.\nEn primer lugar, examinaremos algunos modelos comúnmente utilizados.\n\n\n\nModelos estáticos\n\nEn un modelo estático, un cambio en una variable explicativa sólo tiene efectos en el periodo en que se produce y no tiene influencia en los periodos posteriores.\nTodas las variables del modelo se refieren al mismo periodo temporal: \n   y_t = \\alpha + \\beta z_{t} + u_t,\n donde y_t es la variable dependiente, z_t la variable explicativa, \\alpha y \\beta son parámetros desconocidos, t = 1,  2, \\dots, T y u_t es un término de error no observable tal que \\mathop{\\mathrm{E}}(u_t | z_t) = 0.\n\n\n\nModelos de retardos distribuidos\n\nEn un modelo de retardos distribuidos los cambios en las variables explicativas tienen efectos en más de un periodo.\nEstos modelos se caracterizan por incluir retardos de las variables explicativas: \n  y_t = \\alpha + \\beta_0 z_{t} + \\beta_1 z_{t-1} + \\beta_2 z_{t-2} + \\dots + \\beta_L z_{t-L} + u_t,\n donde L es el número de retardos que aparecen en el modelo, y \\mathop{\\mathrm{E}}(u_t | z_t, z_{t-1}, z_{t-2}, \\dots) = 0.\n\n\n\nEquilibrio dinámico\n\nUtilizaremos como ejemplo un modelo con dos retardos: \n  y_t = \\alpha + \\beta_0 z_{t} + \\beta_1 z_{t-1} + \\beta_2 z_{t-2} + u_t,\n\nEn ausencia de perturbaciones, u_t = 0, en equilibrio dinámico las variables toman siempre el mismo valor: y_t = y^* y z_t = z^*.\nSupongamos que, partiendo de un equilibrio dinámico, la variable z aumenta permanentemente en una unidad en el periodo 3. ¿Cómo se ajustarían las variables al nuevo equilibrio?\n\n\n\nAjuste a un nuevo equilibrio\nLa tabla muestra como se ajustan las variable dependiente a un cambio unitario permanente de z. En el nuevo equilibrio z_t = z^* + 1 e y_t = y^* + \\beta_0 + \\beta_1 + \\beta_2.\n\n\n\nt\nz_t\n\\mathop{}\\!\\Delta z_t\n\\mathop{}\\!\\Delta z_{t-1}\n\\mathop{}\\!\\Delta z_{t-2}\n\\mathop{}\\!\\Delta y_{t}\ny_t\n\n\n\n\n1\nz^*\n0\n0\n0\n0\ny^*\n\n\n2\nz^*\n0\n0\n0\n0\ny^*\n\n\n3\nz^* + 1\n1\n0\n0\n\\beta_0\ny^* + \\beta_0\n\n\n4\nz^* + 1\n0\n1\n0\n\\beta_1\ny^* + \\beta_0 + \\beta_1\n\n\n5\nz^* + 1\n0\n0\n1\n\\beta_2\ny^* + \\beta_0 + \\beta_1 + \\beta_2\n\n\n6\nz^* + 1\n0\n0\n0\n0\ny^* + \\beta_0 + \\beta_1 + \\beta_2\n\n\n7\nz^* + 1\n0\n0\n0\n0\ny^* + \\beta_0 + \\beta_1 + \\beta_2\n\n\n\\vdots\n\n\n\n\n\n\n\n\n\n\n\nMultiplicadores dinámicos\n\nEl parámetro \\beta_0 es el multiplicador de impacto o multiplicador de corto plazo. Es el efecto inmediato o contemporáneo de un cambio unitario de z_t sobre y_t.\nLos parámetros \\beta_h, h = 0, 1, \\dots, son los multiplicadores dinámicos y miden el efecto de z sobre y después de h periodos: \n  \\frac{\\mathop{}\\!\\Delta y_t}{\\mathop{}\\!\\Delta z_{t-h}} = \\frac{\\mathop{}\\!\\Delta y_{t+h}}{\\mathop{}\\!\\Delta z_{t}} = \\beta_h.\n\n\n\n\nMultiplicadores acumulativos\n\nLos multiplicadores acumulativos, \\delta_h, suman los efectos que produce un cambio unitario de z sobre la variable dependiente conforme pasa el tiempo:\n\n\n\nPeriodos transcurridos\nMultiplicador acumulativo\n\n\n\n\n0\n\\delta_0 = \\beta_0\n\n\n1\n\\delta_1 = \\beta_0 + \\beta_1\n\n\n2\n\\delta_2 = \\beta_0 + \\beta_1 + \\beta_2\n\n\n\\vdots\n\\vdots\n\n\nL\n\\delta_L = \\sum_{i=0}^L \\beta_i\n\n\n\nEn la última fila de la tabla aparece el multiplicador de largo plazo, \\delta_L, que es la suma de todos los multiplicadores dinámicos.\n\n\n\nModelos autorregresivos\n\nEn un modelo autorregresivo las variables explicativas son retardos de la variable dependiente.\nModelo autorregresivo de orden p, AR(p): \n   y_t = \\alpha + \\rho_1 y_{t-1} + \\rho_2 y_{t-2} + \\dots +  \\rho_p y_{t-p} + u_{t},\n donde \\mathop{\\mathrm{E}}(u_t | y_{t-1}, y_{t-2}, \\dots) = 0.\n\n\n\nModelos ARDL\n\nUn modelo autorregresivo con retardos distribuidos, \\text{ARDL}(p, q), incluye como variables explicativas p retardos de y y q retardos de z: \n  y_t =\n  \\alpha + \\rho_1 y_{t-1} +\n  \\dots +\n  \\rho_p y_{t-p} +\n  \\beta_0 z_{t} +\n  \\beta_1 z_{t-1} +\n  \\dots +\n  \\beta_q z_{t-q} +\n  u_{t},\n donde \\mathop{\\mathrm{E}}(u_t | z_t, y_{t-1}, z_{t-1}, y_{t-2}, z_{t-2}, \\dots) = 0."
  },
  {
    "objectID": "ols-ts.html#el-modelo-clásico-de-regresión-con-series-temporales",
    "href": "ols-ts.html#el-modelo-clásico-de-regresión-con-series-temporales",
    "title": "7  El estimador MCO con series temporales",
    "section": "7.2 El modelo clásico de regresión con series temporales",
    "text": "7.2 El modelo clásico de regresión con series temporales\n\nIntroducción\n\n¿Podemos usar MCO para estimar modelos de series temporales?\n¿Qué tiene que cumplirse para que MCO tenga buenas propiedades exactas?\n\n\n\nSupuestos\nLos supuestos del modelo clásico de regresión lineal con series temporales son:\n\nST.1: Linealidad en los parámetros.\nST.2: Ausencia de colinealidad perfecta.\nST.3: Media condicional nula.\nST.4: Homoscedasticidad.\nST.5: Ausencia de correlación serial.\nST.6: Normalidad.\n\n\n\nLinealidad en los parámetros\nST.1: Linealidad en los parámetros.\nEl proceso estocástico \\{(y_t, x_{1t}, x_{2t}, \\dots, x_{kt})\\!: t = 1, 2, \\dots, T\\} sigue el modelo: \n  y_t = \\beta_0 + \\beta_1 x_{1t} + \\beta_1 x_{2t} + \\dots + \\beta_1 x_{kt} + u_t\n donde \\{u_t\\!: t = 1, 2, \\dots, T\\} es una secuencia de perturbaciones aleatorias no observables.\n\n\nNotación\n\nNuestra especificación: \n  y_t = \\beta_0 + \\beta_1 x_{1t} + \\beta_1 x_{2t} + \\dots + \\beta_1 x_{kt} + u_t,\n admite modelos dinámicos. Por ejemplo, obtenemos un modelo retardos distribuidos si definimos x_{1t} = z_t, x_{2t} = z_{t-1}, \\dots\n\\boldsymbol{x_t} = (x_{1t}, x_{2t}, \\dots, x_{kt}): los valores que toman las explicativas en el periodo t.\n\\boldsymbol{X} = (\\boldsymbol{x_1}, \\boldsymbol{x_2}, \\dots, \\boldsymbol{x_T}): los valores de todas las explicativas en todos los periodos de la muestra.\n\n\n\nAusencia de colinealidad perfecta\nST.2: Ausencia de colinealidad perfecta.\nEn la muestra ninguna de las variables explicativas es constante ni una combinación lineal perfecta de las otras explicativas.\n\n\nMedia condicional nula\nST.3: Media condicional nula.\nLa media del término de error es 0 condicional a los valores que toman las explicativas en todos los periodos de tiempo : \n  \\mathop{\\mathrm{E}}(u_t | \\boldsymbol{X}) = 0,\\quad t = 1, 2, \\dots, T.\n\n\n\nExogeneidad estricta\nST.3 implica que las variables explicativas son estrictamente exógenas. El término de error u_t debe estar incorrelacionado con todas las explicativas en todos los periodos de tiempo:\n\n\\mathop{\\mathrm{E}}(u_t | \\boldsymbol{x_t}) = 0: exogeneidad contemporánea.\n\\mathop{\\mathrm{E}}(u_t | \\boldsymbol{x_{t-1}}, \\boldsymbol{x_{t-2}}, \\dots, \\boldsymbol{x_{1}}) =  0: incorrelación con valores pasados de \\boldsymbol{x_t}.\n\\mathop{\\mathrm{E}}(u_t | \\boldsymbol{x_{t+1}}, \\boldsymbol{x_{t+2}}, \\dots, \\boldsymbol{x_{T}}) = 0: incorrelación con valores futuros de \\boldsymbol{x_t}.\n\n\n\nRestricciones que impone la exogeneidad estricta\nLa exogeneidad estricta es incompatible con algunos casos importantes:\n\nEfectos de retroalimentación: los valores que toma y_t afectan a los valores futuros de las variables explicativas: \\boldsymbol{x_{t+1}}, \\dots, \\boldsymbol{x_{T}}.\nModelos autorregresivos.\n\n\n\nHomoscedasticidad\nST.4: Homoscedasticidad.\nCondicional a \\boldsymbol{X}, la varianza de u_t es la misma en todos los periodos: \n  \\mathop{\\mathrm{var}}(u_t | \\boldsymbol{X}) = \\sigma^2,\\quad t = 1, 2, \\dots, T.\n\n\n\nAusencia de correlación serial\nST.5: Ausencia de correlación serial.\nCondicional a \\boldsymbol{X}, las perturbaciones aleatorias de dos periodos diferentes están incorrelacionadas: \n  \\mathop{\\mathrm{corr}}(u_t, u_s | \\boldsymbol{X}) = 0,\\quad \\text{para todo $t \\neq s$.}\n\n\n\nNormalidad\nST.6: Normalidad.\nEl término de error u_t es independiente de \\boldsymbol{X} y se distribuye independiente e idénticamente como \\mathop{\\mathrm{Normal}}(0, \\sigma^2).\n\n\nPropiedades de muestras finitas\nLos supuestos del modelo clásico de regresión con series temporales garantizan las siguientes propiedades del estimador de MCO:\n\nInsesgadez: si se cumplen TS.1, TS.2 y TS.3.\nEficiencia: si, además, se cumplen TS.4 y TS.5. En este caso, se pueden usar las fórmulas de MCO de los errores típicos de \\hat{\\beta_j}.\nNormalidad: si, además, se cumple TS.6. Las fórmulas tradicionales de los contrastes t y F son válidas.\n\n\n\nLimitaciones del modelo clásico\n\nNo es posible obtener estimaciones insesgadas de los parámetros de modelos que incluyan retardos de la variable dependiente.\nEl estimador MCO tampoco tienen buenas propiedades exactas en presencia de efectos de retroalimentación.\nAlgunos de los supuestos del modelo clásico son muy restrictivos: normalidad, exogeneidad estricta, ausencia de correlación serial, etc."
  },
  {
    "objectID": "ols-ts.html#propiedades-asintóticas-de-mco-con-series-temporales",
    "href": "ols-ts.html#propiedades-asintóticas-de-mco-con-series-temporales",
    "title": "7  El estimador MCO con series temporales",
    "section": "7.3 Propiedades asintóticas de MCO con series temporales",
    "text": "7.3 Propiedades asintóticas de MCO con series temporales\n\nIntroducción\n¿Qué supuestos son necesarios para que el estimador MCO tenga buenas propiedades asintóticas en modelos de regresión con series temporales?\n\n\nSupuestos\n\nST.1': Linealidad y dependencia débil.\nST.2': Ausencia de colinealidad perfecta.\nST.3': Exogeneidad (contemporánea).\nST.4': Homoscedasticidad.\nST.5': Ausencia de correlación serial.\n\n\n\nLinealidad y dependencia débil\nST.1': Linealidad y dependencia débil.\nEl proceso estocástico \\{(y_t, x_{1t}, x_{2t}, \\dots, x_{kt})\\!: t = 1, 2, \\dots, T\\} es estacionario y débilmente dependiente sigue el modelo: \n  y_t = \\beta_0 + \\beta_1 x_{1t} + \\beta_1 x_{2t} + \\dots + \\beta_1 x_{kt} + u_t\n donde \\{u_t\\!: t = 1, 2, \\dots, T\\} es la secuencia de perturbaciones aleatorias.\n\n\nAusencia de colinealidad perfecta\nST.2': Ausencia de colinealidad perfecta.\nEn la muestra ninguna de las variables explicativas es constante ni una combinación lineal perfecta de las otras explicativas.\n\n\nExogeneidad contemporánea\nST.3': Exogeneidad (contemporánea).\n\nLas variables explicativas \\boldsymbol{x}_t son contemporáneamente exógenas: \n  \\mathop{\\mathrm{E}}(u_{t}|\\boldsymbol{x}_{t}) = 0.\n\nEste supuesto no se viola si se dan fenómenos de retroalimentación o si se incluyen retardos de la variable dependiente entre las explicativas.\n\n\n\nHomoscedasticidad y ausencia de autocorrelación\nST.4': Homoscedasticidad.\n\n  \\mathop{\\mathrm{var}}(u_t|\\boldsymbol{x}_t) = \\sigma^2.\n\nST.5': Ausencia de correlación serial.\n\n  \\mathop{\\mathrm{corr}}(u_t, u_s | \\boldsymbol{x}_t, \\boldsymbol{x}_s) = 0,\\quad \\text{para todo $t \\neq s$.}\n\n\n\nPropiedades asintóticas\n\nConsistencia: si se cumplen TS.1' a TS.3'.\nNormalidad asintótica: si además se cumplen TS.4' y TS.5'.\n\n\n\nConclusiones\n\nCon muestras de tamaño moderado o grande, bastan los supuestos ST.1' a ST.5' para garantizar buenas propiedades del estimador MCO.\nEn particular, no es necesario realizar supuestos muy restrictivos como exogeneidad estricta o normalidad.\nPor otro lado, el supuesto ST.1' requiere estacionariedad y dependencia débil de la variable dependiente y de las variables explicativas."
  },
  {
    "objectID": "ols-ts.html#inferencia-asintótica",
    "href": "ols-ts.html#inferencia-asintótica",
    "title": "7  El estimador MCO con series temporales",
    "section": "7.4 Inferencia asintótica",
    "text": "7.4 Inferencia asintótica\n\nIntroducción\n\nPara estimar los errores típicos de \\hat{\\beta}_j con las fórmulas tradicionales de MCO es necesario que se cumplan los supuestos ST.4' y ST.5'.\nSin embargo, es frecuente encontrar correlación serial en el término de error.\nTambién es frecuente, especialmente en modelos financieros, la presencia de heteroscedasticidad.\n\n\n\nInferencia robusta\n\nLas aportaciones de Newey y West, y de autores posteriores, muestran como pueden obtenerse estimaciones de \\mathop{\\mathrm{var}}(\\hat{\\beta}) que sólo requieren los supuestos ST.1', ST.2' y ST.3' y que son válidas aun en presencia de correlación serial y heteroscedasticidad.\nSe suele usar el acrónimo HAC (heteroskedasticity and autocorrelation consistent) para referirse a esos errores típicos robustos.\n\n\n\nContrastes t robustos\nLa hipótesis nula \\beta_j = b puede contrastarse usando un cociente t robusto que utiliza la estimación MCO, \\hat{\\beta}_j, y un error típico robusto a heteroscedasticidad y autocorrelación, \\mathop{\\mathrm{et}}_{\\text{HAC}}(\\hat{\\beta}_j): \n  t_{j} = \\frac{\\hat{\\beta}_{j} - b}{\\mathop{\\mathrm{et}}_{\\text{HAC}}(\\hat{\\beta}_{j})}.\n\n\n\nContrastes de hipótesis lineales\n\nEn presencia de autocorrelación y heteroscedasticidad no es válido el estadístico F que compara las \\mathop{\\mathrm{SCR}} o los R^2 de los modelos restringido y no restringido. Tampoco existe una versión robusta de este contraste.\nPara contrastar restricciones lineales generales sobre los parámetros del modelo, utilizamos contrastes de Wald construidos con estimaciones de \\mathop{\\mathrm{var}}(\\hat{\\boldsymbol{\\beta}}) robustas a heteroscedasticidad y autocorrelación.\n\n\n\nEstimación e inferencia robusta con MCO\n\nSe estiman por MCO los parámetros del modelo de regresión. MCO es consistente si se cumplen los supuestos ST.1', ST.2' y ST.3'.\nLos contrastes de hipótesis se llevan a cabo utilizando una estimación de \\mathop{\\mathrm{var}}(\\hat{\\boldsymbol{\\beta}}) robusta a heteroscedasticidad y autocorrelación."
  },
  {
    "objectID": "regr-ts.html#selección-del-número-de-retardos",
    "href": "regr-ts.html#selección-del-número-de-retardos",
    "title": "8  Regresión con series temporales",
    "section": "8.1 Selección del número de retardos",
    "text": "8.1 Selección del número de retardos\n\nIntroducción\n\nModelo ARDL(p, q): \n   y_t = \\alpha + \\rho_1 y_{t-1} + \\dots + \\rho_p y_{t-p} +\n   \\beta_0 z_t + \\beta_1 z_{t-1} + \\dots + \\beta_q z_{t-q} +\n   u_t\n\n¿Cuántos retardos hay que incluir en una regresión de series temporales?\n¿Qué métodos podemos usar para determinar el número de retardos?\n\n\n\nSelección de retardos\n\nSi se selecciona un número demasiado pequeño de retardos se omite información relevante.\nCon un número demasiado grande se pierde eficiencia.\n\n\n\nSelección de modelos\n\nSe establece un número máximo de retardos a considerar procurando que sea lo suficientemente elevado para capturar todas las relaciones dinámicas.\nSe estiman modelos de regresión con diferente número de retardos. La muestra debe ser la misma en todas las regresiones.\nSe utiliza algún indicador para determinar cuál es el modelo más adecuado.\n\n\n\nContrastes F\n\nSe contrasta la significación del último retardo usando un contraste F. Si el último retardo no es significativo, se estima un nuevo modelo sin ese retardo y se vuelve a repetir el proceso.\nLos contrastes F no están diseñados para seleccionar modelos y el procedimiento tiende a seleccionar más retardos de los necesarios.\n\n\n\nCriterios de información\n\nEn la selección de modelos se suele usar algún criterio de información: medidas de la bondad del ajuste que incluyen una penalización creciente en el número de parámetros.\nSe estiman todos los modelos con diferente número de retardos que se quieran comparar. Se selecciona aquel con un menor valor del criterio de información.\n\n\n\nAlgunos criterios de información\n\nAIC: criterio de información de Akaike. En los modelos de regresión puede escribirse como: \n   \\text{AIC} = C + T \\log\\left(\\frac{\\mathop{\\mathrm{SCR}}}{T}\\right) + 2 (K + 1),\n donde C es una constante, T es el número de observaciones, \\mathop{\\mathrm{SCR}} es la suma del cuadrado de los residuos y K es el número de parámetros.\nBIC: criterio de información bayesiano. \n   \\text{BIC} = C + T \\log\\left(\\frac{\\mathop{\\mathrm{SCR}}}{T}\\right) + \\log(T) (K + 1).\n Se diferencia del AIC en que penaliza más el número de parámetros.\n\n\n\nComparación entre criterios\n\nUsando el BIC se obtiene una estimación consistente del número de retardos.\nEl AIC produce resultados similares aunque suele seleccionar un número de retardos mayor que el BIC."
  },
  {
    "objectID": "regr-ts.html#regresión-espuria",
    "href": "regr-ts.html#regresión-espuria",
    "title": "8  Regresión con series temporales",
    "section": "8.2 Regresión espuria",
    "text": "8.2 Regresión espuria\n\nRegresión espuria en cortes transversales\n\nDos variables, y y x, parecen estar muy correlacionadas y la regresión de y sobre x produce resultados significativos.\nLa relación entre y y x desaparece cuando se incluye una tercera variable, z, en la regresión.\nLa regresión de y sobre x es un ejemplo de regresión espuria.\n\n\n\nRegresión espuria en series temporales\n\nLa omisión de variables relevantes también puede producir regresiones espurias con datos de series temporales.\nAdemás, es muy frecuente encontrar regresiones espurias cuando se incluyen series temporales con tendencia o que sean altamente persistentes.\n\n\n\nTipos de tendencia\nLa tendencia de una serie temporal está constituida por movimientos persistentes a largo plazo. Distinguimos entre:\n\nTendecias deterministas.\nTendencias estocásticas.\n\n\n\nTendencias deterministas\n\nUna tendencia determinista es una función no aleatoria del tiempo.\nPor ejemplo, si y_t fluctúa alrededor de una tendencia lineal: \n  y_t = \\alpha + \\delta t + u_t,\n donde u_t es un proceso estacionario y débilmente dependiente.\n\n\n\nRegresión y tendencias lineales\n\nRegresión con dos variables, y_t y x_t, las cuales no están relacionadas pero presentan tendencias lineales: \n  y_t = \\alpha + \\beta x_t + u_t.\n\nA pesar de que \\beta = 0, la regresión anterior con frecuencia indicaría que existe una relación significativa entre las dos variables.\nPara evitar encontrar regresiones espurias, es aconsejable introducir una tendencia lineal en la regresión anterior: \n  y_t = \\alpha + \\beta x_t + \\delta t + u_t.\n\n\n\n\nTendencias estocásticas\n\nUna tendencia estocástica es aleatoria y varía con el paso del tiempo.\nEl ejemplo más simple de una variable con tendencia estocástica es el paseo aleatorio: \n  y_t = y_{t-1} + e_t.\n\nUn paseo aleatorio alterna episodios de crecimiento durante varios periodos, con episodios donde la tendencia es decreciente.\n\n\n\nPaseo aleatorio con deriva\n\nAlgunas series tienen una tendencia de crecimiento sostenido a largo plazo.\nPaseo aleatorio con deriva: \n  y_t = \\delta + y_{t-1} + e_t.\n\nSi el parámetro \\delta &gt; 0 la variable y_t tiende a crecer con el paso del tiempo.\n\n\n\nRaíces unitarias\n\nUn paseo aleatorio es un caso especial de un AR(1), donde \\rho = 1, incumpliendo la condición de estabilidad.\nLa condición de estabilidad de procesos AR(p) es más complicada y depende del valor de las raíces del polinomio característico: un polinomio de orden p formado a partir de los coeficientes del proceso AR(p).\nCuando una de esas raíces sea igual a 1, se dice que el proceso tiene una raíz unitaria.\n\n\n\nRaíces unitarias, tendencias estocásticas y procesos I(1)\n\nSi un proceso tiene una raíz unitaria entonces tiene una tendencia estocástica.\nLa diferencia de un proceso con una raíz unitaria no tiene tendencia estocástica.\nUn proceso con una raíz unitaria es un proceso integrado de orden 1, I(1).\n\n\n\nRegresión espuria y tendencias estocásticas\n\nSi y_t y x_t son procesos I(1), la regresión de y_t sobre x_t mostrará con mucha frecuencia resultados muy significativos aunque, en realidad, las dos variables no estén relacionadas.\nCuando y_t y x_t están relacionadas y tienen una tendencia común, los residuos de la regresión de y_t sobre x_t deberían comportarse como un proceso I(0). En ese caso las variables están cointegradas.\n\n\n\nRegresión con variables con tendencias\n\nDebemos añadir un término de tendencia lineal cuando alguna de las variables presente tendencias deterministas.\nLas variables con raíz unitaria deben ser diferenciadas para eliminar las tendencias estocásticas.\nLa posibilidad de que haya relaciones de cointegración entre variables I(1) no se explorará en este curso."
  },
  {
    "objectID": "regr-ts.html#contrastes-de-raíz-unitaria",
    "href": "regr-ts.html#contrastes-de-raíz-unitaria",
    "title": "8  Regresión con series temporales",
    "section": "8.3 Contrastes de raíz unitaria",
    "text": "8.3 Contrastes de raíz unitaria\n\nModelo AR(1)\nEl modelo autorregresivo de orden 1, AR(1), puede escribirse como: \n  y_t = \\alpha + \\rho y_{t-1} + u_t,\n donde u_t es un ruido blanco.\n\n\nEstacionariedad y dependencia débil\nLas propiedades dinámicas del proceso AR(1) dependen del valor de \\rho:\n\ny_t es estacionario y débilmente dependiente si \\lvert\\rho\\rvert &lt; 1.\ny_t es un paseo aleatorio si \\rho = 1.\ny_t tiene un comportamiento explosivo cuando \\rho &gt; 1.\n\n\n\nDetección de raíces unitarias\n\nLos estadísticos habituales no pueden usarse para contrastar la existencia de una raíz unitaria.\nLa estimación MCO del parámetro \\rho está sesgada hacia abajo y el sesgo aumenta cuanto más cerca está \\rho de 1.\nLa distribución del estadístico t = (\\hat{\\rho} - 1)/\\mathop{\\mathrm{et}}({\\rho}) es muy diferente de una t_{T-2} o de una normal cuando \\rho tiene un valor cercano a 1.\n\n\n\nRegresión de Dickey-Fuller\n\nProceso AR(1): \n  y_t = \\alpha + \\rho y_{t-1} + u_t,\n\nRestando y_{t-1} de ambos lados: \n  \\mathop{}\\!\\Delta y_t = \\alpha + (\\rho - 1) y_{t-1} + u_t.\n\nDickey y Fuller plantean contrastar la existencia de una raíz unitaria estimado por MCO el modelo de regresión: \n  \\mathop{}\\!\\Delta y_t = \\alpha + \\beta y_{t-1} + u_t.\n\n\n\n\nContraste \\boldsymbol{t} de Dickey-Fuller\n\nRegresión de Dickey y Fuller: \n  \\mathop{}\\!\\Delta y_t = \\alpha + \\beta y_{t-1} + u_t.\n\nLa existencia de una raíz unitaria implica que \\beta = 0.\nDickey y Fuller proponen contrastar la hipótesis nula H_0\\!: \\beta = 0 frente a la alternativa H_1\\!: \\beta &lt; 0 mediante un contraste t de una cola.\n\n\n\nDistribución del contraste de Dickey-Fuller\n\nNo debe usarse un contraste \\boldsymbol{t} robusto a heteroscedasticidad. Bajo la hipótesis nula el contraste t tradicional es válido aunque haya heteroscedasticidad.\nEl contraste de Dickey-Fuller bajo la hipótesis nula no sigue una distribución normal ni ninguna otra distribución convencional.\nLos valores críticos del contraste se obtienen mediante simulación:\n\n\n\n\n\n\n\n\n10\\%\n5\\%\n1\\%\n\n\n\n\n-2{,}57\n-2{,}86\n-3{,}43\n\n\n\n\n\n\nContraste de Dickey-Fuller aumentado\n\nSi y_t sigue un proceso AR(p) es necesario incluir p-1 retardos de \\mathop{}\\!\\Delta y_t en la regresión de Dickey-Fuller para obtener un contraste válido: \n  \\mathop{}\\!\\Delta y_t =\n  \\alpha + \\beta y_{t-1} + \\phi_1 \\mathop{}\\!\\Delta y_{t-1}  +\n  \\phi_2 \\mathop{}\\!\\Delta y_{t-2} + \\dots  + \\phi_{p-1} \\mathop{}\\!\\Delta y_{t-p+1}  + u_t.\n\nLos valores críticos de Dickey-Fuller son válidos para contrastar H_0\\!: \\beta = 0 frente a H_1\\!: \\beta &lt; 0.\n\n\n\nSelección de retardos\n\nEs conveniente utilizar un criterio de información para seleccionar el número de retardos a incluir en la regresión de Dickey-Fuller.\nPara el constraste de Dickey-Fuller aumentado es preferible el AIC.\n\n\n\nTendencias\n\nSi y_t presenta una tendencia creciente o decreciente es aconsejable añadir una tendencia lineal a la regresión de Dickey-Fuller: \n  \\mathop{}\\!\\Delta y_t =\n  \\alpha + \\delta t + \\beta y_{t-1} + \\phi_1 \\mathop{}\\!\\Delta y_{t-1}  +\n  \\phi_2 \\mathop{}\\!\\Delta y_{t-2} + \\dots  + \\phi_{p-1} \\mathop{}\\!\\Delta y_{t-p+1}  + u_t.\n\nCuando se incluye una tendencia, la distribución del contraste de Dickey-Fuller es diferente y es necesario usar otros valores críticos:\n\n\n\n\n\n\n\n\n10\\%\n5\\%\n1\\%\n\n\n\n\n-3{,}12\n-3{,}41\n-3{,}96\n\n\n\n\n\n\nResumen\n\nMediante la inspección de gráficos, se determina si es necesario incluir una tendencia lineal en la regresión de Dickey-Fuller.\nSe selecciona el número de retardos en la regresión de Dickey-Fuller de acuerdo con el valor del AIC.\nSe rechaza la presencia de raíz unitaria cuando el valor del cociente t para H_0\\!: \\beta = 0 frente a H_1\\!: \\beta &lt; 0 es menor que los valores críticos de la siguiente tabla:\n\n\n\n\n\n\n\n\n\nTendencia lineal\n10\\%\n5\\%\n1\\%\n\n\n\n\nNo\n-2{,}57\n-2{,}86\n-3{,}43\n\n\nSí\n-3{,}12\n-3{,}41\n-3{,}96"
  },
  {
    "objectID": "regr-ts.html#otras-cuestiones",
    "href": "regr-ts.html#otras-cuestiones",
    "title": "8  Regresión con series temporales",
    "section": "8.4 Otras cuestiones",
    "text": "8.4 Otras cuestiones\n\nMultiplicadores de largo plazo (I)\n\nModelo con tres retardos de la variable explicativa: \n   y_t = \\alpha + \\beta_0 z_t + \\beta_1 z_{t-1} + \\beta_2 z_{t-2} + \\beta_3 z_{t-3} + u_t.\n\nEs posible reescribir el modelo anterior como: \n   y_t = \\alpha + \\delta_0 \\mathop{}\\!\\Delta z_t + \\delta_1 \\mathop{}\\!\\Delta z_{t-1} +\n   \\delta_2 \\mathop{}\\!\\Delta z_{t-2} + \\delta_3 z_{t-3} + u_t\n\nCon la regresión anterior se estiman directamente los multiplicadores acumulativos y de largo plazo y sus correspondientes errores típicos.\n\n\n\nMultiplicadores de largo plazo (y II)\n\nModelo ARDL(p, q): \n   y_t = \\alpha + \\rho_1 y_{t-1} + \\dots + \\rho_p y_{t-p} +\n   \\beta_0 z_t + \\beta_1 z_{t-1} + \\dots + \\beta_q z_{t-q} +\n   u_t\n\nEl multiplicador de largo plazo es una función no lineal de los parámetros: \n   \\delta_{\\text{LP}} = \\frac{\\sum_{j=0}^q \\beta_j}{1 - \\sum_{j=1}^p \\rho_j}.\n\nEn este curso no se tratará como obtener los multiplicadores acumulativos y sus errores típicos en modelos ARDL.\n\n\n\nCambio estructural\n\nSe produce un cambio estructural cuando los parámetros del modelo de regresión no permanecen constantes a lo largo de todo el periodo muestral.\nConsideraremos el caso de que los parámetros sólo cambian una vez y que conocemos en qué periodo se produce el cambio.\n\n\n\nContraste de Chow (I)\n\nModelo de regresión original: \n  y_t = \\beta_0 + \\beta_1 x_{1t}  + \\beta_2 x_{2t} + u_t.\n\nEl contraste de Chow puede usarse cuando se sospecha de la existencia de un cambio estructural en el periodo \\tau, 1 &lt; \\tau  &lt; T.\nCreamos una variable ficticia D_t que toma el valor 0 en todos los periodos anteriores a \\tau y valor 1 a partir del periodo \\tau. \n  D_t =\n  \\begin{cases}\n    0 & \\text{si $t &lt; \\tau$}, \\\\\n    1 & \\text{si $t \\geq \\tau$}.\n  \\end{cases}\n\n\n\n\nContraste de Chow (y II)\n\nAñadimos al modelo original la variable D_t así como interacciones de D_t con todas las variables explicativas: \n  y_t = \\beta_0 + \\beta_1 x_{1t}  + \\beta_2 x_{2t} +\n  \\theta_0 D_t + \\theta_1 D_t x_{1t}   + \\theta_2 D_t x_{2t}\n  + u_t.\n\nBajo la hipótesis nula de que no se ha producido cambio estructural, los términos que se han añadido no son significativos.\nPuede usarse un estadístico F para contrastar \\theta_0 = \\theta_1  = \\theta_2 = 0.\nEn caso que se rechace la hipótesis nula, el parámetro \\theta_j muestra como ha cambiado el parámetro \\beta_j a partir del periodo \\tau.\n\n\n\nEstacionalidad\n\nCon datos trimestrales y mensuales es usual observar variables con fuertes oscilaciones estacionales.\nUna forma simple de controlar este tipo de variación es incluir variables ficticias estacionales en el modelo de regresión.\n\n\n\nVariables ficticias estacionales\n\nCon datos trimestrales podemos definir cuatro variables ficticias estacionales D^s_t, s = 1, \\dots, 4: \n  D^s_t =\n  \\begin{cases}\n    0 & \\text{si $t$ corresponde al trimestre $s$}, \\\\\n    1 & \\text{en caso contrario}.\n  \\end{cases}\n\nEn la regresión sólo incluiríamos tres de estas variables. De otra forma, incurriríamos en la trampa de la variables ficticias.\nDe la misma manera, para controlar la estacionalidad en datos mensuales incluiríamos un conjunto de 11 variables ficticias estacionales."
  },
  {
    "objectID": "autocorr.html#propiedades-de-mco-y-autocorrelación-del-término-de-error",
    "href": "autocorr.html#propiedades-de-mco-y-autocorrelación-del-término-de-error",
    "title": "9  Autocorrelación",
    "section": "9.1 Propiedades de MCO y autocorrelación del término de error",
    "text": "9.1 Propiedades de MCO y autocorrelación del término de error\n\nIntroducción\n\nEl supuesto ST.5' requiere que el término de error no esté correlacionado consigo mismo y que se cumpla: \n  \\mathop{\\mathrm{cov}}(u_{t}, u_{s} | \\boldsymbol{x_{t}}, \\boldsymbol{x_{s}}) = 0, \\quad t \\neq s.\n\n¿Qué consecuencias tiene el incumplimiento de este supuesto? ¿Cómo se puede detectar? ¿Cómo se debe proceder en presencia de autocorrelación?\n\n\n\nInsesgadez y consistencia\nLas propiedades de insesgadez y consistencia no dependen del supuesto de no correlación serial.\n\n\nEficiencia\n\nEl teorema de Gauss-Markov requiere que el término de error no presente correlación serial.\nCon autocorrelación MCO no es eficiente.\n\n\n\nInferencia\n\nLas fórmulas de MCO para estimar los errores típicos de \\hat{\\beta}_j no son válidas en presencia de autocorrelación.\nLas fórmulas de MCO frecuentemente subestiman los errores típicos y las estimaciones parecen ser más precisas de lo que realmente son.\nLas fórmulas tradicionales de los contrastes t y F no son válidas con autocorrelación del término de error.\n\n\n\nEstimación e inferencia robusta con MCO\n\nSe estiman por MCO los parámetros del modelo de regresión. MCO es consistente si se cumplen los supuestos ST.1', ST.2' y ST.3'.\nLos contrastes de hipótesis sobre los parámetros del modelo de regresión se llevan a cabo utilizando una estimación de \\mathop{\\mathrm{var}}(\\hat{\\boldsymbol{\\beta}}) robusta a heteroscedasticidad y autocorrelación.\n\n\n\nVariable dependiente retardada (I)\n\nAlgunos autores señalan que MCO es inconsistente cuando hay autocorrelación en modelos con variable dependiente retardada.\nSin embargo, esta conclusión no es válida en modelos cuya especificación dinámica es correcta.\n\n\n\nVariable dependiente retardada (II)\n\nModelo autorregresivo donde el término de error sigue un proceso AR(1): \n\\begin{gather*}\n  y_{t} = \\beta_{0} + \\beta_{1} y _{t-1} + u_{t}, \\\\\n  u_{t} = \\rho u_{t-1} + e_{t},\n\\end{gather*}\n donde \\lvert\\beta_1\\rvert &lt; 1, \\lvert\\rho\\rvert &lt; 1 y e_t es un ruido blanco.\nLa condición de esperanza condicional nula requiere que \\mathop{\\mathrm{cov}}(y_{t-1}, u_t) = 0, pero: \n  \\mathop{\\mathrm{cov}}(y_{t-1}, u_t) = \\mathop{\\mathrm{cov}}(y_{t-1}, \\rho u_{t-1} + e_{t}) = \\rho\n  \\mathop{\\mathrm{cov}}(y_{t-1}, u_{t-1}) \\neq 0.\n\n\n\n\nVariable dependiente retardada (III)\n\nEl modelo anterior puede reescribirse como: \n  y_{t} = \\phi_{0} + \\phi_{1} y_{t-1} + \\phi_{2} y_{t-2} + e_{t},\n donde \\phi_0 = \\beta_0 (1 - \\rho), \\phi_1 = \\beta_1 + \\rho y \\phi_2 = -\\beta_1 \\rho.\nPor tantos, el valor esperado de y_t condicional a sus valores pasados depende de dos retardos: \n  \\mathop{\\mathrm{E}}(y_t | y_{t-1}, y_{t-2}, y_{t-3}, \\dots ) =\n  \\mathop{\\mathrm{E}}(y_t | y_{t-1}, y_{t-2}) =\n  \\phi_{0} + \\phi_{1} y_{t-1} + \\phi_{2} y_{t-2}\n\n\n\n\nVariable dependiente retardada (y IV)\nEn general, la correlación serial de los errores de un modelo con variable dependiente retardada es una indicación de que la especificación dinámica es incompleta."
  },
  {
    "objectID": "autocorr.html#contrastes-de-autocorrelación",
    "href": "autocorr.html#contrastes-de-autocorrelación",
    "title": "9  Autocorrelación",
    "section": "9.2 Contrastes de autocorrelación",
    "text": "9.2 Contrastes de autocorrelación\n\nIntroducción\n\nSe han propuesto diferentes métodos para detectar la autocorrelación del término de error.\nLa hipótesis nula de estos contrastes es que no existe correlación serial en el término de error.\nAlgunas de las propuestas más conocidas sólo son válidas bajo supuestos restrictivos.\n\n\n\nAutocorrelación de orden 1\n\nModelo de regresión: \n  y_{t} = \\beta_{0} + \\beta_{1} x_{1t} + \\dots + \\beta_{k} x_{tk} + u_{t}.\n\nEl término de error sigue un proceso AR(1): \n   u_{t} = \\rho u_{t-1} + e_{t}.\n\n\n\n\nContraste de Breusch-Godfrey\n\nSe estiman los parámetros del modelo de regresión por MCO y se obtienen los residuos \\hat{u}_t.\nSe estima por MCO la regresión auxiliar: \n  \\hat{u}_{t} = \\theta_{0} + \\theta_{1} x_{1t} + \\dots + \\theta_{k} x_{kt} + \\rho \\hat{u}_{t-1}\n  + \\text{error}.\n\nBajo la hipótesis nula de que \\rho = 0, el estadístico de multiplicadores de Lagrange, LM = n_\\text{aux} R^2_\\text{aux}, se distribuye como una \\chi^2 con un grado de libertad.\nTambién se puede usar un contraste F o un contraste t de la hipótesis \\rho = 0 en la regresión auxiliar.\n\n\n\nExtensiones del contraste de Breusch-Godfrey\n\nSe puede extender el contraste a órdenes superiores de autocorrelación incluyendo p retardos de los residuos en la regresión auxiliar: \n  \\hat{u}_{t} = \\theta_{0} + \\theta_{1} x_{1t} + \\dots + \\theta_{k} x_{kt} +\n  \\rho_1 \\hat{u}_{t-1} + \\dots + \\rho_p \\hat{u}_{t-p}\n  + \\text{error}.\n\nBajo la hipótesis nula de no autocorrelación, \\rho_1 = \\dots =  \\rho_p = 0, el estadístico LM = n_\\text{aux} R^2_\\text{aux}, se distribuye como una \\chi^2 con p grados de libertad.\nTambién se puede usar un contraste F de significación conjunta de los retardos de los residuos en la regresión auxiliar.\n\n\n\nContrastes de Breusch-Godfrey y heteroscedasticidad\nPueden obtenerse contrastes de autocorrelación que son validos aun un presencia de heteroscedasticidad usando estimaciones de \\mathop{\\mathrm{var}}(\\hat{\\beta}) robustas a heteroscedasticidad en la regresión auxiliar.\n\n\nOtros contrastes de autocorrelación\n\nEl contraste de Breusch-Godfrey sólo requiere la exogeneidad contemporánea de los regresores y es fácil hacerlo robusto a heteroscedasticidad.\nSi los regresores son estrictamente exógenos se pueden omitir las variables explicativas de la regresión auxiliar y basar el contraste en la regresión auxiliar: \n  \\hat{u}_{t} = \\theta_{0} +\n  \\rho_1 \\hat{u}_{t-1} + \\dots + \\rho_p \\hat{u}_{t-p}\n  + \\text{error}.\n\n\n\n\nEl contraste de Durbin-Watson\n\nEl estadístico de Durbin y Watson se ha usado tradicionalmente para contrastar la hipótesis de no autocorrelación frente a la alternativa de que u_t sigue un AR(1): \n  DW = \\frac{%\n    \\sum_{t=2}^{T}(\\hat{u}_{t} - \\hat{u}_{t-1})^{2}}{%\n    \\sum_{t=1}^{T} \\hat{u}_{t-1}^{2}}.\n\nSe puede expresar aproximadamente usando \\hat{\\rho} la estimación de la pendiente de la regresión de \\hat{u}_t sobre \\hat{u}_{t-1}: \n  DW \\approx 2 (1 - \\hat{\\rho}).\n\n\n\n\nLa distribución del contraste Durbin-Watson\n\nEl estadístico de Durbin-Watson toma valores entre 0 y 4. Bajo la hipótesis nula de no autocorrelación DW \\approx 2. Valores cercanos a 0 indican autocorrelación positiva, mientras que valores próximos a 4 señalan la existencia de correlación serial negativa.\nDurbin y Watson dedujeron la distribución exacta de DW bajo los supuestos del modelo clásico. Además, la distribución depende de los valores de las explicativas en la muestra, del número de regresores, del tamaño muestral y de la inclusión de un término constante.\nEn general, no es recomendable el uso del Durbin-Watson."
  },
  {
    "objectID": "autocorr.html#estimación-eficiente",
    "href": "autocorr.html#estimación-eficiente",
    "title": "9  Autocorrelación",
    "section": "9.3 Estimación eficiente",
    "text": "9.3 Estimación eficiente\n\nIneficiencia de Mínimos Cuadrados Ordinarios\n\nEl estimador MCO puede ser muy ineficiente en presencia de correlación serial.\nSi los regresores son estrictamente exógenos es posible usar estimadores más eficientes pertenecientes a la familia de Mínimos Cuadrados Generalizados, MCG.\n\n\n\nRegresión con término de error AR(1)\n\nModelo de regresión simple: \n  y_{t} = \\beta_{0} + \\beta_{1} x_{t} + u_{t}.\n\nEl término de error sigue un AR(1): \n  u_{t} = \\rho u_{t-1} + e_t,\n donde e_t es un ruido blanco.\n\n\n\nTransformación del modelo de regresión\n\nEscribimos el modelo de regresión para t-1 y multiplicamos por \\rho para obtener: \n  \\rho y_{t - 1} = \\rho \\beta_{0} + \\beta_{1} \\rho x_{t - 1} + \\rho u_{t - 1}.\n\nRestamos la ecuación anterior al modelo de regresión: \n  y_{t} - \\rho y_{t} =\n  \\beta_{0} (1 - \\rho)  +\n  \\beta_{1} (x_{t} - \\rho x_{t - 1}) +\n  u_{t} - \\rho u_{t - 1}.\n\n\n\n\nMínimos Cuadrados Generalizados\n\nFinalmente escribimos el modelo transformado como: \n  \\tilde{y}_{t} =\n  \\beta_{0} \\tilde{x}_{0t}+\n  \\beta_{1} \\tilde{x}_{t} +\n  e_{t},\n donde \\tilde{y}_{t} = y_{t} - \\rho y_{t-1} y \\tilde{x}_{t} =  x_{t} - \\rho x_{t - 1} son las variables en cuasi diferencias y \\tilde{x}_{0t} = (1 - \\rho).\nEl estimador MCG consiste en aplicar MCO al modelo transformado donde las variables aparecen en cuasi diferencias.\n\n\n\nExtensiones\n\nMCG se puede aplicar a modelos de regresión con más de una variable explicativa. En el modelo transformado todos los regresores se cuasi diferencian: \n  \\tilde{y}_{t} =\n  \\beta_{0} \\tilde{x}_{0t}+\n  \\beta_{1} \\tilde{x}_{1t} +\n  \\dots +\n  \\beta_{k} \\tilde{x}_{kt} +\n  e_{t},\n\nAl calcular las cuasi diferencias se pierde una observación. Prais y Winsten proponen una transformación que permite conservar la primera observación en la estimación MCG.\n\n\n\nMínimos Cuadrados Generalizados Factibles\n\nPara calcular las cuasi diferencias de las variables sería necesario conocer el valor de \\rho.\nEl estimador de Mínimos Cuadrados Generalizados Factibles, MCGF, reemplaza el parámetro desconocido \\rho por una estimación, \\hat{\\rho}.\n\n\n\nAplicación de MCGF a errores AR(1)\n\nSe estiman los parámetros del modelo de regresión por MCO y se obtienen los residuos, \\hat{u}_t.\nSe obtiene \\hat{\\rho} estimando por MCO una autorregresión con los residuos: \n  \\hat{u}_t = \\mu + \\rho \\hat{u}_{t-1} + \\text{error}.\n\nSe construyen las cuasi diferencias de las variables utilizando \\hat{\\rho} y se estima por MCO el modelo transformado.\n\n\n\nExtensiones del estimador MCGF\n\nEl procedimiento descrito anteriormente se denomina método de Cochrane-Orcutt. Es posible utilizar la propuesta de Prais-Winsten para conservar la primera observación.\nEl estimador MCGF se suele iterar: a partir de los residuos MCGF se calcula una nueva estimación de \\rho y se repite el proceso hasta que las estimaciones se estabilizan.\nEs posible extender el método para considerar esquemas de autocorrelación más complejos que un AR(1).\n\n\n\nComparación de MCO y MCGF\n\nLa consistencia de MCO sólo requiere exogeneidad contemporánea.\nLa consistencia de MCGF requiere exogeneidad estricta y no es consistente si el modelo de regresión incorpora retardos de la variable dependiente o si hay retroalimentación.\nEn el caso en que MCO y MCGF proporciones resultados similares, serían preferibles las estimaciones de MCGF. Si hay grandes diferencias en las estimaciones de ambos métodos, MCO sería preferible al ser consistente en condiciones menos exigentes."
  }
]