[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ec1027-guide",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "mcrl.html#supuestos",
    "href": "mcrl.html#supuestos",
    "title": "2  El modelo clásico de regresión lineal",
    "section": "2.1 Supuestos",
    "text": "2.1 Supuestos\n\nRLM.1: Linealidad en los parámetros.\nRLM.2: Muestreo aleatorio.\nRLM.3: Ausencia de multicolinealidad perfecta.\nRLM.4: Media condicional nula.\nRLM.5: Homoscedasticidad.\nRLM.6: Normalidad.\n\n\nRLM.1: Linealidad en los parámetros\nEl modelo poblacional puede expresarse como: \\[\n  y = \\beta_{0} + \\beta_{1} x_{1} + \\dots + \\beta_{k} x_{k} + u\n\\] donde:\n\n\\(y\\) es la variable dependiente,\n\\(\\beta_0, \\beta_1, \\dots, \\beta_k\\) son los parámetros desconocidos,\n\\(x_1, x_2, \\dots, x_k\\) son las variables explicativas y\n\\(u\\) es un término de error no observable.\n\n\n\nRLM.2: Muestreo aleatorio\nDisponemos de una muestra aleatoria de \\(n\\) observaciones: \\[\n  \\{(x_{1i}, x_{2i}, \\dots, x_{ki}, y_{i}); i = 1, 2, \\dots, n \\}\n\\]\n\n\nRLM.3: No hay colinealidad perfecta\nEn la muestra se cumplen todas las condiciones:\n\nEl número de observaciones, \\(n\\), es mayor que el de parámetros, \\(k + 1\\).\nNinguna de las variables explicativas es constante.\nNo existen relaciones lineales exactas entre las explicativas.\n\n\n\nRLM.4: Media condicional nula\nEl valor esperado del término de error para cualquier combinación de valores que tomen las variables explicativas es 0: \\[\n  \\mathop{\\mathrm{E}}(u | x_{1}, x_{2}, \\dots, x_{k}) = 0\n\\]\n\n\nRLM.5: Homoscedasticidad\nLa varianza del término de error no depende de los valores que tomen las explicativas: \\[\n  \\mathop{\\mathrm{var}}(u | x_{1}, x_{2}, \\dots, x_{k}) = \\sigma^{2}\n\\]\n\n\nRLM.6: Normalidad\nEl término de error es independiente de las variables explicativas y su distribución es normal con media \\(0\\) y varianza \\(\\sigma^2\\): \\[\n  u \\sim \\mathop{\\mathrm{Normal}}(0, \\sigma^{2})\n\\]"
  },
  {
    "objectID": "mcrl.html#estimación",
    "href": "mcrl.html#estimación",
    "title": "2  El modelo clásico de regresión lineal",
    "section": "2.2 Estimación",
    "text": "2.2 Estimación\n\nFunción de regresión muestral\nFunción de regresión muestral: \\[\n  \\hat{y}= \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{1} + \\dots + \\hat{\\beta}_{k} x_{k}\n\\] donde:\n\n\\(\\hat{y}\\) son las predicciones,\n\\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_k\\) son las estimaciones de los parámetros.\n\n\n\nResiduos\nResiduos: \\[\n   \\hat{u}= y - \\hat{y}\n\\]\nLa función de regresión muestral también puede expresarse como: \\[\n  y = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{1} + \\dots + \\hat{\\beta}_{k} x_{k} + \\hat{u}\n\\]\n\n\nEstimación por MCO\nEl estimador de mínimos cuadrados ordinarios, MCO, minimiza la suma del cuadrado de los residuos: \\[\n  \\min_{\\{\\hat{\\beta}_{0}, \\dots, \\hat{\\beta}_{k}\\}} \\sum_{i = 1}^n \\hat{u}_i^{2}\n\\]\n\n\nEcuaciones normales\nEl estimador MCO se obtiene resolviendo las ecuaciones normales: \\[\n\\begin{gather*}\n  \\sum_{i = 1}^n \\hat{u}_{i} = 0 \\\\\n  \\sum_{i = 1}^n \\hat{u}_{i} x_{ji} = 0 \\quad (j = 1, 2, \\dots, k)\n\\end{gather*}\n\\]\n\n\nSumas de cuadrados\nSuma de cuadrados total: \\[\n   \\mathop{\\mathrm{SCT}}= \\sum_{1=1}^n (y_i - \\bar{y})^{2}\n\\]\nSuma de cuadrados explicada: \\[\n   \\mathop{\\mathrm{SCE}}= \\sum_{1=1}^n (\\hat{y}_i - \\bar{y})^{2}\n\\]\nSuma de cuadrados de los residuos: \\[\n  \\mathop{\\mathrm{SCR}}= \\sum_{1=1}^n \\hat{u}_i^{2}\n\\]\nLa estimación por MCO garantiza que: \\[\n  \\mathop{\\mathrm{SCT}}= \\mathop{\\mathrm{SCE}}+ \\mathop{\\mathrm{SCR}}\n\\]\n\n\nBondad del ajuste\n\nError típico de la regresión: \\[\n  \\hat{\\sigma}= \\sqrt{\\frac{\\mathop{\\mathrm{SCR}}}{n - k - 1}}\n\\]\nCoeficiente de determinación: \\[\n  R^2= 1 - \\frac{\\mathop{\\mathrm{SCR}}}{\\mathop{\\mathrm{SCT}}}\n\\]\n\\(R^2\\) ajustado: \\[\n  \\bar{R}^2= 1 - \\frac{\\mathop{\\mathrm{SCR}}/(n - k - 1)}{\\mathop{\\mathrm{SCT}}/(n - 1)}\n\\]"
  },
  {
    "objectID": "mcrl.html#propiedades-del-estimador-de-mco",
    "href": "mcrl.html#propiedades-del-estimador-de-mco",
    "title": "2  El modelo clásico de regresión lineal",
    "section": "2.3 Propiedades del estimador de MCO",
    "text": "2.3 Propiedades del estimador de MCO\n\nPropiedades de muestras pequeñas\n\nSe refieren al método de estimación, MCO, no a las estimaciones obtenidas con una muestra particular.\nDependen del cumplimiento de los supuestos del modelo de regresión lineal clásico.\nNo dependen del tamaño muestral: son válidas para cualquier \\(n\\).\n\n\n\nInsesgadez\nInsesgadez: El valor esperado de las estimaciones coincide con los parámetros poblacionales: \\[\n  \\mathop{\\mathrm{E}}(\\hat{\\beta}_{j})  = \\beta_{j} \\quad (j = 0, 1, \\dots, k).\n\\]\nEl estimador de MCO es insesgado si se cumplen los supuestos RLM.1 a RLM.4.\n\n\nOmisión de variables relevantes\n\nEjemplo: modelo que cumple los supuestos RLM.1 a RLM.4: \\[\n  y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + u\n\\]\nRealizamos la regresión de \\(y\\) sobre \\(x_1\\) omitiendo la variable \\(x_2\\).\n¿Qué ocurre si especificamos un modelo donde falta una de las explicativas?\n\n\n\nSesgo\n\nCuando se omite \\(x_2\\): \\[\n  \\mathop{\\mathrm{E}}(\\hat{\\beta}_{1}) = \\beta_{1} + \\beta_2 \\delta_{12}\n\\] donde \\(\\delta_{12}\\) es la pendiente de la regresión de \\(x_1\\) sobre \\(x_2\\).\nSesgo por omisión de \\(x_2\\): \\[\n  \\operatorname{Sesgo}(\\hat{\\beta}_{1}) = \\mathop{\\mathrm{E}}(\\hat{\\beta}_{1}) - \\beta_{1} = \\beta_2 \\delta_{12}\n\\]\n\n\n\nSesgo de variable omitida\nEl estimador de MCO presenta sesgo cuando se cumplen las dos condiciones:\n\n\\(\\beta_2 \\neq 0\\): las variables omitidas son relevantes.\n\\(\\delta_{12} \\neq 0\\): las variables omitidas están correlacionadas con alguna de las variables incluidas en la regresión.\n\n\n\nVarianza del estimador MCO\nSi se cumplen los supuestos de Gauss-Markov (RLM.1 a RLM.5): \\[\n  \\mathop{\\mathrm{var}}(\\hat{\\beta}_{j} | \\boldsymbol{x}) = \\frac{\\sigma^{2}}{\\mathop{\\mathrm{SCT}}_{j}(1 - R^2_{j})}\n  \\quad (j = 1, 2, \\dots, k)\n\\] donde:\n\n\\(\\mathop{\\mathrm{SCT}}_j = \\sum_{i=1}^n(x_{ij} - \\bar{x}_j)^2\\)\n\\(R^2_j\\) es el coeficiente de determinación de una regresión de \\(x_j\\) sobre el resto de explicativas.\n\n\n\nEficiencia\nTeorema de Gauss-Markov: Si se cumplen los supuestos de Gauss-Markov, el estimador MCO es el estimador lineal insesgado óptimo.\n\nUn estimador lineal es una función lineal de los valores de la variable dependiente: \\[\n   \\hat{\\beta}_j = \\sum_{i=1}^n w_{ij} y_i\n\\]\nDentro de un grupo de estimadores, el estimador óptimo es el que tiene menor varianza muestral.\n\n\n\nDistribución muestral\nBajo los supuestos del modelo clásico de regresión lineal (RLM.1 a RLM.6): \\[\n  \\hat{\\beta}_{j} | \\boldsymbol{x} \\sim \\mathop{\\mathrm{Normal}}\\big(\\beta_j, \\mathop{\\mathrm{var}}(\\hat{\\beta}_j | \\boldsymbol{x})\\big)\n  \\quad (j = 0, 1, \\dots, k)\n\\]"
  },
  {
    "objectID": "mcrl.html#contraste-de-hipótesis-sobre-un-parámetro",
    "href": "mcrl.html#contraste-de-hipótesis-sobre-un-parámetro",
    "title": "2  El modelo clásico de regresión lineal",
    "section": "2.4 Contraste de hipótesis sobre un parámetro",
    "text": "2.4 Contraste de hipótesis sobre un parámetro\n\nHipótesis nula e hipótesis alternativa\n\nHipótesis nula, \\(H_0\\): la hipótesis para la que queremos determinar si hay o no suficiente evidencia en contra.\nHipótesis alternativa, \\(H_1\\): la hipótesis que aceptaremos si rechazamos la hipótesis nula.\n\n\n\nContrastes acerca de un parámetro\n\nModelo poblacional: \\[\n  y = \\beta_{0} + \\beta_{1} x_{1} + \\dots + \\beta_{j} x_{j} + \\dots +\n  \\beta_{k} x_{k} + u.\n\\]\nHipótesis nula: el valor del parámetro \\(\\beta_j\\) es igual a \\(b_j\\), \\[\n  H_{0}\\!: \\beta_{j} = b_{j}.\n\\]\n\n\n\nEstadístico de contraste\nEl estadístico \\(\\boldsymbol{t}\\) para la \\(H_0\\!: \\beta_j = b_j\\) es: \\[\n  t = \\frac{\\hat{\\beta}_{j} - b_{j}}{\\mathop{\\mathrm{et}}(\\hat{\\beta}_{j})},\n\\] donde \\(\\mathop{\\mathrm{et}}(\\hat{\\beta}_{j})\\) es el error típico del estimador del parámetro \\(\\beta_j\\).\n\n\nDistribución bajo la hipótesis nula\n+MATS: figcol fig-02_1027-tdens-*.pdf 0.5\nSi se cumplen los supuestos del modelo clásico de regresión lineal, cuando la hipótesis nula es cierta el estadístico \\(t\\) se distribuye como una variable aleatoria \\(t\\) con \\(n - k - 1\\) grados de libertad: \\[\n  t \\sim t_{n-k-1}.\n\\]\n\n\nLa hipótesis alternativa\nLa hipótesis alternativa es, usualmente, la negación de la hipótesis nula: \\[\n  H_{1}\\!: \\beta_{j} \\neq b_{j}.\n\\]\n\n\nLa región de rechazo\n+MATS: figcol fig-02_1027-ttest-rejection-area-*.pdf 0.5\nLa hipótesis alternativa determina la región de rechazo: valores grandes en valor absoluto del estadístico \\(t\\) son evidencia en contra de la hipótesis nula.\n\n\nNivel de significación\n\nEl nivel de significación determina el tamaño de la región de rechazo.\nEs la probabilidad de rechazar la hipótesis nula cuando ésta es cierta: \\[\n  \\alpha = \\Pr(\\text{rechazar $H_{0}$}\\,|\\,\\text{$H_{0}$ es cierta}).\n\\]\n\n\n\nValores críticos\n\nLos valores críticos delimitan la región de rechazo.\nSe determinan a partir de la distribución bajo la hipótesis nula y del nivel de significación.\n\n\n\nValores críticos de un contraste de dos colas\n+MATS: figcol fig-02_1027-ttest-two-tails-*.pdf 0.5\nEn un contraste de \\(H_0\\!: \\beta_j = b_j\\) frente a \\(H_1\\!: \\beta_j \\neq b_j\\), el valor crítico \\(c_{\\alpha}\\) cumple: \\[\n  \\Pr(|t_{n - k - 1}| &gt; c_{\\alpha/2}) = \\alpha.\n\\]\n\n\nRegla de decisión\n+MATS: figcol fig-02_1027-ttest-h0-rejected-*.pdf 0.5\nRegla de decisión: se rechaza \\(H_0\\) (en favor de \\(H_1\\)) a un nivel de significación \\(\\alpha\\) cuando el estadístico \\(t\\) cae en la región de rechazo: \\[\n  |t| &gt; c_{\\alpha/2}.\n\\]\n\n\nNo rechazo\n+MATS: figcol fig-02_1027-ttest-h0-not-rejected-*.pdf 0.5\nDecimos que no se rechaza la \\(H_0\\) cuando el estadístico \\(t\\) se sitúa fuera de la región de rechazo. Evitamos expresiones como “aceptar la hipótesis nula”.\n\n\nValor-\\(p\\)\nEl valor-\\(\\boldsymbol{p}\\) es la probabilidad, cuando \\(H_0\\) es cierta, de observar un valor del estadístico de contraste al menos tan grande como el que se ha obtenido.\n\n\nValor-\\(p\\) de un contraste de dos colas\n+MATS: figcol fig-02_1027-ttest-p-value-*.pdf 0.5\nEl valor-\\(p\\) se corresponde con el área de las dos colas delimitadas por el estadístico de contraste: \\[\n  \\text{valor-}p = \\Pr(|t_{n-k-1}| &gt; |t|).\n\\]\n\n\nRegla de decisión con el valor-\\(p\\)\nRegla de decisión: se rechaza \\(H_0\\) a un nivel de significación \\(\\alpha\\) cuando el valor-\\(p\\) es menor que el nivel de significación \\(\\alpha\\): \\[\n  \\text{valor-}p &lt; \\alpha.\n\\]\n\n\nInterpretación del valor-\\(p\\)\n\nEl valor-\\(p\\) no es la probabilidad de que \\(H_0\\) sea cierta.\nMide la intensidad de la evidencia en contra de \\(H_0\\): cuanto más pequeño es el valor-\\(p\\), menos soportan los datos la \\(H_0\\).\n\n\n\nResumen\n\nContraste de \\(H_0\\!: \\beta_j = b_j\\) frente a \\(H_{1}\\!: \\beta_{j} \\neq b_{j}\\).\nEstadístico de contraste: \\[\n  t = \\frac{\\hat{\\beta}_{j} - b_{j}}{\\mathop{\\mathrm{et}}(\\hat{\\beta}_{j})}.\n\\]\nSe rechaza \\(H_0\\) a un nivel de significación \\(\\alpha\\) si:\n\n\\(|t| &gt; c_{\\alpha/2}\\), donde \\(c_{\\alpha/2}\\) se obtiene de la distribución \\(t_{n - k - 1}\\); o\n\\(\\text{valor-}p &lt; \\alpha\\), donde el valor-\\(p\\) se obtiene de la distribución \\(t_{n - k - 1}\\).\n\n\n\n\nContrastes de significación individual\n\nEs un caso especial del contraste \\(t\\) de dos colas: \\(H_0\\!: \\beta_j = 0\\) frente a \\(H_{1}\\!: \\beta_{j} \\neq 0\\).\nEl estadístico de contraste es el cociente \\(\\boldsymbol{t}\\): \\[\n  t = \\frac{\\hat{\\beta}_{j}}{\\mathop{\\mathrm{et}}(\\hat{\\beta}_{j})}.\n\\]\nCuando se rechaza la \\(H_0\\) se suele usar expresiones como \\(x_j\\) es estadísticamente significativa al nivel \\(\\alpha\\).\n\n\n\nContrastes de una cola (I)\n+MATS: figcol fig-02_1027-ttest-one-tail-pos-*.pdf 0.5\n\n\\(H_0\\!: \\beta_j = b_j\\) frente a \\(H_{1}\\!: \\beta_{j} &gt; b_{j}\\).\nEstadístico de contraste: \\[\n  t = (\\hat{\\beta}_{j} - b_{j})/\\mathop{\\mathrm{et}}(\\hat{\\beta}_{j}).\n\\]\nSe rechaza \\(H_0\\) si \\(t &gt; c_{\\alpha}\\), donde: \\[\n  \\Pr(t_{n-k-1} &gt; c_{\\alpha}) = \\alpha.\n\\]\n\n\n\nContrastes de una cola (y II)\n+MATS: figcol fig-02_1027-ttest-one-tail-neg-*.pdf 0.5\n\n\\(H_0\\!: \\beta_j = b_j\\) frente a \\(H_{1}\\!: \\beta_{j} &lt; b_{j}\\).\nEstadístico de contraste: \\[\n  t = (\\hat{\\beta}_{j} - b_{j})/\\mathop{\\mathrm{et}}(\\hat{\\beta}_{j}).\n\\]\nSe rechaza \\(H_0\\) si \\(t &lt; -c_{\\alpha}\\), donde: \\[\n  \\Pr(t_{n-k-1} &lt; -c_{\\alpha}) = \\alpha.\n\\]\n\n\n\nContrastes de una cola y valores-\\(p\\)\n\nLos valores-\\(p\\) que presentan por defecto los programas estadísticos son válidos para contrastes de dos colas.\nSi el estadístico \\(t\\) está en la misma cola de la región de rechazo, el valor-\\(p\\) será la mitad del que imprime el programa informático.\nSi el estadístico \\(t\\) está en la cola opuesta a la de la región de rechazo, el valor-\\(p\\) será mayor que \\(0.5\\) y no se rechazará la \\(H_0\\) a ninguno de los niveles de significación habituales."
  },
  {
    "objectID": "mcrl.html#intervalos-de-confianza",
    "href": "mcrl.html#intervalos-de-confianza",
    "title": "2  El modelo clásico de regresión lineal",
    "section": "2.5 Intervalos de confianza",
    "text": "2.5 Intervalos de confianza\n\nIntervalos de confianza\nUn intervalo de confianza para el parámetro \\(\\beta_j\\) a un nivel de confianza de \\(1-\\alpha\\) es un intervalo \\((\\overline{\\beta}_j, \\underline{\\beta}_j)\\) que contendrá el valor del parámetro \\(\\beta_j\\) con una probabilidad \\(1-\\alpha\\): \\[\n\\Pr(\\underline{\\beta}_j &lt; \\beta_j &lt; \\overline{\\beta}_j) = 1 - \\alpha.  \n\\]\n\n\nEstimación por intervalos\nEstimación por intervalos: en vez de obtener un único valor (estimación puntual), calculamos un intervalo de valores para el parámetro \\(\\beta_j\\).\n+MATS: fig fig-02_1027-conf-interval-*.pdf\n\n\nContraste de hipótesis\nContraste de hipótesis: Rechazamos \\(H_0\\!: \\beta_j = b_j\\) frente a \\(H_1\\!: \\beta_j \\neq b_j\\) a un nivel de significación \\(\\alpha\\) si \\(b_j\\) no está incluido en el intervalo de confianza \\((\\overline{\\beta}_j, \\underline{\\beta}_j)\\) construido a un nivel de confianza \\(1 - \\alpha\\).\n+MATS: fig fig-02_1027-conf-interval-test-*.pdf\n\n\nCálculo\nSi se cumplen los supuestos del modelo clásico de regresión lineal, RLM.1 a RLM.6, los límites de un intervalo a un nivel de confianza \\(1 - \\alpha\\) para el parámetro \\(\\beta_j\\) son: \\[\n  \\hat{\\beta}_j \\pm c_{\\alpha/2} \\cdot \\mathop{\\mathrm{et}}(\\hat{\\beta}_j),\n\\] donde \\(c_{\\alpha/2}\\) es el valor crítico de dos colas de una distribución \\(t_{n-k-1}\\) a un nivel de significación \\(\\alpha\\).\n\n\nSimetría\nEl intervalo de confianza del parámetro \\(\\beta_j\\) es simétrico y su centro es la estimación \\(\\hat{\\beta}_j\\).\n+MATS: fig fig-02_1027-conf-interval-estimation-*.pdf"
  },
  {
    "objectID": "mcrl.html#contrastes-de-hipótesis-lineales-sobre-los-parámetros",
    "href": "mcrl.html#contrastes-de-hipótesis-lineales-sobre-los-parámetros",
    "title": "2  El modelo clásico de regresión lineal",
    "section": "2.6 Contrastes de hipótesis lineales sobre los parámetros",
    "text": "2.6 Contrastes de hipótesis lineales sobre los parámetros\n\nHipótesis lineales sobre los parámetros\n\nLa hipótesis nula se compone de una o más combinaciones lineales de los parámetros. Por ejemplo:\n\n\\(H_0\\!: \\beta_3 = 0, \\beta_4 = 1\\).\n\\(H_0\\!: \\beta_3 + \\beta_4 = 1, \\beta_5 = -1\\).\n\nLa hipótesis alternativa es la negación de la nula: al menos una de las igualdades no se cumple.\n\n\n\nRestricciones de exclusión\n\nRestricciones de exclusión múltiple: las pendientes de \\(q\\) regresores son nulas: \\[\n  H_{0}\\!: \\beta_{1} = \\beta_{2} = \\dots = \\beta_{q} = 0.\n\\]\nEn la hipótesis alternativa al menos una de las pendientes \\(\\beta_{1}, \\beta_{2}, \\dots, \\beta_{q}\\) es distinta de \\(0\\): \\[\n  H_{1}\\!: \\text{no $H_0$}.\n\\]\n\n\n\nModelo no restringido\nModelo no restringido: los parámetros pueden tomar cualquier valor y es válido tanto bajo la \\(H_0\\) como bajo la \\(H_1\\): \\[\n  y = \\beta_{0} + \\beta_{1} x_{1} + \\dots + \\beta_{q} x_{q} +\n  \\beta_{q+1} x_{q+1} + \\dots +  \\beta_{k} x_{k} + u.\n\\]\n\n\nModelo restringido\nModelo restringido: los parámetros \\(\\beta_1, \\dots, \\beta_q\\) son iguales a \\(0\\) y sólo es válido bajo la \\(H_0\\): \\[\n  y = \\beta_{0} + \\beta_{q+1} x_{q+1} + \\dots +  \\beta_{k} x_{k} + u.\n\\]\n\n\nBondad del ajuste\n\nSi las restricciones de exclusión son ciertas, no debe haber mucha diferencia entre el ajuste del modelo restringido y el del modelo no restringido.\nEl contraste se basará en la comparación de las sumas de cuadrados de los residuos del modelo restringido, \\(\\mathop{\\mathrm{SCR}}_{r}\\), y del modelo no restringido, \\(\\mathop{\\mathrm{SCR}}_{nr}\\).\n\n\n\nEstadístico de contraste\nEl estadístico de contraste es el estadístico \\(\\boldsymbol{F}\\): \\[\n  F = \\frac{(\\mathop{\\mathrm{SCR}}_{r}- \\mathop{\\mathrm{SCR}}_{nr}) / q}{\\mathop{\\mathrm{SCR}}_{nr}/ (n - k - 1)}.\n\\]\n\n\nDistribución bajo la hipótesis nula\nSi se cumplen los supuestos del modelo clásico de regresión lineal, cuando la hipótesis nula es cierta el estadístico \\(F\\) se distribuye como una variable aleatoria \\(F\\) con \\(q\\) y \\(n - k - 1\\) grados de libertad: \\[\n  F \\sim F_{q, n-k-1}.\n\\]\n\n\nRegión de rechazo\n+MATS: figcol fig-02_1027-ftest-rejection-area-*.pdf 0.5\n\nValor crítico a un nivel de significación \\(\\alpha\\): \\[\n  \\Pr(F_{q, n-k-1} &gt; c_{\\alpha}) = \\alpha.\n\\]\nValores del estadístico \\(F\\) mayores que \\(c_{\\alpha}\\) representan evidencia en contra de la hipótesis nula.\n\n\n\nLa regla de decisión\nRegla de decisión: se rechaza \\(H_0\\) a un nivel de significación \\(\\alpha\\) cuando:\n\nEl estadístico \\(F\\) cae en la región de rechazo: \\[\n  F &gt; c_{\\alpha}.\n\\]\nEl valor-\\(p\\) es menor que el nivel de significación \\(\\alpha\\). El valor-\\(p\\) es: \\[\n  \\text{valor-}p = \\Pr(F_{q, n-k-1} &gt; F).\n\\]\n\n\n\nEl contraste \\(F\\) y el \\(R^2\\)\nPara contrastar restricciones de exclusión el estadístico \\(F\\) puede calcularse a partir de los coeficientes de determinación del modelo restringido, \\(R^2_r\\), y del modelo no restringido, \\(R^2_{nr}\\): \\[\n  F = \\frac{(R^2_{nr}- R^2_r) / q}{(1 - R^2_{nr}) / (n - k - 1)}.\n\\]\n\n\nSignificación de la regresión\n\nSignificación conjunta de la regresión: contraste de la hipótesis nula de que todas las pendientes son nulas: \\[\n  H_0\\!: \\beta_1 = \\beta_2 = \\dots = \\beta_k = 0.\n\\]\nEl estadístico \\(F\\) en términos del coeficiente de determinación es: \\[\n  F = \\frac{R^2/ k}{(1 - R^2) / (n - k - 1)}.\n\\]\n\n\n\nRestricciones lineales generales\n\nSe pueden usar los estadísticos \\(F\\) basados en las sumas de cuadrados de los residuos para contrastar restricciones lineales generales.\nLas fórmulas basadas en los coeficientes de determinación sólo pueden usarse si los modelos restringido y no restringido tienen la misma variable dependiente.\nCualquier conjunto de restricciones lineales siempre puede transformarse en restricciones de exclusión mediante una reparametrización del modelo."
  },
  {
    "objectID": "mcrl.html#comment-predicción",
    "href": "mcrl.html#comment-predicción",
    "title": "2  El modelo clásico de regresión lineal",
    "section": "2.7 COMMENT Predicción",
    "text": "2.7 COMMENT Predicción\n\nPredicción puntual\n\nPredicción del valor medio.\nPredicción del valor.\n\n\n\nIntervalos de predicción\n\nValor medio.\nValor de la variable dependiente."
  },
  {
    "objectID": "asymp.html#introducción",
    "href": "asymp.html#introducción",
    "title": "3  Propiedades asintóticas del estimador MCO",
    "section": "3.1 Introducción",
    "text": "3.1 Introducción\n\nEl supuesto de normalidad\nEl supuesto RLM.6 del modelo clásico de regresión requiere la normalidad del término de error: \\[\nu \\sim \\mathop{\\mathrm{Normal}}(0, \\sigma^{2}).\n\\]\n\n\nInferencia en el modelo clásico\nEl supuesto de normalidad permite conocer la distribución de los contrates de hipótesis bajo la hipótesis nula.\n\n\nFalta de normalidad\nEn muchos casos el supuesto de normalidad no es muy razonable:\n\nVariable dependiente sólo toma valores positivos.\nVariable dependiente sólo toma algunos valores discretos.\n\n\n\nTeoría asintótica\nLa teoría asintótica estudia el comportamiento de estimadores y contrastes de hipótesis cuando el tamaño muestral crece indefinidamente.\n\n\nTeoremas sobre límites\nBajo ciertas condiciones, secuencias de variables aleatorias obtenidas por muestreo aleatorio cumplen conforme \\(n \\to \\infty\\):\n\nLeyes de grandes números: establecen la convergencia de medias muestrales a la media poblacional.\nTeoremas del límite central: establecen la distribución asintótica de las medias muestrales.\n\n\n\nPropiedades asintóticas\n\nConsistencia.\nDistribución asintótica.\nEficiencia asintótica.\n\n\n\nConsistencia\nUn estimador es consistente si conforme aumenta el tamaño muestral converge en probabilidad al parámetro poblacional: \\[\n\\mathop{\\mathrm{plim}}\\hat{\\beta}_j = \\beta_j.\n\\]\nIntuitivamente, cuanto mayor es el tamaño muestral más cerca está un estimador consistente del parámetro poblacional.\n\n\nConsistencia e insesgadez\n\nLas propiedades de consistencia e insesgadez son independientes: una no implica la otra.\nUn estimador es consistente si es insesgado y su varianza decrece y converge a 0 conforme aumenta el tamaño muestral.\n\n\n\nDistribución asintótica\n\nLa distribución asintótica de un estimador es su distribución en el límite.\nProporciona una aproximación a la distribución del estimador para un tamaño muestral finito.\n\n\n\nNormalidad asintótica\n\nUn estimador es asintóticamente normal si su distribución asintótica es normal.\nNo es necesaria la normalidad del término de error, basta con que se cumplan las condiciones de un teorema del límite central.\n\n\n\nConsideraciones prácticas\n\nLas propiedades asintóticas no requieren el supuesto de normalidad.\nRequisitos técnicos: momentos finitos hasta orden 4. Supondremos implícitamente que se cumple.\nLa calidad de las aproximaciones asintóticas suele mejorar con el tamaño muestral. Pero, ¿cuántas observaciones son necesarias en la práctica?"
  },
  {
    "objectID": "asymp.html#propiedades-asintóticas-de-mco",
    "href": "asymp.html#propiedades-asintóticas-de-mco",
    "title": "3  Propiedades asintóticas del estimador MCO",
    "section": "3.2 Propiedades asintóticas de MCO",
    "text": "3.2 Propiedades asintóticas de MCO\n\nConsistencia\n\nMCO es consistente si se cumplen los supuestos RLM.1 a RLM.4.\nEl supuesto RLM.4, \\(E(u|\\boldsymbol{x}) = 0\\), puede relajarse y basta con: \\[\n  \\mathop{\\mathrm{E}}(u) = 0 \\text{\\ y} \\mathop{\\mathrm{cov}}(x_{j}, u) \\text{\\ para $j = 1, 2, \\dots, k$}.\n\\]\n\n\n\nInconsistencia\n\nLa inconsistencia de \\(\\hat{\\beta}_i\\) es la diferencia entre el límite del estimador y el parámetro poblacional. En el caso de MCO: \\[\n  \\mathop{\\mathrm{plim}}\\hat{\\beta}_{i} - \\beta_{i} = \\frac{\\mathop{\\mathrm{cov}}(x_{i}, u)}{\\mathop{\\mathrm{var}}(x_{i})}.\n\\]\nMCO es inconsistente si el término de error y, al menos, una de las explicativas están correlacionados.\n\n\n\nVariable relevante omitida\n\nSe cumplen los supuestos RLM.1 a RLM.4 en el modelo poblacional: \\[\n  y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + u.\n\\]\nSi se omite la variable \\(x_2\\), el sesgo del estimador MCO de \\(\\beta_1\\) es: \\[\n  \\mathop{\\mathrm{plim}}\\hat{\\beta}_{1} - \\beta_{1} = \\beta_2 \\delta_{21},\n\\] donde \\(\\delta_{21} = \\mathop{\\mathrm{cov}}(x_1, x_2) / \\mathop{\\mathrm{var}}(x_1)\\), mide la intensidad de la asociación de las variables \\(x_2\\) y \\(x_1\\).\nOmitir una variable relevante, \\(\\beta_2 \\neq 0\\), causa inconsistencia si la variable omitida está correlacionada con alguna de las explicativas, \\(\\delta_{21} \\neq 0\\).\n\n\n\nVarianza de los estimadores MCO\nSi se cumplen los supuestos de Gauss-Markov, RLM.1 a RLM.5:\n\n\\(\\hat{\\sigma}^2\\) es un estimador consistente de \\(\\mathop{\\mathrm{var}}(u)\\).\nLas fórmulas para estimar las varianzas de los estimadores MCO son válidas asintóticamente: \\[\n  \\widehat{\\mathop{\\mathrm{var}}}(\\hat{\\beta}_{j}) = \\frac{\\hat{\\sigma}^2}{\\mathop{\\mathrm{SCT}}_{j}(1-R^2_{j})}.\n\\]\n\n\n\nNormalidad asintótica del estimador MCO\nSi se cumplen los supuestos de Gauss-Markov, RLM.1 a RLM.5, la distribución asintótica del estimador MCO es normal: \\[\n    \\frac{\\hat{\\beta}_{j} - \\beta_{j}}{\\mathop{\\mathrm{dt}}(\\hat{\\beta}_{j})} \\stackrel{\\mathclap[\\scriptstyle]{\\mathrm{a}}}{\\sim}\\mathop{\\mathrm{Normal}}(0, 1),\n    \\text{\\ para $j = 0, 1, 2, \\dots, k$}.\n  \\]"
  },
  {
    "objectID": "asymp.html#inferencia-asintótica",
    "href": "asymp.html#inferencia-asintótica",
    "title": "3  Propiedades asintóticas del estimador MCO",
    "section": "3.3 Inferencia asintótica",
    "text": "3.3 Inferencia asintótica\n\nContraste de hipótesis en muestras grandes\n\nNo es necesario el supuesto de normalidad del término de error para usar los contrastes \\(t\\) y \\(F\\).\nDisponemos de otros contrastes que sólo son válidos asintóticamente.\n\n\n\nContrastes \\(t\\)\n\nSi se cumplen los supuestos de Gauss-Markov, podemos usar la distribución \\(t\\) para determinar valores críticos y los valores-\\(p\\) de los contrates \\(t\\). \\[\n  \\frac{\\hat{\\beta}_{j} - \\beta_{j}}{\\mathop{\\mathrm{et}}(\\hat{\\beta}_{j})} \\stackrel{\\mathclap[\\scriptstyle]{\\mathrm{a}}}{\\sim}t_{n - k - 1}.\n\\]\n\n\n\nIntervalos de confianza\n\nSi se cumplen los supuestos de Gauss-Markov, podemos usar la fórmula habitual para obtener un intervalo a un nivel de confianza \\(1 - \\alpha\\) para el parámetro \\(\\beta_j\\): \\[\n  \\hat{\\beta}_j \\pm c_{\\alpha/2} \\cdot \\mathop{\\mathrm{et}}(\\hat{\\beta}_j),\n\\] donde \\(c_{\\alpha/2}\\) es el valor crítico de dos colas de una distribución \\(t_{n-k-1}\\) a un nivel de significación \\(\\alpha\\).\n\n\n\nContrastes \\(F\\)\n\nSi se cumplen los supuestos de Gauss-Markov, podemos usar la distribución \\(F\\) para determinar valores críticos y los valores-\\(p\\) de los contrates \\(F\\). \\[\n  \\frac{(\\mathop{\\mathrm{SCR}}_{r}- \\mathop{\\mathrm{SCR}}_{nr}) / q}{\\mathop{\\mathrm{SCR}}_{nr}/ (n - k - 1)}  \\stackrel{\\mathclap[\\scriptstyle]{\\mathrm{a}}}{\\sim}F_{q,\\, n - k - 1}.\n\\]\n\n\n\nOtros contrastes asintóticos\nSe han propuesto otros principios de contrastación de restricciones generales sobre los parámetros y que sólo tienen validez asintótica:\n\nContrastes de Wald.\nContrastes de multiplicadores de Lagrange.\nContrastes de la razón de verosimilitudes.\n\n\n\nContrastes de Wald\n\nLos contrastes de Wald pueden interpretarse como una generalización de los contrastes \\(t\\).\nSólo requieren la estimación del modelo no restringido.\nBajo la \\(H_0\\) se distribuyen como una \\(\\chi^2\\) con \\(q\\) grados de libertad. También se usan versiones de este contraste que se distribuyen como una \\(F_{q,\\, n - k - 1}\\).\n\n\n\nContrastes de multiplicadores de Lagrange\n\nPara utilizar un contraste de multiplicadores de Lagrange, \\(\\mathop{\\mathrm{LM}}\\), sólo es necesaria la estimación del modelo restringido.\nSon especialmente útiles cuando la estimación del modelo no restringido es difícil.\nFrecuentemente se calculan a partir de una regresión auxiliar.\n\n\n\nContraste de variables omitidas\n\nModelo de regresión poblacional que cumple RLM.1 a RLM.5: \\[\n  y = \\beta_0 + \\beta_1 x_1  + \\beta_2 x_2  + \\beta_3 x_3  + \\beta_4 x_4 + u.\n\\]\nEstamos interesados en contrastar la hipótesis nula \\[\n  H_0\\!: \\beta_3 = \\beta_4 = 0.\n\\]\n\n\n\nContraste \\(\\mathop{\\mathrm{LM}}\\) de variables omitidas\n\nEstimación del modelo restringido por MCO: \\[\n  y = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1  + \\hat{\\beta}_2 x_2 + \\hat{u}.\n\\]\nRegresión auxiliar usando los residuos como variable dependiente y todas las explicativas, tanto incluidas como excluidas: \\[\n  \\hat{u}\\text{\\ sobre $x_1, x_2, x_3, x_4$}.\n\\]\nEl estadístico de contraste es \\(\\mathop{\\mathrm{LM}}= n R^2_{aux}\\): el número de observaciones multiplicado por el \\(R^2\\) de la regresión auxiliar.\n\n\n\nRegla de decisión\nSi la hipótesis nula que impone \\(q\\) restricciones es correcta:\n\nEl estadístico \\(\\mathop{\\mathrm{LM}}\\) se distribuye asintóticamente como una \\(\\chi^2\\) con \\(q\\) grados de libertad: \\[\n  n R^2_{aux} \\stackrel{\\mathclap[\\scriptstyle]{\\mathrm{a}}}{\\sim}\\chi^2_q.\n\\]\nUna versión alternativa del contraste utiliza un estadístico \\(F\\) para contrastar restricciones de exclusión en la regresión auxiliar.\n\n\n\nContrastes de razón de verosimilitudes\n\nSe pueden interpretar como una generalización del contraste \\(F\\).\nRequieren la estimación del modelo restringido y del modelo no restringido.\nSon útiles cuando en modelos no lineales que se estiman con métodos diferentes a MCO."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "4  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bibliografía",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  }
]