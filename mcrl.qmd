# El modelo clásico de regresión lineal


\newcommand{\yhat}{\hat{y}}
\newcommand{\bhat}{\hat{\beta}}
\newcommand{\uhat}{\hat{u}}

\DeclareMathOperator{\SCT}{SCT}
\DeclareMathOperator{\SCE}{SCE}
\DeclareMathOperator{\SCR}{SCR}

\newcommand{\SCRr}{\SCR_{r}}
\newcommand{\SCRnr}{\SCR_{nr}}

\DeclareMathOperator{\se}{et}
\DeclareMathOperator{\sd}{dt}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\corr}{corr}
\DeclareMathOperator{\Exp}{E}
\DeclareMathOperator*{\plim}{plim}
\DeclareMathOperator{\Normal}{Normal}
\DeclareMathOperator{\LM}{LM}

\newcommand{\uhat}{\hat{u}}
\newcommand{\yhat}{\hat{y}}
\newcommand{\bhat}{\hat{\beta}}
\newcommand{\ahat}{\hat{\alpha}}
\newcommand{\dhat}{\hat{\delta}}
\newcommand{\phat}{\hat{p}}
\newcommand{\hhat}{\hat{h}}

\newcommand{\utilde}{\tilde{u}}
\newcommand{\ytilde}{\tilde{y}}
\newcommand{\btilde}{\tilde{\beta}}
\newcommand{\atilde}{\tilde{\alpha}}

\newcommand{\ymean}{\bar{y}}

\newcommand{\SER}{\hat{\sigma}}
\newcommand{\Rsq}{R^2}
\newcommand{\Rbarsq}{\bar{R}^2}

\newcommand{\Rsqr}{R^2_r}
\newcommand{\Rsqnr}{R^2_{nr}}


## Supuestos

- **RLM.1**: Linealidad en los parámetros.

- **RLM.2**: Muestreo aleatorio.

- **RLM.3**: Ausencia de multicolinealidad perfecta.

- **RLM.4**: Media condicional nula.

- **RLM.5**: Homoscedasticidad.

- **RLM.6**: Normalidad.


### RLM.1: Linealidad en los parámetros

El **modelo poblacional** puede expresarse como:
$$
  y = \beta_{0} + \beta_{1} x_{1} + \dots + \beta_{k} x_{k} + u
$$
donde:

- $y$ es la **variable dependiente**,

- $\beta_0, \beta_1, \dots, \beta_k$ son los **parámetros**
  desconocidos,

- $x_1, x_2, \dots, x_k$ son las **variables explicativas** y

- $u$ es un **término de error** no observable.


### RLM.2: Muestreo aleatorio

Disponemos de una **muestra aleatoria** de $n$ observaciones:
$$
  \{(x_{1i}, x_{2i}, \dots, x_{ki}, y_{i}); i = 1, 2, \dots, n \}
$$

### RLM.3: No hay colinealidad perfecta

En la muestra se cumplen **todas** las condiciones:

- El número de observaciones, $n$, es mayor que el de parámetros, $k +
  1$.

- Ninguna de las variables explicativas es constante.

- No existen **relaciones lineales exactas** entre las explicativas.


### RLM.4: Media condicional nula

El valor esperado del término de error para cualquier combinación de
valores que tomen las variables explicativas es 0:
$$
  \Exp(u | x_{1}, x_{2}, \dots, x_{k}) = 0
$$

### RLM.5: Homoscedasticidad

La varianza del término de error no depende de los valores
que tomen las explicativas:
$$
  \var(u | x_{1}, x_{2}, \dots, x_{k}) = \sigma^{2}
$$


### RLM.6: Normalidad

El término de error es **independiente** de las variables explicativas y
su distribución es **normal** con media 0 y varianza $\sigma^2$:
$$
  u \sim \Normal(0, \sigma^{2})
$$


## Estimación


### Función de regresión muestral

**Función de regresión muestral**:
$$
  \yhat = \bhat_{0} + \bhat_{1} x_{1} + \dots + \bhat_{k} x_{k}
$$
donde:

- $\yhat$ son los **predicciones**,

- $\bhat_0, \bhat_1, \dots, \bhat_k$ son las **estimaciones** de los parámetros.


### Residuos

**Residuos**:
$$
   \uhat = y - \yhat
$$

La función de regresión muestral también puede expresarse como:
$$
  y = \bhat_{0} + \bhat_{1} x_{1} + \dots + \bhat_{k} x_{k} + \uhat
$$


### Estimación por MCO

El estimador de **mínimos cuadrados ordinarios**, MCO, minimiza la suma
del cuadrado de los residuos:
$$
  \min_{\{\bhat_{0}, \dots, \bhat_{k}\}} \sum_{i = 1}^n \uhat_i^{2}
$$


### Ecuaciones normales

El estimador MCO se obtiene resolviendo las **ecuaciones normales**:
$$
\begin{gather*}
  \sum_{i = 1}^n \uhat_{i} = 0 \\
  \sum_{i = 1}^n \uhat_{i} x_{ji} = 0 \quad (j = 1, 2, \dots, k)
\end{gather*}
$$

### Sumas de cuadrados

Suma de cuadrados total:
$$
   \SCT = \sum_{1=1}^n (y_i - \ymean)^{2}
$$

Suma de cuadrados explicada:
$$
   \SCE = \sum_{1=1}^n (\yhat_i - \ymean)^{2}
$$

Suma de cuadrados de los residuos:
$$
  \SCR = \sum_{1=1}^n \uhat_i^{2}
$$

La estimación por MCO garantiza que:
$$
  \SCT = \SCE + \SCR
$$


### Bondad del ajuste

- Error típico de la regresión:
  $$
    \SER = \sqrt{\frac{\SCR}{n - k - 1}}
  $$

- Coeficiente de determinación:
  $$
    \Rsq = 1 - \frac{\SCR}{\SCT}
  $$

- $\Rsq$ ajustado:
  $$
    \Rbarsq = 1 - \frac{\SCR/(n - k - 1)}{\SCT/(n - 1)}
  $$


## Propiedades del estimador de MCO


### Propiedades de muestras pequeñas

- Se refieren al método de estimación, MCO, no a las estimaciones
  obtenidas con una muestra particular.

- Dependen del cumplimiento de los supuestos del modelo de regresión
  lineal clásico.

- No dependen del tamaño muestral: son válidas para cualquier $n$.



### Insesgadez

**Insesgadez**: El valor esperado de las estimaciones coincide con los
parámetros poblacionales:
$$
  \Exp(\bhat_{j})  = \beta_{j} \quad (j = 0, 1, \dots, k).
$$

El estimador de MCO es insesgado si se cumplen los supuestos **RLM.1** a
**RLM.4**.


### Omisión de variables relevantes

- Ejemplo: modelo que cumple los supuestos **RLM.1** a **RLM.4**:
  $$
    y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + u
  $$

- Realizamos la regresión de $y$ sobre $x_1$ omitiendo la variable
  $x_2$.

- ¿Qué ocurre si especificamos un modelo donde falta una de las
  explicativas?


### Sesgo

- Cuando se omite $x_2$:
  $$
    \Exp(\bhat_{1}) = \beta_{1} + \beta_2 \delta_{12}
  $$
  donde $\delta_{12}$ es la pendiente de la regresión de $x_1$ sobre
  $x_2$.

- **Sesgo** por omisión de $x_2$:
  $$
    \operatorname{Sesgo}(\bhat_{1}) = \Exp(\bhat_{1}) - \beta_{1} = \beta_2 \delta_{12}
  $$

### Sesgo de variable omitida

El estimador de MCO presenta sesgo cuando se cumplen **las dos**
condiciones:

- $\beta_2 \neq 0$: las variables omitidas son **relevantes**.

- $\delta_{12} \neq 0$: las variables omitidas están correlacionadas
  con alguna de las variables incluidas en la regresión.


### Varianza del estimador MCO

Si se cumplen los **supuestos de Gauss-Markov** (**RLM.1** a **RLM.5**):
$$
  \var(\bhat_{j} | \boldsymbol{x}) = \frac{\sigma^{2}}{\SCT_{j}(1 - \Rsq_{j})}
  \quad (j = 1, 2, \dots, k)
$$
donde:

- $\SCT_j = \sum_{i=1}^n(x_{ij} - \bar{x}_j)^2$

- $\Rsq_j$ es el coeficiente de determinación de una regresión de
  $x_j$ sobre el resto de explicativas.


### Eficiencia

**Teorema de Gauss-Markov**: Si se cumplen los supuestos de
Gauss-Markov, el estimador MCO es el **estimador lineal insesgado
óptimo**.

- Un **estimador lineal** es una función lineal de los valores de la variable dependiente:
  $$
     \bhat_j = \sum_{i=1}^n w_{ij} y_i
  $$


- Dentro de un grupo de estimadores, el **estimador óptimo** es el que
  tiene menor varianza muestral.


### Distribución muestral

Bajo los supuestos del modelo clásico de regresión lineal (**RLM.1** a **RLM.6**):
$$
  \bhat_{j} | \boldsymbol{x} \sim \Normal\big(\beta_j, \var(\bhat_j | \boldsymbol{x})\big)
  \quad (j = 0, 1, \dots, k)
$$

