{
  "hash": "dbdce67ac03e8160cb955d8569929824",
  "result": {
    "markdown": "# Heteroscedasticidad\n\n\\DeclareMathOperator{\\SCT}{SCT}\n\\DeclareMathOperator{\\SCE}{SCE}\n\\DeclareMathOperator{\\SCR}{SCR}\n\n\\newcommand{\\SCRr}{\\SCR_{r}}\n\\newcommand{\\SCRnr}{\\SCR_{nr}}\n\n\\DeclareMathOperator{\\se}{et}\n\\DeclareMathOperator{\\sd}{dt}\n\\DeclareMathOperator{\\cov}{cov}\n\\DeclareMathOperator{\\var}{var}\n\\DeclareMathOperator{\\corr}{corr}\n\\DeclareMathOperator{\\Exp}{E}\n\\DeclareMathOperator*{\\plim}{plim}\n\\DeclareMathOperator{\\Normal}{Normal}\n\\DeclareMathOperator{\\LM}{LM}\n\n\\newcommand{\\uhat}{\\hat{u}}\n\\newcommand{\\yhat}{\\hat{y}}\n\\newcommand{\\bhat}{\\hat{\\beta}}\n\\newcommand{\\ahat}{\\hat{\\alpha}}\n\\newcommand{\\dhat}{\\hat{\\delta}}\n\\newcommand{\\phat}{\\hat{p}}\n\\newcommand{\\hhat}{\\hat{h}}\n\n\\newcommand{\\utilde}{\\tilde{u}}\n\\newcommand{\\ytilde}{\\tilde{y}}\n\\newcommand{\\btilde}{\\tilde{\\beta}}\n\\newcommand{\\atilde}{\\tilde{\\alpha}}\n\n\\newcommand{\\ymean}{\\bar{y}}\n\n\\newcommand{\\SER}{\\hat{\\sigma}}\n\\newcommand{\\Rsq}{R^2}\n\\newcommand{\\Rbarsq}{\\bar{R}^2}\n\n\\newcommand{\\Rsqr}{R^2_r}\n\\newcommand{\\Rsqnr}{R^2_{nr}}\n\\newcommand{\\bm}[1]{\\boldsymbol{#1}}\n\\newcommand{\\stext}[1]{\\text{\\ #1}}\n\n\\newcommand{\\adistr}{\\stackrel{\\mathclap[\\scriptstyle]{\\mathrm{a}}}{\\sim}}\n\n\n::: {.cell}\n\n:::\n\n\n## Introducción\n\n### Homoscedasticidad (I)\n\n\n::: {#fig-het .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![Homoscedasticidad](het_files/figure-html/fig-het-1.png){#fig-het-1 width=288}\n:::\n\n::: {.cell-output-display}\n![Heteroscedasticidad](het_files/figure-html/fig-het-2.png){#fig-het-2 width=288}\n:::\n\nDispersión alrededor de la función de regresión poblacional\n:::\n\n\n**Homoscedasticidad**. La dispersión alrededor de la función de regresión poblacional, FRP, es constante:\n$$\n  \\var(u_i | x_i) = \\sigma^2.\n$$\n\n\n### Homoscedasticidad (y II)\n\n\n::: {#fig-het-shades .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![Homoscedasticidad](het_files/figure-html/fig-het-shades-1.png){#fig-het-shades-1 width=288}\n:::\n\n::: {.cell-output-display}\n![Heteroscedasticidad](het_files/figure-html/fig-het-shades-2.png){#fig-het-shades-2 width=288}\n:::\n\nDispersión alrededor de la función de regresión poblacional\n:::\n\n\n**Homoscedasticidad**. Todas las observaciones son igual de informativas a la hora de determinar por donde pasa la FRP.\n\n### Heteroscedasticidad (I)\n\n**Heteroscedasticidad**. La dispersión alrededor de la FRP cambia con los valores de la explicativa:\n$$\n  \\var(u_i | x_i) = \\sigma^2_i.\n$$\n\n### Heteroscedasticidad (y II)\n\n**Heteroscedasticidad**. Algunas observaciones contienen menos información para estimar la FRP.\n\n## Consecuencias de la heteroscedasticidad\n\n### Insesgadez y consistencia\n\n- La heteroscedasticidad no afecta al supuesto **RLM.4**.\n\n- Si se cumplen los supuestos **RLM.1** a **RLM.4**, el estimador MCO es insesgado y consistente aunque exista heteroscedasticidad.\n\n### Varianza de los estimadores de MCO\n\nCuando hay heteroscedasticidad, las fórmulas usuales de las varianzas de los estimadores de MCO, $\\var(\\bhat)$, no son válidas incluso en muestras grandes.\n\n\n### Contraste de hipótesis\n\nCuando el término de error es heteroscedástico, los procedimientos habituales de contraste de hipótesis dejan de ser válidos incluso en muestras grandes.\n\n### Eficiencia\n\n- Con heteroscedasticidad, MCO no es el estimador lineal insesgado óptimo.\n\n- Si se conoce el patrón de la heteroscedasticidad es posible construir estimadores más eficientes que MCO.\n\n## Inferencia robusta a heteroscedasticidad\n\n### Errores típicos robustos\n\nUn procedimiento común con datos de corte transversal:\n\n- Estimar los parámetros por MCO.\n\n- Modificar el cálculo de los errores típicos de los estimadores para que sean válidos (asintóticamente) haya o no haya heteroscedasticidad.\n\n### Varianza del estimador MCO\n\n- Se cumplen los supuestos **RLM.1** a **RLM.4** en el modelo de regresión simple:\n  $$\n    y_i = \\beta_0 + \\beta_1 x_i + u_i.\n  $$\n\n- Heteroscedasticidad:\n  $$\n    \\var(u_{i} | x_{1i}) = \\sigma^{2}_{i}.\n  $$\n\n- La varianza del estimador MCO es:\n  $$\n    \\var(\\bhat_{1}) = \\frac{\\sum(x_{i} - \\bar{x})^{2}\\sigma^2_{i}}{%\n      \\Big[\\sum(x_{i} - \\bar{x})^{2}\\Big]^{2}}.\n  $$\n\n\n### Estimación robusta de $\\var(\\bhat)$\n\n- [[https://en.wikipedia.org/wiki/Halbert_White][Halbert White]] mostró que se puede obtener una estimación consistente de $\\var(\\bhat)$ exista o no heteroscedasticidad:\n  $$\n    \\widehat{\\var}_R(\\bhat_{1}) = \\frac{\\sum(x_{i} - \\bar{x})^{2} \\uhat^2_i}{%\n      \\Big[\\sum(x_{i} - \\bar{x})^{2}\\Big]^{2}}.\n  $$\n\n- Se han descrito alternativas a la propuesta original de White que son equivalentes asintóticamente pero que, bajo ciertas condiciones, son superiores en muestras pequeñas.\n\n\n### Contrastes $t$ robustos\n\nUn estadístico asintóticamente válido para contrastar la hipótesis nula $\\beta_j = b$ se construye con la estimación de MCO, $\\bhat_j$, y el error típico robusto a heteroscedasticidad, $\\se_R(\\bhat_j)$:\n$$\n  t_{j} = \\frac{\\bhat_{j} - b}{\\se_{R}(\\bhat_{j})}.\n$$\n\n\n### Contrastes de hipótesis lineales\n\n- En presencia de heteroscedasticidad no es válido el estadístico $F$ que compara las $\\SCR$ o los $\\Rsq$ de los modelos restringido y no restringido. Tampoco existe una versión robusta de este contraste.\n\n- Para contrastar restricciones lineales generales sobre los parámetros del modelo, utilizamos contrastes de Wald construidos con estimaciones de $\\var(\\hat{\\bm\\beta})$ robustas a heteroscedasticidad.\n\n## Contrastes de heteroscedasticidad\n\n\n### ¿Por qué?\n\n\n- Si no hay grandes problemas de heteroscedasticidad no sería necesario usar errores típicos robustos.\n\n- MCO es el estimador óptimo (en cierto sentido) cuando el término de error es homoscedástico. Si hay heteroscedasticidad quizá queramos emplear estimadores más eficientes que MCO.\n\n\n### Contrastes de especificación\n\n- Tratamos de verificar si se cumple alguno de los supuestos sobre el modelo poblacional.\n\n- Frecuentemente la hipótesis nula afirma que no hay problemas de especificación y el estimador MCO tiene buenas propiedades.\n\n- A menudo se utiliza un contraste de multiplicadores de Lagrange.\n\n\n### Contrastes de heteroscedasticidad\n\n- La hipótesis nula es el supuesto **RLM.5**:\n  $$\n    H_0\\!: \\var(u_i | x_{1i}, \\dots, x_{ki}) = \\sigma^2.\n  $$\n\n- Bajo la hipótesis alternativa hay heteroscedasticidad:\n  $$\n    H_1\\!: \\var(u_i | x_{1i}, \\dots, x_{ki}) = \\sigma^2_i.\n  $$\n\n\n### Enfoque general\n\n- Podemos reescribir la hipótesis nula como:\n  $$\n    H_0\\!: \\Exp(u^2_i | x_{1i}, \\dots, x_{ki}) = \\sigma^2.\n  $$\n\n- Si no se cumple $H_0$ la esperanza condicional de $u^2$ depende de los regresores. Si la relación fuera lineal:\n  $$\n     u^2  = \\delta_0   + \\delta_1 x_{1} +  \\dots + \\delta_k x_{k} + \\text{error}.\n  $$\n\n- Si observáramos $u_i$ podríamos contrastar homoscedasticidad por medio de la hipótesis nula:\n  $$\n    H_0\\!: \\delta_1 = \\delta_2 = \\dots = \\delta_k = 0\n  $$\n\n\n### Regresión auxiliar\n\n- Los contrastes modernos de heteroscedasticidad utilizan una\n  regresión auxiliar cuya variable dependiente es $\\uhat^2$.\n\n- Cada contraste se diferencia por los $k_{aux}$ regresores que se incluyen.\n\n- El estadístico $\\LM = n \\Rsq_{aux}$ se distribuye bajo la $H_0$ como una $\\chi^2_{k_{aux}}$.\n\n- También puede usarse el contraste de significación de la regresión:\n  $$\n    F = \\frac{\\Rsq_{aux} / k_{aux}}{(1 - \\Rsq_{aux}) / (n - k_{aux} - 1)}.\n  $$\n\n### Contraste de Breusch-Pagan\n\n- En la actualidad, se usa comúnmente la variante, debida a [[https://en.wikipedia.org/wiki/Roger_Koenker][Koenker]],\n  del contraste que propusieron [[https://en.wikipedia.org/wiki/Trevor_S._Breusch][Breusch]] y [[https://en.wikipedia.org/wiki/Adrian_Pagan][Pagan]].\n\n- En la regresión auxiliar se usan como regresores las $k$ variables explicativas del modelo de regresión original.\n\n### Contraste de Breusch-Pagan: ejemplo\n\n- Modelo de regresión con 3 explicativas:\n  $$\n    y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{3i} + u_i.\n  $$\n\n- Después de estimar por MCO se obtienen los residuos $\\uhat$.\n\n- Regresión auxiliar:\n  $$\n    \\uhat^2_i = \\delta_0 + \\delta_1 x_{1i} + \\delta_2 x_{2i} + \\delta_3 x_{3i} + \\text{error}.\n  $$\n\n- Se estima por MCO la regresión auxiliar y se calcula el estadístico $\\LM$ o el estadístico $F$ de la regresión auxiliar.\n\n### Contraste de White: motivación\n\n[[https://en.wikipedia.org/wiki/Halbert_White][White]] demuestra que la heteroscedasticidad sólo es problemática si la varianza condicional de $u$ depende de:\n\n- las variables explicativas: $x_1, x_2, \\dots, x_k$;\n\n- sus cuadrados: $x^2_1, x^2_2, \\dots, x^2_k$; o\n\n- sus productos cruzados: $x_1 \\cdot x_2, x_1 \\cdot x_3, \\dots$\n\n\n### Contraste de White\n\n- La regresión auxiliar incluye las explicativas, sus cuadrados y sus productos cruzados.\n\n- Pueden aparecer problemas de multicolinealidad perfecta en la regresión auxiliar, especialmente si hay variables ficticias. En ese caso, habría que eliminar de la regresión auxiliar los regresores redundantes.\n\n\n### Contraste de White: ejemplo\n\n- Modelo de regresión con 3 explicativas:\n  $$\n    y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{3i} + u_i.\n  $$\n\n- Después de estimar por MCO se obtienen los residuos $\\uhat$.\n\n- Regresión auxiliar:\n  $$\n  \\begin{align*}\n    \\uhat^2_i\n    = \\delta_0 &+ \\delta_1 x_{1i} + \\delta_2 x_{2i} + \\delta_3 x_{3i}\n    + \\delta_4 x^{2}_{1i} + \\delta_{5} x^{2}_{2i} + \\delta_6 x^{2}_{3i} \\\\\n    &+ \\delta_7 x_{1i} x_{2i} + \\delta_8 x_{1i} x_{3i} + \\delta_9 x_{2i}x_{3i}\n     +  \\text{error}.\n  \\end{align*}\n  $$\n\n- Se estima por MCO la regresión auxiliar y se calcula el estadístico $\\LM$ o el estadístico $F$ de la regresión auxiliar.\n\n\n### Contraste de White: alternativas (I)\n\n- En la regresión auxiliar del contraste de White hay un gran número de parámetros, lo que puede traducirse en baja **potencia** (capacidad de detectar heteroscedasticidad cuando realmente está presente).\n\n- Para mitigar ese problema, en ocasiones se omiten los productos cruzados en la regresión auxiliar:\n  $$\n    \\uhat^2_i\n    = \\delta_0 + \\delta_1 x_{1i} + \\delta_2 x_{2i} + \\delta_3 x_{3i}\n    + \\delta_4 x^{2}_{1i} + \\delta_{5} x^{2}_{2i} + \\delta_6 x^{2}_{3i}\n      +  \\text{error}.\n  $$\n\n\n### Contraste de White: alternativas (y II)\n\n[[https://en.wikipedia.org/wiki/Jeffrey_Wooldridge][Wooldridge]] propone una variante del contraste de White donde sólo se incluyen en la regresión auxiliar las predicciones de MCO, $\\yhat$, y sus cuadrados, $\\yhat^2$:\n$$\n  \\uhat^2_i = \\delta_0 + \\delta_1 \\yhat_{i} + \\delta_2 \\yhat^2_{i} +  \\text{error}.\n$$\n\n## Ineficiencia de MCO\n\n\n### Heteroscedasticidad\n\n+MATS: figcol img/het3.pdf 0.5\n\nCon **heteroscedasticidad** la dispersión alrededor de la FRP cambia con los valores de la explicativa.\n\n\n### Ineficiencia de MCO\n\n+MATS: figcol img/het4.pdf 0.5\n\nLa **ineficiencia de MCO** se debe a que se tratan por igual todas las observaciones, aunque no todas contienen información igual de precisa para estimar la FRP.\n\n\n### Estimación eficiente\n\n**Estimación eficiente** con heteroscedasticidad:\n\n- Se asigna un peso diferente a cada observación.\n\n- Las observaciones más imprecisas reciben ponderaciones menores.\n\n\n### Mínimos Cuadrados Generalizados\n\nEl estimador eficiente cuando existe heteroscedasticidad pertenece a la familia de **Mínimos Cuadrados Generalizados** (MCG). Los estimadores MCG consisten en:\n\n1. **Transformar el modelo** de forma que se cumplan los supuestos de Gauss-Markov.\n\n2. Estimar el **modelo transformado por MCO**.\n\n\n## Mínimos cuadrados ponderados\n\n\n### Modelo de regresión\n\nEl modelo de regresión cumple los supuestos **RLM.1** a **RLM.4**:\n$$\n  y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}\n  + \\dots + \\beta_k x_{ki} + u_i.\n$$\n\n\n### Heteroscedasticidad\n\n- La varianza del término de error puede expresarse como:\n  $$\n    \\var(u_i | x_{1i}, x_{2i}, \\dots, x_{ki}) =\n    \\sigma^2 h(x_{1i}, x_{2i}, \\dots, x_{ki}) =\n    \\sigma^2 h(\\bm{x}_i).\n  $$\n\n- La función $h(\\bm{x}_i)$ toma siempre valores positivos y expresa la relación entre las explicativas y la varianza del término de error.\n\n- La constante desconocida $\\sigma^2$ es positiva.\n\n\n### Heteroscedasticidad conocida\n\n- Supondremos que la función $h(\\bm{x}_i)$ **es conocida** y no depende de parámetros desconocidos.\n\n### Transformación del modelo\n\n- Si conocemos $h_i = h(\\bm{x}_i)$ podemos dividir el modelo original por $\\sqrt{h_i}$.\n\n- El término de error del modelo transformado, $u^*_i = u_i / \\sqrt{h_i}$, es homoscedástico:\n  $$\n  \\begin{align*}\n    \\var(u_i^*| \\bm{x}_i)\n    &= \\Exp\\big((u_i/\\sqrt{h_i})^2| \\bm{x}_i\\big) \\\\\n    &= (1/h_{i})\\Exp(u^2| \\bm{x}_i) \\\\\n    &= (1/h_i) \\sigma^2 h_i \\\\\n    &= \\sigma^2.\n  \\end{align*}\n  $$\n\n### Ejemplo\n\n- Modelo original. Si $x_{2i}$ siempre toma valores positivos y $h(x_{1i}, x_{2i}) = x_{2i}$:\n  $$\n  \\begin{gather*}\n    y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + u_i, \\\\\n    \\var(u_i | x_{1i}, x_{2i}) = \\sigma^2 x_{2i}.\n  \\end{gather*}\n  $$\n\n- Modelo transformado:\n  $$\n  \\begin{gather*}\n    y^*_i = \\beta_0 x^*_{0i} + \\beta_1 x^*_{1i} + \\beta_2 x^*_{2i} + u^*_i, \\\\\n    \\var(u^*_i | x_{1i}, x_{2i}) = \\sigma^2.\n  \\end{gather*}\n  $$\n  donde\n  $$\n  \\begin{equation*}\n    y^*_i = y_i / \\sqrt{x_{2i}},\\quad\n    x^*_{0i} = 1 / \\sqrt{x_{2i}},\\quad\n    x^*_{1i} = x_{1i} / \\sqrt{x_{2i}},\\quad\n    x^*_{2i} = x_{2i} / \\sqrt{x_{2i}}.\n  \\end{equation*}\n  $$\n\n\n### Mínimos Cuadrados Ponderados\n\nEstimador de **Mínimos Cuadrados Ponderados**, (MCP):\n\n- La varianza del término de error es proporcional a $h_i$ que es una función conocida de variables observables:\n  $$\n    \\var(u_i | x_{1i}, x_{2i}, \\dots, x_{ki}) = \\sigma^2 h_i.\n  $$\n\n- Se transforma el modelo dividiendo por $\\sqrt{h_i}$.\n\n- Se estima el modelo transformado por MCO.\n\n\n### Propiedades de MCP\n\n- Si se cumplen los supuestos **RLM.1** a **RLM.4** el estimador MCP es insesgado y consistente.\n\n- El modelo transformado cumple con los supuestos de Gauss-Markov, por lo que MCP es el estimador lineal insesgado óptimo.\n\n\n### Predicciones y residuos\n\n- El modelo transformado sólo sirve para obtener las estimaciones de MCP de los parámetros: $\\btilde_0, \\btilde_1, \\dots, \\btilde_k$.\n\n- Para calcular las predicciones usamos las variables originales sin transformar:\n  $$\n    \\ytilde_i = \\btilde_{0}  + \\btilde_{1} x_{1i} + \\dots + \\btilde_{k} x_{ki}.\n  $$\n\n- También usamos las variables originales para calcular los residuos:\n  $$\n    \\utilde_i = y_i - \\ytilde_i = y_i - \\btilde_{0}  - \\btilde_{1} x_{1i} - \\dots - \\btilde_{k} x_{ki}.\n  $$\n\n\n\n### Suma ponderada de los cuadrados de los residuos\n\n- El estimador MCP minimiza la suma **ponderada de residuos al cuadrado**:\n  $$\n    \\sum_{i} \\utilde^{2}_{i}/h_{i} =\n    \\sum_{i} \\big(\n    y_{i} - \\btilde_{0} - \\btilde_{1} x_{1i} - \\dots - \\btilde_{k} x_{ki}\n    \\big)^{2}/h_{i}.\n  $$\n\n- El **peso** o **ponderación**, $w_i$, que se asigna a cada observación es la inversa de $h_i$, $w_i = 1 / h_i$.\n\n\n### Bondad del ajuste\n\nEn general, no es posible comparar el $R^2$ obtenido con MCP y el obtenido con MCO:\n\n- Diferentes programas informáticos calculan de forma diferente el $R^2$ de MCP y los distintos métodos no son equivalentes.\n\n- Los $R^2$ que se calculan para MCP no se pueden interpretar como medidas de bondad del ajuste.\n\n\n## Mínimos cuadrados ponderados factibles\n\n\n### Heteroscedasticidad desconocida\n\n- Hasta ahora hemos supuesto que conocemos $h_i$. ¿Qué podemos hacer en caso contrario?\n\n- Aunque no conozcamos la forma exacta de la heteroscedasticidad, con frecuencia sabemos que está relacionada con algunas de las variables del modelo.\n\n- El estimador de **Mínimos Cuadrados Ponderados Factibles** (MCPF) se obtiene de forma análoga al estimador de MCP salvo que utiliza una estimación de $h_i$ para obtener el modelo transformado.\n\n\n### Modelización de la heteroscedasticidad\n\nEspecificaremos un modelo para la varianza del término de error que dependa de variables observables y de parámetros desconocidos. Por ejemplo, podemos escribir:\n$$\n  \\var(u_i | \\bm{x}_{i}) = \\sigma^2 h(\\bm{x}_{i})\n$$\ndonde\n$$\n  h(\\bm{x}_{i}) = \\exp(\\delta_{0} + \\delta_{1} x_{1i} + \\dots + \\delta_{k} x_{ki}),\n$$\ny $\\delta_0, \\delta_1, \\dots, \\delta_k$ son parámetros desconocidos. La función exponencial garantiza que $h(\\bm{x}_{i}) > 0$ sean cuales sean los valores de los parámetros.\n\n### Estimación de $h_i$ (I)\n\nVarianza condicional del término de error:\n$$\n  \\Exp(u_i^2 | \\bm{x}_{i}) = \\sigma^2\n  \\exp(\\delta_{0} + \\delta_{1} x_{1i} + \\dots + \\delta_{k} x_{ki}).\n$$\nSi observásemos el término de error, $u_i$, podríamos estimar:\n$$\n  \\log(u_i^2) =\n  \\alpha_0 + \\delta_{1} x_{1i} + \\dots +\n  \\delta_{k} x_{ki} + \\text{error}.\n$$\nPara poder estimar los parámetros de la regresión anterior, reemplazamos $u_i$ por los residuos de MCO, $\\hat{u}_i$.\n\n\n### Estimación de $h_i$ (y II)\n\n1. Estimamos por MCO los parámetros de la función de regresión:\n  $$\n    \\log(\\uhat_i^2) = \\alpha_0 + \\delta_{1} x_{1i} +\n    \\dots + \\delta_{k} x_{ki} + \\text{error}.\n  $$\n\n2. Obtenemos $\\hhat_i$ a partir de las predicciones de la regresión anterior:\n   $$\n     \\hhat_i = \\exp(\\ahat_0 + \\dhat_{1} x_{1i} + \\dots + \\dhat_{k} x_{ki}).\n   $$\n\n### Mínimos Cuadrados Ponderados Factibles\n\nProcedimiento para la estimación por **Mínimos Cuadrados Ponderados Factibles** (MCPF):\n\n+ATTR_LATEX: :options [label=\\textbf{Paso \\arabic*}:, wide=0pt, leftmargin=1em]\n\n1. Se estima el modelo por MCO y se guardan los residuos $\\uhat_i$.\n\n2. Se estima por MCO una regresión de $\\log(\\uhat^2_i)$ sobre las\n   explicativas.\n\n3. Se obtiene $\\hhat_i$ tomando la exponencial de las predicciones de la regresión del paso anterior.\n\n4. Se estima el modelo por MCP usando $1/\\hhat_i$ como ponderaciones.\n\n### Propiedades de MCPF\n\n- Al usar una estimación de $h_i$, MCPF no es insesgado ni lineal.\n\n- Si se cumplen los supuestos **RLM.1** a **RLM.4** el estimador de MCPF es consistente.\n\n- Si la especificación de la heteroscedasticidad es correcta, MCPF es asintóticamente más eficiente que MCO.\n\n\n### Especificaciones alternativas\n\nSe han usado especificaciones alternativas de la heteroscedasticidad. En consecuencia se modificaría el **Paso 2** del procedimiento para obtener el estimador MCPF descrito antes:\n\n- Regresar $\\log(\\uhat^2_i)$ sobre un subconjunto de las explicativas.\n\n- Regresar $\\log(\\uhat^2_i)$ sobre las explicativas, sus cuadrados y productos cruzados.\n\n- Regresar $\\log(\\uhat^2_i)$ sobre las predicciones, $\\yhat_i$, y sus cuadrados, $\\yhat^2_i$.\n\nLos restantes pasos del procedimiento de estimación no se alterarían.\n\n\n## Otras cuestiones\n\n\n### Comparación con MCO\n\n- MCO y MCP(F) son estimadores consistentes cuando se cumplen los supuestos **RLM.1** a **RLM.4**.\n\n- Grandes diferencias en las estimaciones de MCO y de MCP(F) indicarían el posible incumplimiento de otras de las hipótesis de Gauss-Markov.\n\n\n### Inferencia robusta\n\n- Si el supuesto sobre la naturaleza de la heteroscedasticidad es correcto, los contrastes $t$ y $F$ calculados a partir de las estimaciones MCP(F) tienen validez asintótica.\n\n- Si el supuesto no captura toda la heteroscedasticidad, sería necesario usar un estimador robusto de $\\var(\\btilde)$ después de la estimación MCP(F).\n\n- En general, es recomendable el uso de métodos de inferencia robustos a heteroscedasticidad con los estimadores de MCO y de MCP(F).\n\n",
    "supporting": [
      "het_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}