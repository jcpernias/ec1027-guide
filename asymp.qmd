---
title: "Propiedades asintóticas del estimador MCO"
---

{{< include _macros.qmd >}}

## Introducción


### El supuesto de normalidad

El supuesto **RLM.6** del modelo clásico de regresión requiere la normalidad del término de error:
$$
u \sim \Normal(0, \sigma^{2}).
$$



### Inferencia en el modelo clásico

El supuesto de normalidad permite conocer la distribución de los
contrates de hipótesis bajo la hipótesis nula.


### Falta de normalidad

En muchos casos el supuesto de normalidad no es muy razonable:

- Variable dependiente sólo toma valores positivos.

- Variable dependiente sólo toma algunos valores discretos.


### Teoría asintótica

La **teoría asintótica** estudia el comportamiento de estimadores y contrastes de hipótesis cuando el tamaño muestral crece indefinidamente.


### Teoremas sobre límites

Bajo ciertas condiciones, secuencias de variables aleatorias obtenidas por muestreo aleatorio cumplen conforme $n \to \infty$:

- **Leyes de grandes números**: establecen la convergencia de medias muestrales a la media poblacional.

- **Teoremas del límite central**: establecen la **distribución asintótica** de las medias muestrales.

### Propiedades asintóticas

- **Consistencia**.

- **Distribución asintótica**.

- **Eficiencia asintótica**.

### Consistencia

Un estimador es consistente si conforme aumenta el tamaño muestral converge en probabilidad al parámetro poblacional:
$$
\plim \bhat_j = \beta_j.
$$

Intuitivamente, cuanto mayor es el tamaño muestral más cerca está un estimador consistente del parámetro poblacional.


### Consistencia e insesgadez

- Las propiedades de consistencia e insesgadez son independientes: una no implica la otra.

- Un estimador es consistente si es insesgado y su varianza decrece y converge a 0 conforme aumenta el tamaño muestral.


### Distribución asintótica

- La **distribución asintótica** de un estimador es su distribución en el límite.

- Proporciona una aproximación a la distribución del estimador para un tamaño muestral finito.


### Normalidad asintótica

- Un estimador es **asintóticamente normal** si su distribución asintótica es normal.

- No es necesaria la normalidad del término de error, basta con que se cumplan las condiciones de un teorema del límite central.


### Consideraciones prácticas

- Las propiedades asintóticas no requieren el supuesto de normalidad.

- Requisitos técnicos: momentos finitos hasta orden 4. Supondremos implícitamente que se cumple.

- La calidad de las aproximaciones asintóticas suele mejorar con el tamaño muestral. Pero, ¿cuántas observaciones son necesarias en la práctica?


## Propiedades asintóticas de MCO

### Consistencia

- MCO es consistente si se cumplen los supuestos **RLM.1** a **RLM.4**.

- El supuesto **RLM.4**, $E(u|\bm{x}) = 0$, puede relajarse y basta con:
  $$
    \Exp(u) = 0 \stext{y} \cov(x_{j}, u) \text{\ para $j = 1, 2, \dots, k$}.
  $$

### Inconsistencia

- La **inconsistencia** de $\bhat_i$ es la diferencia entre el límite del estimador y el parámetro poblacional. En el caso de MCO:
  $$
    \plim \bhat_{i} - \beta_{i} = \frac{\cov(x_{i}, u)}{\var(x_{i})}.
  $$

- MCO es **inconsistente** si el término de error y, al menos, una de las explicativas están correlacionados.


### Variable relevante omitida

- Se cumplen los supuestos **RLM.1** a **RLM.4** en el modelo poblacional:
  $$
    y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + u.
  $$

- Si se omite la variable $x_2$, el sesgo del estimador MCO de $\beta_1$ es:
  $$
    \plim \bhat_{1} - \beta_{1} = \beta_2 \delta_{21},
  $$
  donde $\delta_{21} = \cov(x_1, x_2) / \var(x_1)$, mide la intensidad de la asociación de las variables $x_2$ y $x_1$.

- Omitir una variable relevante, $\beta_2 \neq 0$, causa inconsistencia si la variable omitida está correlacionada con alguna de las explicativas, $\delta_{21} \neq 0$.

### Varianza de los estimadores MCO

Si se cumplen los supuestos de Gauss-Markov, **RLM.1** a **RLM.5**:

- $\hat{\sigma}^2$ es un estimador consistente de $\var(u)$.

- Las fórmulas para estimar las varianzas de los estimadores MCO son válidas asintóticamente:
  $$
    \widehat{\var}(\bhat_{j}) = \frac{\hat{\sigma}^2}{\SCT_{j}(1-\Rsq_{j})}.
  $$


### Normalidad asintótica del estimador MCO

Si se cumplen los supuestos de Gauss-Markov, **RLM.1** a **RLM.5**, la distribución asintótica del estimador MCO es normal:
  $$
    \frac{\bhat_{j} - \beta_{j}}{\sd(\bhat_{j})} \adistr \Normal(0, 1),
    \text{\ para $j = 0, 1, 2, \dots, k$}.
  $$



## Inferencia asintótica


### Contraste de hipótesis en muestras grandes

- No es necesario el supuesto de normalidad del término de error para usar los contrastes $t$ y $F$.

- Disponemos de otros contrastes que sólo son válidos asintóticamente.


### Contrastes $t$

- Si se cumplen los supuestos de Gauss-Markov, podemos usar la distribución $t$ para determinar valores críticos y los valores-$p$ de los contrates $t$.
  $$
    \frac{\bhat_{j} - \beta_{j}}{\se(\bhat_{j})} \adistr t_{n - k - 1}.
  $$


### Intervalos de confianza

- Si se cumplen los supuestos de Gauss-Markov, podemos usar la fórmula habitual para obtener un intervalo a un nivel de confianza $1 - \alpha$ para el parámetro $\beta_j$:
  $$
    \bhat_j \pm c_{\alpha/2} \cdot \se(\bhat_j),
  $$
  donde $c_{\alpha/2}$ es el valor crítico de dos colas de una distribución $t_{n-k-1}$ a un nivel de significación $\alpha$.

### Contrastes $F$

- Si se cumplen los supuestos de Gauss-Markov, podemos usar la distribución $F$ para determinar valores críticos y los valores-$p$ de los contrates $F$.
  $$
    \frac{(\SCRr - \SCRnr) / q}{\SCRnr / (n - k - 1)}  \adistr F_{q,\, n - k - 1}.
  $$

### Otros contrastes asintóticos

Se han propuesto otros principios de contrastación de restricciones generales sobre los parámetros y que sólo tienen validez asintótica:

- **Contrastes de Wald**.

- **Contrastes de multiplicadores de Lagrange**.

- **Contrastes de la razón de verosimilitudes**.


### Contrastes de Wald

- Los **contrastes de Wald** pueden interpretarse como una generalización de los contrastes $t$.

- Sólo requieren la estimación del modelo no restringido.

- Bajo la $H_0$ se distribuyen como una $\chi^2$ con $q$ grados de libertad. También se usan versiones de este contraste que se distribuyen como una $F_{q,\, n - k - 1}$.

### Contrastes de multiplicadores de Lagrange

- Para utilizar un **contraste de multiplicadores de Lagrange**, $\LM$, sólo es necesaria la estimación del modelo restringido.

- Son especialmente útiles cuando la estimación del modelo no restringido es difícil.

- Frecuentemente se calculan a partir de una **regresión auxiliar**.


### Contraste de variables omitidas

- Modelo de regresión poblacional que cumple **RLM.1** a **RLM.5**:
  $$
    y = \beta_0 + \beta_1 x_1  + \beta_2 x_2  + \beta_3 x_3  + \beta_4 x_4 + u.
  $$

- Estamos interesados en contrastar la hipótesis nula
  $$
    H_0\!: \beta_3 = \beta_4 = 0.
  $$

### Contraste $\LM$ de variables omitidas

- Estimación del modelo restringido por MCO:
  $$
    y = \bhat_0 + \bhat_1 x_1  + \bhat_2 x_2 + \uhat.
  $$

- **Regresión auxiliar** usando los residuos como variable dependiente y todas las explicativas, tanto incluidas como excluidas:
  $$
    \uhat \text{\ sobre $x_1, x_2, x_3, x_4$}.
  $$

- El estadístico de contraste es $\LM = n \Rsq_{aux}$: el número de observaciones multiplicado por el $\Rsq$ de la regresión auxiliar.

### Regla de decisión

Si la hipótesis nula que impone $q$ restricciones es correcta:

- El estadístico $\LM$ se distribuye asintóticamente como una
  $\chi^2$ con $q$ grados de libertad:
  $$
    n \Rsq_{aux} \adistr \chi^2_q.
  $$

- Una versión alternativa del contraste utiliza un estadístico $F$ para contrastar restricciones de exclusión en la regresión auxiliar.


### Contrastes de razón de verosimilitudes

- Se pueden interpretar como una generalización del contraste $F$.

- Requieren la estimación del modelo restringido y del modelo no
  restringido.

- Son útiles cuando en modelos no lineales que se estiman con métodos diferentes a MCO.
